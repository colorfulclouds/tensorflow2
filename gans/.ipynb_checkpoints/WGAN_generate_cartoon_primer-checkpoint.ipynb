{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.initializers import truncated_normal , random_normal , constant\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 96\n",
    "HEIGHT = 96\n",
    "CHANNEL = 3\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = 'faces/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n",
    "\n",
    "#为WGAN增加的\n",
    "N_CRITIC = 5 #训练G时使用\n",
    "CLIP_VALUE = 0.01 #更新G的权重参数时进行截断使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(X_train , y_train),(X_test , y_test) = mnist.load_data()\n",
    "X_train = X_train/127.5-1\n",
    "X_train = np.expand_dims(X_train , 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def load_mnist():\n",
    "    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\n",
    "    \n",
    "def write_image_mnist(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = generated_image*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('mnist_wgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('generated_faces_wgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(output_size):\n",
    "    return Conv2D(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def dense(output_size):\n",
    "    return Dense(output_size , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def deconv2d(output_size):\n",
    "    return Conv2DTranspose(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def batch_norm():\n",
    "    return BatchNormalization(momentum=0.9 , epsilon=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "    \n",
    "    #cartoon 图像使用 96*96*3\n",
    "    model.add(Dense(6*6*8*64 , input_shape=(LATENT_DIM,) , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0)))\n",
    "    \n",
    "    model.add(Reshape((6, 6, 64*8)))\n",
    "    \n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(deconv2d(64*4))\n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(deconv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(deconv2d(64*1))\n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(deconv2d(3))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    image = model(noise)\n",
    "    \n",
    "    return Model(noise , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='critic')\n",
    "    \n",
    "    model.add(Conv2D(filters=64 , kernel_size=(5,5) , strides=(2,2) , padding='same' , input_shape=(WIDTH , HEIGHT , CHANNEL) , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0) , name='conv1'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(conv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(conv2d(64*4))\n",
    "    model.add(batch_norm())  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    \n",
    "    model.add(conv2d(64*8))\n",
    "    model.add(batch_norm())  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #===\n",
    "    #如果没有下面的两个FC层 训练时发生损失不下降 且生成不出图像\n",
    "    #model.add(dense(1024))\n",
    "    #model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    #===\n",
    "    model.add(dense(1))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    validity = model(image)\n",
    "    \n",
    "    return Model(image , validity , name='critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsprop = RMSprop(lr=0.00005)\n",
    "\n",
    "def wgan_loss(y_true , y_pred):\n",
    "    return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 48, 48, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               4718848   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,029,249\n",
      "Trainable params: 9,027,457\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 18432)             1861632   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 12, 12, 256)       3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 24, 24, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 48, 48, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 96, 96, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 96, 96, 3)         0         \n",
      "=================================================================\n",
      "Total params: 6,171,523\n",
      "Trainable params: 6,169,603\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_i = critic()\n",
    "critic_i.compile(optimizer=rmsprop , loss=wgan_loss , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator()\n",
    "\n",
    "z = Input(shape=(LATENT_DIM , ) , name='z')\n",
    "image = generator_i(z)\n",
    "validity = critic_i(image)\n",
    "\n",
    "combined_model_i = Model(z , validity)\n",
    "combined_model_i.compile(optimizer=rmsprop , loss=wgan_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:1.800870 gene_loss:0.420035\n",
      "epoch:1 loss:1.860250 gene_loss:0.373381\n",
      "epoch:2 loss:1.920574 gene_loss:0.317472\n",
      "epoch:3 loss:1.981230 gene_loss:0.266104\n",
      "epoch:4 loss:2.048466 gene_loss:0.214421\n",
      "epoch:5 loss:2.102844 gene_loss:0.164625\n",
      "epoch:6 loss:2.177643 gene_loss:0.116024\n",
      "epoch:7 loss:2.234054 gene_loss:0.074761\n",
      "epoch:8 loss:2.292432 gene_loss:0.035752\n",
      "epoch:9 loss:2.346036 gene_loss:-0.000503\n",
      "epoch:10 loss:2.383422 gene_loss:-0.029925\n",
      "epoch:11 loss:2.435460 gene_loss:-0.066685\n",
      "epoch:12 loss:2.483436 gene_loss:-0.100914\n",
      "epoch:13 loss:2.527373 gene_loss:-0.136183\n",
      "epoch:14 loss:2.573527 gene_loss:-0.175185\n",
      "epoch:15 loss:2.600257 gene_loss:-0.206935\n",
      "epoch:16 loss:2.648395 gene_loss:-0.244676\n",
      "epoch:17 loss:2.698341 gene_loss:-0.280201\n",
      "epoch:18 loss:2.731285 gene_loss:-0.308801\n",
      "epoch:19 loss:2.777992 gene_loss:-0.343140\n",
      "epoch:20 loss:2.809980 gene_loss:-0.375676\n",
      "epoch:21 loss:2.846431 gene_loss:-0.406862\n",
      "epoch:22 loss:2.896318 gene_loss:-0.442894\n",
      "epoch:23 loss:2.931679 gene_loss:-0.476197\n",
      "epoch:24 loss:2.967732 gene_loss:-0.502855\n",
      "epoch:25 loss:3.009877 gene_loss:-0.541955\n",
      "epoch:26 loss:3.043080 gene_loss:-0.578210\n",
      "epoch:27 loss:3.080724 gene_loss:-0.608123\n",
      "epoch:28 loss:3.114736 gene_loss:-0.639368\n",
      "epoch:29 loss:3.152452 gene_loss:-0.670766\n",
      "epoch:30 loss:3.184833 gene_loss:-0.696321\n",
      "epoch:31 loss:3.218126 gene_loss:-0.728540\n",
      "epoch:32 loss:3.255679 gene_loss:-0.755183\n",
      "epoch:33 loss:3.283276 gene_loss:-0.782989\n",
      "epoch:34 loss:3.316247 gene_loss:-0.813692\n",
      "epoch:35 loss:3.342631 gene_loss:-0.843401\n",
      "epoch:36 loss:3.371713 gene_loss:-0.871868\n",
      "epoch:37 loss:3.396650 gene_loss:-0.897811\n",
      "epoch:38 loss:3.416933 gene_loss:-0.929203\n",
      "epoch:39 loss:3.442396 gene_loss:-0.958726\n",
      "epoch:40 loss:3.462317 gene_loss:-0.988890\n",
      "epoch:41 loss:3.484373 gene_loss:-1.010362\n",
      "epoch:42 loss:3.499585 gene_loss:-1.040476\n",
      "epoch:43 loss:3.523549 gene_loss:-1.067938\n",
      "epoch:44 loss:3.541131 gene_loss:-1.093709\n",
      "epoch:45 loss:3.555992 gene_loss:-1.110958\n",
      "epoch:46 loss:3.573487 gene_loss:-1.142668\n",
      "epoch:47 loss:3.582377 gene_loss:-1.165317\n",
      "epoch:48 loss:3.600518 gene_loss:-1.186283\n",
      "epoch:49 loss:3.611414 gene_loss:-1.207350\n",
      "epoch:50 loss:3.623909 gene_loss:-1.223641\n",
      "epoch:51 loss:3.632505 gene_loss:-1.238885\n",
      "epoch:52 loss:3.638532 gene_loss:-1.250348\n",
      "epoch:53 loss:3.646517 gene_loss:-1.259397\n",
      "epoch:54 loss:3.654231 gene_loss:-1.267517\n",
      "epoch:55 loss:3.656303 gene_loss:-1.269654\n",
      "epoch:56 loss:3.666466 gene_loss:-1.281696\n",
      "epoch:57 loss:3.662129 gene_loss:-1.287136\n",
      "epoch:58 loss:3.668307 gene_loss:-1.293097\n",
      "epoch:59 loss:3.678344 gene_loss:-1.298773\n",
      "epoch:60 loss:3.683491 gene_loss:-1.303894\n",
      "epoch:61 loss:3.688515 gene_loss:-1.309410\n",
      "epoch:62 loss:3.689039 gene_loss:-1.313105\n",
      "epoch:63 loss:3.691084 gene_loss:-1.319014\n",
      "epoch:64 loss:3.697028 gene_loss:-1.322747\n",
      "epoch:65 loss:3.700343 gene_loss:-1.326219\n",
      "epoch:66 loss:3.703829 gene_loss:-1.331751\n",
      "epoch:67 loss:3.697633 gene_loss:-1.333479\n",
      "epoch:68 loss:3.707332 gene_loss:-1.337785\n",
      "epoch:69 loss:3.713638 gene_loss:-1.341491\n",
      "epoch:70 loss:3.717249 gene_loss:-1.346172\n",
      "epoch:71 loss:3.714482 gene_loss:-1.347456\n",
      "epoch:72 loss:3.719082 gene_loss:-1.352338\n",
      "epoch:73 loss:3.719952 gene_loss:-1.351900\n",
      "epoch:74 loss:3.724361 gene_loss:-1.357224\n",
      "epoch:75 loss:3.718209 gene_loss:-1.355056\n",
      "epoch:76 loss:3.726168 gene_loss:-1.362246\n",
      "epoch:77 loss:3.725958 gene_loss:-1.362939\n",
      "epoch:78 loss:3.732908 gene_loss:-1.365084\n",
      "epoch:79 loss:3.730633 gene_loss:-1.368068\n",
      "epoch:80 loss:3.732203 gene_loss:-1.369081\n",
      "epoch:81 loss:3.732306 gene_loss:-1.373073\n",
      "epoch:82 loss:3.735403 gene_loss:-1.373769\n",
      "epoch:83 loss:3.738641 gene_loss:-1.377871\n",
      "epoch:84 loss:3.741515 gene_loss:-1.380299\n",
      "epoch:85 loss:3.741395 gene_loss:-1.381990\n",
      "epoch:86 loss:3.742612 gene_loss:-1.385100\n",
      "epoch:87 loss:3.747990 gene_loss:-1.388430\n",
      "epoch:88 loss:3.746643 gene_loss:-1.388014\n",
      "epoch:89 loss:3.749258 gene_loss:-1.392131\n",
      "epoch:90 loss:3.751371 gene_loss:-1.394149\n",
      "epoch:91 loss:3.744534 gene_loss:-1.395823\n",
      "epoch:92 loss:3.753626 gene_loss:-1.396357\n",
      "epoch:93 loss:3.752528 gene_loss:-1.401235\n",
      "epoch:94 loss:3.756095 gene_loss:-1.402631\n",
      "epoch:95 loss:3.755993 gene_loss:-1.403121\n",
      "epoch:96 loss:3.756748 gene_loss:-1.406181\n",
      "epoch:97 loss:3.760028 gene_loss:-1.407248\n",
      "epoch:98 loss:3.758530 gene_loss:-1.408002\n",
      "epoch:99 loss:3.760330 gene_loss:-1.408598\n",
      "epoch:100 loss:3.761470 gene_loss:-1.410711\n",
      "epoch:101 loss:3.760570 gene_loss:-1.411721\n",
      "epoch:102 loss:3.761513 gene_loss:-1.413747\n",
      "epoch:103 loss:3.766203 gene_loss:-1.413663\n",
      "epoch:104 loss:3.766399 gene_loss:-1.416789\n",
      "epoch:105 loss:3.765654 gene_loss:-1.416289\n",
      "epoch:106 loss:3.769537 gene_loss:-1.418425\n",
      "epoch:107 loss:3.770400 gene_loss:-1.417408\n",
      "epoch:108 loss:3.771278 gene_loss:-1.420343\n",
      "epoch:109 loss:3.767241 gene_loss:-1.419450\n",
      "epoch:110 loss:3.773172 gene_loss:-1.421840\n",
      "epoch:111 loss:3.772480 gene_loss:-1.418556\n",
      "epoch:112 loss:3.776966 gene_loss:-1.420197\n",
      "epoch:113 loss:3.773909 gene_loss:-1.412541\n",
      "epoch:114 loss:3.772980 gene_loss:-1.400950\n",
      "epoch:115 loss:3.774919 gene_loss:-1.354633\n",
      "epoch:116 loss:3.771385 gene_loss:-1.221941\n",
      "epoch:117 loss:3.763036 gene_loss:-0.924664\n",
      "epoch:118 loss:3.724260 gene_loss:-0.000616\n",
      "epoch:119 loss:3.682104 gene_loss:2.896336\n",
      "epoch:120 loss:3.653187 gene_loss:3.695208\n",
      "epoch:121 loss:3.654542 gene_loss:3.831090\n",
      "epoch:122 loss:3.686568 gene_loss:3.848909\n",
      "epoch:123 loss:3.707632 gene_loss:3.787703\n",
      "epoch:124 loss:3.719571 gene_loss:3.799576\n",
      "epoch:125 loss:3.719064 gene_loss:3.832646\n",
      "epoch:126 loss:3.723291 gene_loss:3.755047\n",
      "epoch:127 loss:3.662630 gene_loss:3.585912\n",
      "epoch:128 loss:3.670440 gene_loss:3.825033\n",
      "epoch:129 loss:3.591325 gene_loss:3.462052\n",
      "epoch:130 loss:3.626841 gene_loss:3.606547\n",
      "epoch:131 loss:3.589535 gene_loss:2.677309\n",
      "epoch:132 loss:2.917056 gene_loss:3.037976\n",
      "epoch:133 loss:3.406198 gene_loss:1.826543\n",
      "epoch:134 loss:3.564296 gene_loss:0.555579\n",
      "epoch:135 loss:3.583303 gene_loss:0.047030\n",
      "epoch:136 loss:3.636562 gene_loss:-0.930673\n",
      "epoch:137 loss:3.683672 gene_loss:-1.210616\n",
      "epoch:138 loss:3.678567 gene_loss:-1.263962\n",
      "epoch:139 loss:3.730843 gene_loss:-1.374221\n",
      "epoch:140 loss:3.747844 gene_loss:-1.407357\n",
      "epoch:141 loss:3.744097 gene_loss:-1.401793\n",
      "epoch:142 loss:3.769295 gene_loss:-1.427093\n",
      "epoch:143 loss:3.767578 gene_loss:-1.425042\n",
      "epoch:144 loss:3.775392 gene_loss:-1.423202\n",
      "epoch:145 loss:3.769835 gene_loss:-1.436710\n",
      "epoch:146 loss:3.782098 gene_loss:-1.439194\n",
      "epoch:147 loss:3.774203 gene_loss:-1.439119\n",
      "epoch:148 loss:3.779330 gene_loss:-1.428738\n",
      "epoch:149 loss:3.789065 gene_loss:-1.451127\n",
      "epoch:150 loss:3.773814 gene_loss:-1.441053\n",
      "epoch:151 loss:3.785183 gene_loss:-1.447200\n",
      "epoch:152 loss:3.789193 gene_loss:-1.452189\n",
      "epoch:153 loss:3.786023 gene_loss:-1.444722\n",
      "epoch:154 loss:3.789387 gene_loss:-1.453479\n",
      "epoch:155 loss:3.791456 gene_loss:-1.450381\n",
      "epoch:156 loss:3.791378 gene_loss:-1.456021\n",
      "epoch:157 loss:3.786525 gene_loss:-1.446327\n",
      "epoch:158 loss:3.792854 gene_loss:-1.454338\n",
      "epoch:159 loss:3.797639 gene_loss:-1.454190\n",
      "epoch:160 loss:3.791793 gene_loss:-1.457832\n",
      "epoch:161 loss:3.796422 gene_loss:-1.453021\n",
      "epoch:162 loss:3.797290 gene_loss:-1.457848\n",
      "epoch:163 loss:3.795675 gene_loss:-1.458439\n",
      "epoch:164 loss:3.796690 gene_loss:-1.456377\n",
      "epoch:165 loss:3.797052 gene_loss:-1.456924\n",
      "epoch:166 loss:3.797020 gene_loss:-1.461794\n",
      "epoch:167 loss:3.798138 gene_loss:-1.462679\n",
      "epoch:168 loss:3.795525 gene_loss:-1.459507\n",
      "epoch:169 loss:3.799709 gene_loss:-1.463236\n",
      "epoch:170 loss:3.797415 gene_loss:-1.462568\n",
      "epoch:171 loss:3.800309 gene_loss:-1.462101\n",
      "epoch:172 loss:3.801439 gene_loss:-1.465779\n",
      "epoch:173 loss:3.800993 gene_loss:-1.463876\n",
      "epoch:174 loss:3.800044 gene_loss:-1.459620\n",
      "epoch:175 loss:3.803715 gene_loss:-1.467523\n",
      "epoch:176 loss:3.800036 gene_loss:-1.461625\n",
      "epoch:177 loss:3.803378 gene_loss:-1.466579\n",
      "epoch:178 loss:3.802316 gene_loss:-1.465777\n",
      "epoch:179 loss:3.803385 gene_loss:-1.469251\n",
      "epoch:180 loss:3.803820 gene_loss:-1.466992\n",
      "epoch:181 loss:3.802335 gene_loss:-1.464209\n",
      "epoch:182 loss:3.804917 gene_loss:-1.466965\n",
      "epoch:183 loss:3.803838 gene_loss:-1.467476\n",
      "epoch:184 loss:3.806154 gene_loss:-1.469299\n",
      "epoch:185 loss:3.803665 gene_loss:-1.467576\n",
      "epoch:186 loss:3.805012 gene_loss:-1.469079\n",
      "epoch:187 loss:3.804050 gene_loss:-1.467617\n",
      "epoch:188 loss:3.803851 gene_loss:-1.469971\n",
      "epoch:189 loss:3.804494 gene_loss:-1.470789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:190 loss:3.803133 gene_loss:-1.471437\n",
      "epoch:191 loss:3.804697 gene_loss:-1.466181\n",
      "epoch:192 loss:3.808078 gene_loss:-1.472929\n",
      "epoch:193 loss:3.803762 gene_loss:-1.466535\n",
      "epoch:194 loss:3.808145 gene_loss:-1.472509\n",
      "epoch:195 loss:3.806570 gene_loss:-1.473524\n",
      "epoch:196 loss:3.806127 gene_loss:-1.468312\n",
      "epoch:197 loss:3.808488 gene_loss:-1.473478\n",
      "epoch:198 loss:3.806086 gene_loss:-1.470021\n",
      "epoch:199 loss:3.807036 gene_loss:-1.473267\n",
      "epoch:200 loss:3.808108 gene_loss:-1.471969\n",
      "epoch:201 loss:3.810417 gene_loss:-1.474874\n",
      "epoch:202 loss:3.804861 gene_loss:-1.469447\n",
      "epoch:203 loss:3.810587 gene_loss:-1.474670\n",
      "epoch:204 loss:3.811357 gene_loss:-1.474313\n",
      "epoch:205 loss:3.808753 gene_loss:-1.471510\n",
      "epoch:206 loss:3.809780 gene_loss:-1.475173\n",
      "epoch:207 loss:3.805023 gene_loss:-1.466982\n",
      "epoch:208 loss:3.809957 gene_loss:-1.476110\n",
      "epoch:209 loss:3.809104 gene_loss:-1.473895\n",
      "epoch:210 loss:3.809937 gene_loss:-1.468246\n",
      "epoch:211 loss:3.810262 gene_loss:-1.476298\n",
      "epoch:212 loss:3.809054 gene_loss:-1.471915\n",
      "epoch:213 loss:3.810659 gene_loss:-1.475420\n",
      "epoch:214 loss:3.811375 gene_loss:-1.475767\n",
      "epoch:215 loss:3.805084 gene_loss:-1.466134\n",
      "epoch:216 loss:3.810925 gene_loss:-1.476489\n",
      "epoch:217 loss:3.809400 gene_loss:-1.471074\n",
      "epoch:218 loss:3.808325 gene_loss:-1.476193\n",
      "epoch:219 loss:3.810348 gene_loss:-1.476899\n",
      "epoch:220 loss:3.812244 gene_loss:-1.472789\n",
      "epoch:221 loss:3.811481 gene_loss:-1.475896\n",
      "epoch:222 loss:3.811627 gene_loss:-1.476032\n",
      "epoch:223 loss:3.811593 gene_loss:-1.476245\n",
      "epoch:224 loss:3.809040 gene_loss:-1.475425\n",
      "epoch:225 loss:3.812187 gene_loss:-1.476184\n",
      "epoch:226 loss:3.812743 gene_loss:-1.475680\n",
      "epoch:227 loss:3.810408 gene_loss:-1.475788\n",
      "epoch:228 loss:3.812041 gene_loss:-1.475490\n",
      "epoch:229 loss:3.810767 gene_loss:-1.467273\n",
      "epoch:230 loss:3.814708 gene_loss:-1.476939\n",
      "epoch:231 loss:3.807978 gene_loss:-1.475027\n",
      "epoch:232 loss:3.810637 gene_loss:-1.476156\n",
      "epoch:233 loss:3.811375 gene_loss:-1.473057\n",
      "epoch:234 loss:3.810875 gene_loss:-1.475934\n",
      "epoch:235 loss:3.812467 gene_loss:-1.473121\n",
      "epoch:236 loss:3.809401 gene_loss:-1.475493\n",
      "epoch:237 loss:3.811613 gene_loss:-1.475058\n",
      "epoch:238 loss:3.811930 gene_loss:-1.471073\n",
      "epoch:239 loss:3.810814 gene_loss:-1.475715\n",
      "epoch:240 loss:3.809575 gene_loss:-1.470867\n",
      "epoch:241 loss:3.810469 gene_loss:-1.474349\n",
      "epoch:242 loss:3.810198 gene_loss:-1.467566\n",
      "epoch:243 loss:3.810969 gene_loss:-1.474041\n",
      "epoch:244 loss:3.811363 gene_loss:-1.472402\n",
      "epoch:245 loss:3.809845 gene_loss:-1.472913\n",
      "epoch:246 loss:3.809321 gene_loss:-1.471226\n",
      "epoch:247 loss:3.809637 gene_loss:-1.465066\n",
      "epoch:248 loss:3.808515 gene_loss:-1.469851\n",
      "epoch:249 loss:3.809890 gene_loss:-1.468130\n",
      "epoch:250 loss:3.809020 gene_loss:-1.469478\n",
      "epoch:251 loss:3.805746 gene_loss:-1.468432\n",
      "epoch:252 loss:3.808328 gene_loss:-1.466168\n",
      "epoch:253 loss:3.807935 gene_loss:-1.467075\n",
      "epoch:254 loss:3.809191 gene_loss:-1.464924\n",
      "epoch:255 loss:3.806081 gene_loss:-1.464284\n",
      "epoch:256 loss:3.806103 gene_loss:-1.461425\n",
      "epoch:257 loss:3.804668 gene_loss:-1.461034\n",
      "epoch:258 loss:3.805292 gene_loss:-1.457893\n",
      "epoch:259 loss:3.806031 gene_loss:-1.455755\n",
      "epoch:260 loss:3.802709 gene_loss:-1.451885\n",
      "epoch:261 loss:3.799859 gene_loss:-1.450937\n",
      "epoch:262 loss:3.799748 gene_loss:-1.435196\n",
      "epoch:263 loss:3.797676 gene_loss:-1.437134\n",
      "epoch:264 loss:3.795197 gene_loss:-1.432701\n",
      "epoch:265 loss:3.797103 gene_loss:-1.430027\n",
      "epoch:266 loss:3.795346 gene_loss:-1.426219\n",
      "epoch:267 loss:3.794714 gene_loss:-1.426568\n",
      "epoch:268 loss:3.797412 gene_loss:-1.422055\n",
      "epoch:269 loss:3.792794 gene_loss:-1.437320\n",
      "epoch:270 loss:3.797281 gene_loss:-1.441041\n",
      "epoch:271 loss:3.800119 gene_loss:-1.437232\n",
      "epoch:272 loss:3.797910 gene_loss:-1.448459\n",
      "epoch:273 loss:3.798880 gene_loss:-1.443929\n",
      "epoch:274 loss:3.796959 gene_loss:-1.448738\n",
      "epoch:275 loss:3.799006 gene_loss:-1.453507\n",
      "epoch:276 loss:3.801910 gene_loss:-1.454304\n",
      "epoch:277 loss:3.799232 gene_loss:-1.456229\n",
      "epoch:278 loss:3.798586 gene_loss:-1.454738\n",
      "epoch:279 loss:3.801032 gene_loss:-1.458447\n",
      "epoch:280 loss:3.801937 gene_loss:-1.458344\n",
      "epoch:281 loss:3.800484 gene_loss:-1.455689\n",
      "epoch:282 loss:3.794732 gene_loss:-1.458255\n",
      "epoch:283 loss:3.804265 gene_loss:-1.460206\n",
      "epoch:284 loss:3.802918 gene_loss:-1.458702\n",
      "epoch:285 loss:3.804200 gene_loss:-1.460052\n",
      "epoch:286 loss:3.803147 gene_loss:-1.459206\n",
      "epoch:287 loss:3.800845 gene_loss:-1.459269\n",
      "epoch:288 loss:3.803156 gene_loss:-1.460336\n",
      "epoch:289 loss:3.801630 gene_loss:-1.460427\n",
      "epoch:290 loss:3.803591 gene_loss:-1.458237\n",
      "epoch:291 loss:3.800580 gene_loss:-1.459037\n",
      "epoch:292 loss:3.804042 gene_loss:-1.460210\n",
      "epoch:293 loss:3.800008 gene_loss:-1.456195\n",
      "epoch:294 loss:3.801560 gene_loss:-1.459653\n",
      "epoch:295 loss:3.803990 gene_loss:-1.457825\n",
      "epoch:296 loss:3.804033 gene_loss:-1.459163\n",
      "epoch:297 loss:3.804796 gene_loss:-1.458429\n",
      "epoch:298 loss:3.802609 gene_loss:-1.456841\n",
      "epoch:299 loss:3.803462 gene_loss:-1.457644\n",
      "epoch:300 loss:3.802781 gene_loss:-1.457858\n",
      "epoch:301 loss:3.798342 gene_loss:-1.455584\n",
      "epoch:302 loss:3.802701 gene_loss:-1.455765\n",
      "epoch:303 loss:3.803177 gene_loss:-1.455708\n",
      "epoch:304 loss:3.801391 gene_loss:-1.454580\n",
      "epoch:305 loss:3.800960 gene_loss:-1.452250\n",
      "epoch:306 loss:3.801890 gene_loss:-1.452973\n",
      "epoch:307 loss:3.799864 gene_loss:-1.450176\n",
      "epoch:308 loss:3.801259 gene_loss:-1.451901\n",
      "epoch:309 loss:3.802431 gene_loss:-1.452596\n",
      "epoch:310 loss:3.797836 gene_loss:-1.451606\n",
      "epoch:311 loss:3.799834 gene_loss:-1.448949\n",
      "epoch:312 loss:3.800870 gene_loss:-1.452412\n",
      "epoch:313 loss:3.799181 gene_loss:-1.450766\n",
      "epoch:314 loss:3.797024 gene_loss:-1.447178\n",
      "epoch:315 loss:3.799073 gene_loss:-1.449635\n",
      "epoch:316 loss:3.800139 gene_loss:-1.449059\n",
      "epoch:317 loss:3.800158 gene_loss:-1.449536\n",
      "epoch:318 loss:3.799536 gene_loss:-1.448928\n",
      "epoch:319 loss:3.800183 gene_loss:-1.448858\n",
      "epoch:320 loss:3.798604 gene_loss:-1.447539\n",
      "epoch:321 loss:3.799952 gene_loss:-1.448447\n",
      "epoch:322 loss:3.800328 gene_loss:-1.448768\n",
      "epoch:323 loss:3.798761 gene_loss:-1.450087\n",
      "epoch:324 loss:3.800790 gene_loss:-1.446362\n",
      "epoch:325 loss:3.800352 gene_loss:-1.451042\n",
      "epoch:326 loss:3.800002 gene_loss:-1.450151\n",
      "epoch:327 loss:3.799006 gene_loss:-1.451241\n",
      "epoch:328 loss:3.796824 gene_loss:-1.450783\n",
      "epoch:329 loss:3.799276 gene_loss:-1.449430\n",
      "epoch:330 loss:3.799672 gene_loss:-1.451250\n",
      "epoch:331 loss:3.799339 gene_loss:-1.449720\n",
      "epoch:332 loss:3.800267 gene_loss:-1.448344\n",
      "epoch:333 loss:3.798463 gene_loss:-1.449887\n",
      "epoch:334 loss:3.798480 gene_loss:-1.449338\n",
      "epoch:335 loss:3.798357 gene_loss:-1.446509\n",
      "epoch:336 loss:3.797076 gene_loss:-1.442923\n",
      "epoch:337 loss:3.799729 gene_loss:-1.448876\n",
      "epoch:338 loss:3.799149 gene_loss:-1.448814\n",
      "epoch:339 loss:3.796888 gene_loss:-1.446486\n",
      "epoch:340 loss:3.803026 gene_loss:-1.444268\n",
      "epoch:341 loss:3.802582 gene_loss:-1.448195\n",
      "epoch:342 loss:3.800331 gene_loss:-1.447650\n",
      "epoch:343 loss:3.800129 gene_loss:-1.447686\n",
      "epoch:344 loss:3.803508 gene_loss:-1.446368\n",
      "epoch:345 loss:3.801474 gene_loss:-1.446800\n",
      "epoch:346 loss:3.801497 gene_loss:-1.445892\n",
      "epoch:347 loss:3.801229 gene_loss:-1.436724\n",
      "epoch:348 loss:3.802127 gene_loss:-1.430782\n",
      "epoch:349 loss:3.801180 gene_loss:-1.433602\n",
      "epoch:350 loss:3.801736 gene_loss:-1.427966\n",
      "epoch:351 loss:3.802942 gene_loss:-1.432876\n",
      "epoch:352 loss:3.802167 gene_loss:-1.422081\n",
      "epoch:353 loss:3.797636 gene_loss:-1.420668\n",
      "epoch:354 loss:3.800975 gene_loss:-1.427561\n",
      "epoch:355 loss:3.797350 gene_loss:-1.426504\n",
      "epoch:356 loss:3.797863 gene_loss:-1.428514\n",
      "epoch:357 loss:3.798460 gene_loss:-1.395426\n",
      "epoch:358 loss:3.797107 gene_loss:-1.392509\n",
      "epoch:359 loss:3.795204 gene_loss:-1.378657\n",
      "epoch:360 loss:3.796784 gene_loss:-1.385253\n",
      "epoch:361 loss:3.792332 gene_loss:-1.388414\n",
      "epoch:362 loss:3.789626 gene_loss:-1.357930\n",
      "epoch:363 loss:3.797198 gene_loss:-1.361272\n",
      "epoch:364 loss:3.798148 gene_loss:-1.358185\n",
      "epoch:365 loss:3.795796 gene_loss:-1.345825\n",
      "epoch:366 loss:3.789873 gene_loss:-1.349687\n",
      "epoch:367 loss:3.793930 gene_loss:-1.342282\n",
      "epoch:368 loss:3.798053 gene_loss:-1.357994\n",
      "epoch:369 loss:3.799215 gene_loss:-1.387466\n",
      "epoch:370 loss:3.796216 gene_loss:-1.402176\n",
      "epoch:371 loss:3.796114 gene_loss:-1.419897\n",
      "epoch:372 loss:3.795979 gene_loss:-1.438995\n",
      "epoch:373 loss:3.803533 gene_loss:-1.445397\n",
      "epoch:374 loss:3.797394 gene_loss:-1.441060\n",
      "epoch:375 loss:3.801467 gene_loss:-1.457547\n",
      "epoch:376 loss:3.803271 gene_loss:-1.458072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:377 loss:3.801017 gene_loss:-1.457913\n",
      "epoch:378 loss:3.802518 gene_loss:-1.463113\n",
      "epoch:379 loss:3.800969 gene_loss:-1.455535\n",
      "epoch:380 loss:3.806221 gene_loss:-1.462500\n",
      "epoch:381 loss:3.806056 gene_loss:-1.465640\n",
      "epoch:382 loss:3.805408 gene_loss:-1.464550\n",
      "epoch:383 loss:3.805786 gene_loss:-1.461814\n",
      "epoch:384 loss:3.802516 gene_loss:-1.465362\n",
      "epoch:385 loss:3.807281 gene_loss:-1.467212\n",
      "epoch:386 loss:3.804342 gene_loss:-1.463690\n",
      "epoch:387 loss:3.809145 gene_loss:-1.467850\n",
      "epoch:388 loss:3.802522 gene_loss:-1.462652\n",
      "epoch:389 loss:3.809324 gene_loss:-1.469354\n",
      "epoch:390 loss:3.810121 gene_loss:-1.465284\n",
      "epoch:391 loss:3.807993 gene_loss:-1.468410\n",
      "epoch:392 loss:3.806672 gene_loss:-1.465441\n",
      "epoch:393 loss:3.809218 gene_loss:-1.469035\n",
      "epoch:394 loss:3.808664 gene_loss:-1.466835\n",
      "epoch:395 loss:3.811724 gene_loss:-1.470194\n",
      "epoch:396 loss:3.809163 gene_loss:-1.469512\n",
      "epoch:397 loss:3.809860 gene_loss:-1.466820\n",
      "epoch:398 loss:3.810922 gene_loss:-1.470439\n",
      "epoch:399 loss:3.808951 gene_loss:-1.464538\n",
      "epoch:400 loss:3.808584 gene_loss:-1.470210\n",
      "epoch:401 loss:3.809601 gene_loss:-1.470578\n",
      "epoch:402 loss:3.807806 gene_loss:-1.465382\n",
      "epoch:403 loss:3.810939 gene_loss:-1.469647\n",
      "epoch:404 loss:3.810291 gene_loss:-1.466125\n",
      "epoch:405 loss:3.809946 gene_loss:-1.469888\n",
      "epoch:406 loss:3.810728 gene_loss:-1.468397\n",
      "epoch:407 loss:3.807815 gene_loss:-1.465322\n",
      "epoch:408 loss:3.810333 gene_loss:-1.470204\n",
      "epoch:409 loss:3.812193 gene_loss:-1.468187\n",
      "epoch:410 loss:3.809925 gene_loss:-1.469411\n",
      "epoch:411 loss:3.806525 gene_loss:-1.467579\n",
      "epoch:412 loss:3.812353 gene_loss:-1.469286\n",
      "epoch:413 loss:3.811913 gene_loss:-1.467350\n",
      "epoch:414 loss:3.808903 gene_loss:-1.460889\n",
      "epoch:415 loss:3.812365 gene_loss:-1.468039\n",
      "epoch:416 loss:3.810362 gene_loss:-1.464135\n",
      "epoch:417 loss:3.808744 gene_loss:-1.464093\n",
      "epoch:418 loss:3.807594 gene_loss:-1.458855\n",
      "epoch:419 loss:3.809764 gene_loss:-1.454093\n",
      "epoch:420 loss:3.806230 gene_loss:-1.426877\n",
      "epoch:421 loss:3.802041 gene_loss:-1.323590\n",
      "epoch:422 loss:3.762635 gene_loss:-0.973413\n",
      "epoch:423 loss:3.478844 gene_loss:-0.865372\n",
      "epoch:424 loss:3.581551 gene_loss:-1.162380\n",
      "epoch:425 loss:3.730293 gene_loss:-1.360811\n",
      "epoch:426 loss:3.742976 gene_loss:-1.415027\n",
      "epoch:427 loss:3.775798 gene_loss:-1.440340\n",
      "epoch:428 loss:3.792179 gene_loss:-1.451719\n",
      "epoch:429 loss:3.778127 gene_loss:-1.432086\n",
      "epoch:430 loss:3.797972 gene_loss:-1.461024\n",
      "epoch:431 loss:3.798882 gene_loss:-1.460272\n",
      "epoch:432 loss:3.800871 gene_loss:-1.458772\n",
      "epoch:433 loss:3.801056 gene_loss:-1.459155\n",
      "epoch:434 loss:3.792227 gene_loss:-1.460019\n",
      "epoch:435 loss:3.803142 gene_loss:-1.467705\n",
      "epoch:436 loss:3.806405 gene_loss:-1.466090\n",
      "epoch:437 loss:3.783177 gene_loss:-1.456848\n",
      "epoch:438 loss:3.806258 gene_loss:-1.466617\n",
      "epoch:439 loss:3.805521 gene_loss:-1.467168\n",
      "epoch:440 loss:3.802260 gene_loss:-1.464749\n",
      "epoch:441 loss:3.805332 gene_loss:-1.466676\n",
      "epoch:442 loss:3.805502 gene_loss:-1.468661\n",
      "epoch:443 loss:3.807564 gene_loss:-1.463066\n",
      "epoch:444 loss:3.810510 gene_loss:-1.471615\n",
      "epoch:445 loss:3.810607 gene_loss:-1.469230\n",
      "epoch:446 loss:3.806828 gene_loss:-1.469625\n",
      "epoch:447 loss:3.806931 gene_loss:-1.467462\n",
      "epoch:448 loss:3.811295 gene_loss:-1.472658\n",
      "epoch:449 loss:3.810064 gene_loss:-1.471554\n",
      "epoch:450 loss:3.807329 gene_loss:-1.464053\n",
      "epoch:451 loss:3.811397 gene_loss:-1.472651\n",
      "epoch:452 loss:3.811562 gene_loss:-1.468168\n",
      "epoch:453 loss:3.812463 gene_loss:-1.473376\n",
      "epoch:454 loss:3.812754 gene_loss:-1.473777\n",
      "epoch:455 loss:3.808936 gene_loss:-1.467067\n",
      "epoch:456 loss:3.812076 gene_loss:-1.468916\n",
      "epoch:457 loss:3.812842 gene_loss:-1.473144\n",
      "epoch:458 loss:3.812187 gene_loss:-1.473627\n",
      "epoch:459 loss:3.808435 gene_loss:-1.465179\n",
      "epoch:460 loss:3.813661 gene_loss:-1.474056\n",
      "epoch:461 loss:3.810908 gene_loss:-1.472906\n",
      "epoch:462 loss:3.813103 gene_loss:-1.468409\n",
      "epoch:463 loss:3.814553 gene_loss:-1.473970\n",
      "epoch:464 loss:3.811780 gene_loss:-1.471440\n",
      "epoch:465 loss:3.814988 gene_loss:-1.473421\n",
      "epoch:466 loss:3.812139 gene_loss:-1.472802\n",
      "epoch:467 loss:3.812517 gene_loss:-1.472985\n",
      "epoch:468 loss:3.810974 gene_loss:-1.471710\n",
      "epoch:469 loss:3.814078 gene_loss:-1.471663\n",
      "epoch:470 loss:3.811500 gene_loss:-1.470987\n",
      "epoch:471 loss:3.812784 gene_loss:-1.471239\n",
      "epoch:472 loss:3.811942 gene_loss:-1.470582\n",
      "epoch:473 loss:3.807044 gene_loss:-1.466520\n",
      "epoch:474 loss:3.812519 gene_loss:-1.469149\n",
      "epoch:475 loss:3.811619 gene_loss:-1.467593\n",
      "epoch:476 loss:3.810374 gene_loss:-1.466250\n",
      "epoch:477 loss:3.809420 gene_loss:-1.467731\n",
      "epoch:478 loss:3.807955 gene_loss:-1.466418\n",
      "epoch:479 loss:3.808544 gene_loss:-1.463887\n",
      "epoch:480 loss:3.809004 gene_loss:-1.464327\n",
      "epoch:481 loss:3.809391 gene_loss:-1.464232\n",
      "epoch:482 loss:3.808085 gene_loss:-1.460007\n",
      "epoch:483 loss:3.810126 gene_loss:-1.464071\n",
      "epoch:484 loss:3.809446 gene_loss:-1.462611\n",
      "epoch:485 loss:3.810921 gene_loss:-1.462554\n",
      "epoch:486 loss:3.808551 gene_loss:-1.459369\n",
      "epoch:487 loss:3.810175 gene_loss:-1.461400\n",
      "epoch:488 loss:3.806309 gene_loss:-1.460870\n",
      "epoch:489 loss:3.805163 gene_loss:-1.458575\n",
      "epoch:490 loss:3.808238 gene_loss:-1.457716\n",
      "epoch:491 loss:3.806947 gene_loss:-1.455528\n",
      "epoch:492 loss:3.804751 gene_loss:-1.452825\n",
      "epoch:493 loss:3.802274 gene_loss:-1.451465\n",
      "epoch:494 loss:3.802249 gene_loss:-1.450823\n",
      "epoch:495 loss:3.802227 gene_loss:-1.453934\n",
      "epoch:496 loss:3.787605 gene_loss:-1.450279\n",
      "epoch:497 loss:3.802574 gene_loss:-1.451437\n",
      "epoch:498 loss:3.802876 gene_loss:-1.454360\n",
      "epoch:499 loss:3.803266 gene_loss:-1.455146\n",
      "epoch:500 loss:3.805510 gene_loss:-1.451116\n",
      "epoch:501 loss:3.804435 gene_loss:-1.455823\n",
      "epoch:502 loss:3.806364 gene_loss:-1.457812\n",
      "epoch:503 loss:3.806132 gene_loss:-1.454588\n",
      "epoch:504 loss:3.808984 gene_loss:-1.457091\n",
      "epoch:505 loss:3.807419 gene_loss:-1.453946\n",
      "epoch:506 loss:3.806785 gene_loss:-1.454482\n",
      "epoch:507 loss:3.810213 gene_loss:-1.452991\n",
      "epoch:508 loss:3.807269 gene_loss:-1.458499\n",
      "epoch:509 loss:3.808095 gene_loss:-1.459839\n",
      "epoch:510 loss:3.809124 gene_loss:-1.461022\n",
      "epoch:511 loss:3.812011 gene_loss:-1.463951\n",
      "epoch:512 loss:3.809472 gene_loss:-1.460449\n",
      "epoch:513 loss:3.808802 gene_loss:-1.466013\n",
      "epoch:514 loss:3.811111 gene_loss:-1.465691\n",
      "epoch:515 loss:3.812031 gene_loss:-1.467623\n",
      "epoch:516 loss:3.807443 gene_loss:-1.461229\n",
      "epoch:517 loss:3.811974 gene_loss:-1.468153\n",
      "epoch:518 loss:3.810012 gene_loss:-1.468758\n",
      "epoch:519 loss:3.810381 gene_loss:-1.468246\n",
      "epoch:520 loss:3.809588 gene_loss:-1.468038\n",
      "epoch:521 loss:3.811028 gene_loss:-1.461349\n",
      "epoch:522 loss:3.810998 gene_loss:-1.469333\n",
      "epoch:523 loss:3.809979 gene_loss:-1.463023\n",
      "epoch:524 loss:3.812913 gene_loss:-1.468751\n",
      "epoch:525 loss:3.811977 gene_loss:-1.467416\n",
      "epoch:526 loss:3.806575 gene_loss:-1.466256\n",
      "epoch:527 loss:3.807797 gene_loss:-1.466115\n",
      "epoch:528 loss:3.808954 gene_loss:-1.466124\n",
      "epoch:529 loss:3.808046 gene_loss:-1.461353\n",
      "epoch:530 loss:3.800510 gene_loss:-1.462944\n",
      "epoch:531 loss:3.804414 gene_loss:-1.459307\n",
      "epoch:532 loss:3.806110 gene_loss:-1.457096\n",
      "epoch:533 loss:3.803698 gene_loss:-1.455047\n",
      "epoch:534 loss:3.801297 gene_loss:-1.452511\n",
      "epoch:535 loss:3.799586 gene_loss:-1.446650\n",
      "epoch:536 loss:3.796140 gene_loss:-1.444776\n",
      "epoch:537 loss:3.792238 gene_loss:-1.440514\n",
      "epoch:538 loss:3.789584 gene_loss:-1.436527\n",
      "epoch:539 loss:3.790531 gene_loss:-1.418379\n",
      "epoch:540 loss:3.781837 gene_loss:-1.409625\n",
      "epoch:541 loss:3.784960 gene_loss:-1.395477\n",
      "epoch:542 loss:3.779840 gene_loss:-1.397153\n",
      "epoch:543 loss:3.779708 gene_loss:-1.384650\n",
      "epoch:544 loss:3.783565 gene_loss:-1.378206\n",
      "epoch:545 loss:3.788211 gene_loss:-1.367096\n",
      "epoch:546 loss:3.786657 gene_loss:-1.363357\n",
      "epoch:547 loss:3.790874 gene_loss:-1.395685\n",
      "epoch:548 loss:3.785728 gene_loss:-1.379174\n",
      "epoch:549 loss:3.794040 gene_loss:-1.342606\n",
      "epoch:550 loss:3.792108 gene_loss:-1.353904\n",
      "epoch:551 loss:3.788278 gene_loss:-1.391150\n",
      "epoch:552 loss:3.793314 gene_loss:-1.400942\n",
      "epoch:553 loss:3.794532 gene_loss:-1.397853\n",
      "epoch:554 loss:3.799513 gene_loss:-1.384308\n",
      "epoch:555 loss:3.804792 gene_loss:-1.394548\n",
      "epoch:556 loss:3.798630 gene_loss:-1.417405\n",
      "epoch:557 loss:3.799943 gene_loss:-1.417951\n",
      "epoch:558 loss:3.802042 gene_loss:-1.438580\n",
      "epoch:559 loss:3.800769 gene_loss:-1.442994\n",
      "epoch:560 loss:3.798546 gene_loss:-1.433012\n",
      "epoch:561 loss:3.803968 gene_loss:-1.451694\n",
      "epoch:562 loss:3.803253 gene_loss:-1.458284\n",
      "epoch:563 loss:3.804242 gene_loss:-1.447930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:564 loss:3.806898 gene_loss:-1.453367\n",
      "epoch:565 loss:3.805144 gene_loss:-1.456576\n",
      "epoch:566 loss:3.803969 gene_loss:-1.462035\n",
      "epoch:567 loss:3.800801 gene_loss:-1.444471\n",
      "epoch:568 loss:3.807053 gene_loss:-1.463873\n",
      "epoch:569 loss:3.807825 gene_loss:-1.461736\n",
      "epoch:570 loss:3.799843 gene_loss:-1.461870\n",
      "epoch:571 loss:3.802947 gene_loss:-1.461673\n",
      "epoch:572 loss:3.807482 gene_loss:-1.463588\n",
      "epoch:573 loss:3.793334 gene_loss:-1.448540\n",
      "epoch:574 loss:3.808275 gene_loss:-1.467096\n",
      "epoch:575 loss:3.809508 gene_loss:-1.466359\n",
      "epoch:576 loss:3.805493 gene_loss:-1.459304\n",
      "epoch:577 loss:3.797812 gene_loss:-1.449726\n",
      "epoch:578 loss:3.806082 gene_loss:-1.467113\n",
      "epoch:579 loss:3.806259 gene_loss:-1.466015\n",
      "epoch:580 loss:3.788645 gene_loss:-1.426539\n",
      "epoch:581 loss:3.798097 gene_loss:-1.461115\n",
      "epoch:582 loss:3.802943 gene_loss:-1.463788\n",
      "epoch:583 loss:3.801775 gene_loss:-1.463731\n",
      "epoch:584 loss:3.801646 gene_loss:-1.462528\n",
      "epoch:585 loss:3.797191 gene_loss:-1.457582\n",
      "epoch:586 loss:3.803156 gene_loss:-1.462825\n",
      "epoch:587 loss:3.802771 gene_loss:-1.462010\n",
      "epoch:588 loss:3.798552 gene_loss:-1.449248\n",
      "epoch:589 loss:3.791139 gene_loss:-1.457781\n",
      "epoch:590 loss:3.790360 gene_loss:-1.455384\n",
      "epoch:591 loss:3.791133 gene_loss:-1.457239\n",
      "epoch:592 loss:3.791744 gene_loss:-1.453509\n",
      "epoch:593 loss:3.790772 gene_loss:-1.448592\n",
      "epoch:594 loss:3.790119 gene_loss:-1.443173\n",
      "epoch:595 loss:3.786067 gene_loss:-1.445549\n",
      "epoch:596 loss:3.784101 gene_loss:-1.439263\n",
      "epoch:597 loss:3.790317 gene_loss:-1.443855\n",
      "epoch:598 loss:3.785713 gene_loss:-1.442669\n",
      "epoch:599 loss:3.787293 gene_loss:-1.442222\n",
      "epoch:600 loss:3.787100 gene_loss:-1.441318\n",
      "epoch:601 loss:3.787921 gene_loss:-1.438919\n",
      "epoch:602 loss:3.774770 gene_loss:-1.439072\n",
      "epoch:603 loss:3.775160 gene_loss:-1.428676\n",
      "epoch:604 loss:3.775787 gene_loss:-1.439100\n",
      "epoch:605 loss:3.776689 gene_loss:-1.425355\n",
      "epoch:606 loss:3.764596 gene_loss:-1.406394\n",
      "epoch:607 loss:3.757731 gene_loss:-1.396307\n",
      "epoch:608 loss:3.738503 gene_loss:-1.363322\n",
      "epoch:609 loss:3.715271 gene_loss:-1.338148\n",
      "epoch:610 loss:3.678931 gene_loss:-1.334221\n",
      "epoch:611 loss:3.714850 gene_loss:-1.319256\n",
      "epoch:612 loss:3.681361 gene_loss:-1.312602\n",
      "epoch:613 loss:3.718761 gene_loss:-1.314754\n",
      "epoch:614 loss:3.682650 gene_loss:-1.363741\n",
      "epoch:615 loss:3.725690 gene_loss:-1.349786\n",
      "epoch:616 loss:3.726209 gene_loss:-1.330971\n",
      "epoch:617 loss:3.746121 gene_loss:-1.322982\n",
      "epoch:618 loss:3.737923 gene_loss:-1.328917\n",
      "epoch:619 loss:3.731503 gene_loss:-1.340022\n",
      "epoch:620 loss:3.730609 gene_loss:-1.368007\n",
      "epoch:621 loss:3.731773 gene_loss:-1.331991\n",
      "epoch:622 loss:3.724676 gene_loss:-1.366811\n",
      "epoch:623 loss:3.738247 gene_loss:-1.343471\n",
      "epoch:624 loss:3.702375 gene_loss:-1.355873\n",
      "epoch:625 loss:3.685091 gene_loss:-1.375329\n",
      "epoch:626 loss:3.743128 gene_loss:-1.385572\n",
      "epoch:627 loss:3.722312 gene_loss:-1.403656\n",
      "epoch:628 loss:3.763082 gene_loss:-1.422003\n",
      "epoch:629 loss:3.751063 gene_loss:-1.415628\n",
      "epoch:630 loss:3.765297 gene_loss:-1.437701\n",
      "epoch:631 loss:3.769837 gene_loss:-1.436804\n",
      "epoch:632 loss:3.766673 gene_loss:-1.423746\n",
      "epoch:633 loss:3.777617 gene_loss:-1.424640\n",
      "epoch:634 loss:3.779671 gene_loss:-1.430356\n",
      "epoch:635 loss:3.788169 gene_loss:-1.441236\n",
      "epoch:636 loss:3.784151 gene_loss:-1.432058\n",
      "epoch:637 loss:3.785510 gene_loss:-1.435031\n",
      "epoch:638 loss:3.790434 gene_loss:-1.447286\n",
      "epoch:639 loss:3.795120 gene_loss:-1.440816\n",
      "epoch:640 loss:3.793273 gene_loss:-1.437946\n",
      "epoch:641 loss:3.796907 gene_loss:-1.440243\n",
      "epoch:642 loss:3.796477 gene_loss:-1.423697\n",
      "epoch:643 loss:3.800597 gene_loss:-1.441014\n",
      "epoch:644 loss:3.800224 gene_loss:-1.437758\n",
      "epoch:645 loss:3.796119 gene_loss:-1.431715\n",
      "epoch:646 loss:3.799215 gene_loss:-1.422401\n",
      "epoch:647 loss:3.801086 gene_loss:-1.368611\n",
      "epoch:648 loss:3.801492 gene_loss:-1.249989\n",
      "epoch:649 loss:3.793880 gene_loss:-1.051154\n",
      "epoch:650 loss:3.773288 gene_loss:0.809860\n",
      "epoch:651 loss:3.760129 gene_loss:2.787801\n",
      "epoch:652 loss:3.776691 gene_loss:3.695201\n",
      "epoch:653 loss:3.779477 gene_loss:3.862596\n",
      "epoch:654 loss:3.771554 gene_loss:3.747455\n",
      "epoch:655 loss:3.772312 gene_loss:3.564461\n",
      "epoch:656 loss:3.764826 gene_loss:2.212163\n",
      "epoch:657 loss:3.733815 gene_loss:3.593504\n",
      "epoch:658 loss:3.771874 gene_loss:3.858304\n",
      "epoch:659 loss:3.761497 gene_loss:3.675471\n",
      "epoch:660 loss:3.766714 gene_loss:2.411943\n",
      "epoch:661 loss:3.770209 gene_loss:1.298477\n",
      "epoch:662 loss:3.757162 gene_loss:2.457674\n",
      "epoch:663 loss:3.748787 gene_loss:0.789049\n",
      "epoch:664 loss:3.722771 gene_loss:0.981399\n",
      "epoch:665 loss:3.719001 gene_loss:0.680748\n",
      "epoch:666 loss:3.682515 gene_loss:-1.080801\n",
      "epoch:667 loss:3.755416 gene_loss:-1.116573\n",
      "epoch:668 loss:3.740132 gene_loss:-1.221843\n",
      "epoch:669 loss:3.764294 gene_loss:-1.382722\n",
      "epoch:670 loss:3.752203 gene_loss:-1.350270\n",
      "epoch:671 loss:3.756111 gene_loss:-1.405982\n",
      "epoch:672 loss:3.750445 gene_loss:-1.426960\n",
      "epoch:673 loss:3.787977 gene_loss:-1.440427\n",
      "epoch:674 loss:3.752480 gene_loss:-1.417896\n",
      "epoch:675 loss:3.795930 gene_loss:-1.453096\n",
      "epoch:676 loss:3.791843 gene_loss:-1.436461\n",
      "epoch:677 loss:3.762102 gene_loss:-1.421534\n",
      "epoch:678 loss:3.798451 gene_loss:-1.457572\n",
      "epoch:679 loss:3.792318 gene_loss:-1.452771\n",
      "epoch:680 loss:3.801783 gene_loss:-1.454903\n",
      "epoch:681 loss:3.790409 gene_loss:-1.462778\n",
      "epoch:682 loss:3.798810 gene_loss:-1.439746\n",
      "epoch:683 loss:3.803647 gene_loss:-1.459489\n",
      "epoch:684 loss:3.804434 gene_loss:-1.465915\n",
      "epoch:685 loss:3.804812 gene_loss:-1.466376\n",
      "epoch:686 loss:3.792640 gene_loss:-1.450363\n",
      "epoch:687 loss:3.803163 gene_loss:-1.470124\n",
      "epoch:688 loss:3.807742 gene_loss:-1.459003\n",
      "epoch:689 loss:3.805300 gene_loss:-1.468290\n",
      "epoch:690 loss:3.807045 gene_loss:-1.468181\n",
      "epoch:691 loss:3.805929 gene_loss:-1.469538\n",
      "epoch:692 loss:3.804566 gene_loss:-1.458469\n",
      "epoch:693 loss:3.809989 gene_loss:-1.469940\n",
      "epoch:694 loss:3.805210 gene_loss:-1.466017\n",
      "epoch:695 loss:3.806637 gene_loss:-1.466796\n",
      "epoch:696 loss:3.800375 gene_loss:-1.466645\n",
      "epoch:697 loss:3.811315 gene_loss:-1.471077\n",
      "epoch:698 loss:3.801930 gene_loss:-1.440709\n",
      "epoch:699 loss:3.812690 gene_loss:-1.474518\n",
      "epoch:700 loss:3.809900 gene_loss:-1.471232\n",
      "epoch:701 loss:3.807443 gene_loss:-1.473413\n",
      "epoch:702 loss:3.811510 gene_loss:-1.470586\n",
      "epoch:703 loss:3.808959 gene_loss:-1.469432\n",
      "epoch:704 loss:3.810621 gene_loss:-1.474449\n",
      "epoch:705 loss:3.813788 gene_loss:-1.470700\n",
      "epoch:706 loss:3.810039 gene_loss:-1.464960\n",
      "epoch:707 loss:3.811415 gene_loss:-1.473471\n",
      "epoch:708 loss:3.812606 gene_loss:-1.471708\n",
      "epoch:709 loss:3.810609 gene_loss:-1.468694\n",
      "epoch:710 loss:3.812766 gene_loss:-1.475761\n",
      "epoch:711 loss:3.810977 gene_loss:-1.467398\n",
      "epoch:712 loss:3.813095 gene_loss:-1.472988\n",
      "epoch:713 loss:3.812386 gene_loss:-1.470388\n",
      "epoch:714 loss:3.816007 gene_loss:-1.475715\n",
      "epoch:715 loss:3.815928 gene_loss:-1.476482\n",
      "epoch:716 loss:3.806674 gene_loss:-1.469362\n",
      "epoch:717 loss:3.813875 gene_loss:-1.474574\n",
      "epoch:718 loss:3.815970 gene_loss:-1.475292\n",
      "epoch:719 loss:3.815499 gene_loss:-1.476008\n",
      "epoch:720 loss:3.814479 gene_loss:-1.472442\n",
      "epoch:721 loss:3.813304 gene_loss:-1.474525\n",
      "epoch:722 loss:3.814374 gene_loss:-1.475471\n",
      "epoch:723 loss:3.816257 gene_loss:-1.476771\n",
      "epoch:724 loss:3.813090 gene_loss:-1.460188\n",
      "epoch:725 loss:3.817194 gene_loss:-1.477916\n",
      "epoch:726 loss:3.814814 gene_loss:-1.472716\n",
      "epoch:727 loss:3.816735 gene_loss:-1.477266\n",
      "epoch:728 loss:3.817643 gene_loss:-1.476799\n",
      "epoch:729 loss:3.817128 gene_loss:-1.475079\n",
      "epoch:730 loss:3.816233 gene_loss:-1.477578\n",
      "epoch:731 loss:3.815096 gene_loss:-1.476826\n",
      "epoch:732 loss:3.814106 gene_loss:-1.475258\n",
      "epoch:733 loss:3.815178 gene_loss:-1.477861\n",
      "epoch:734 loss:3.816237 gene_loss:-1.472747\n",
      "epoch:735 loss:3.816658 gene_loss:-1.476203\n",
      "epoch:736 loss:3.816209 gene_loss:-1.472725\n",
      "epoch:737 loss:3.816939 gene_loss:-1.474892\n",
      "epoch:738 loss:3.815842 gene_loss:-1.477662\n",
      "epoch:739 loss:3.815795 gene_loss:-1.476064\n",
      "epoch:740 loss:3.816797 gene_loss:-1.477928\n",
      "epoch:741 loss:3.816433 gene_loss:-1.472965\n",
      "epoch:742 loss:3.817517 gene_loss:-1.478091\n",
      "epoch:743 loss:3.815863 gene_loss:-1.476045\n",
      "epoch:744 loss:3.814921 gene_loss:-1.477552\n",
      "epoch:745 loss:3.816469 gene_loss:-1.473877\n",
      "epoch:746 loss:3.817394 gene_loss:-1.477820\n",
      "epoch:747 loss:3.815436 gene_loss:-1.459991\n",
      "epoch:748 loss:3.816916 gene_loss:-1.477173\n",
      "epoch:749 loss:3.816360 gene_loss:-1.477289\n",
      "epoch:750 loss:3.817605 gene_loss:-1.475795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:751 loss:3.814669 gene_loss:-1.475324\n",
      "epoch:752 loss:3.816029 gene_loss:-1.472886\n",
      "epoch:753 loss:3.816287 gene_loss:-1.475584\n",
      "epoch:754 loss:3.814265 gene_loss:-1.470305\n",
      "epoch:755 loss:3.813953 gene_loss:-1.466035\n",
      "epoch:756 loss:3.811086 gene_loss:-1.459377\n",
      "epoch:757 loss:3.812134 gene_loss:-1.444570\n",
      "epoch:758 loss:3.810899 gene_loss:-1.429932\n",
      "epoch:759 loss:3.809475 gene_loss:-1.427191\n",
      "epoch:760 loss:3.807673 gene_loss:-1.431852\n",
      "epoch:761 loss:3.807743 gene_loss:-1.416010\n",
      "epoch:762 loss:3.803235 gene_loss:-1.386970\n",
      "epoch:763 loss:3.794336 gene_loss:-1.268582\n",
      "epoch:764 loss:3.793496 gene_loss:1.222467\n",
      "epoch:765 loss:3.699900 gene_loss:2.037062\n",
      "epoch:766 loss:3.768307 gene_loss:2.956992\n",
      "epoch:767 loss:3.773214 gene_loss:3.725720\n",
      "epoch:768 loss:3.787179 gene_loss:3.778543\n",
      "epoch:769 loss:3.780385 gene_loss:3.257481\n",
      "epoch:770 loss:3.772239 gene_loss:3.242775\n",
      "epoch:771 loss:3.774400 gene_loss:3.496365\n",
      "epoch:772 loss:3.767679 gene_loss:3.846582\n",
      "epoch:773 loss:3.774600 gene_loss:3.540165\n",
      "epoch:774 loss:3.760538 gene_loss:1.343095\n",
      "epoch:775 loss:3.611834 gene_loss:1.653849\n",
      "epoch:776 loss:3.721817 gene_loss:3.567609\n",
      "epoch:777 loss:3.755222 gene_loss:3.468486\n",
      "epoch:778 loss:3.740576 gene_loss:-0.295584\n",
      "epoch:779 loss:3.722093 gene_loss:2.684886\n",
      "epoch:780 loss:3.745549 gene_loss:3.148640\n",
      "epoch:781 loss:3.708889 gene_loss:1.358345\n",
      "epoch:782 loss:3.740794 gene_loss:3.462085\n",
      "epoch:783 loss:3.734425 gene_loss:2.307476\n",
      "epoch:784 loss:3.737519 gene_loss:1.973735\n",
      "epoch:785 loss:3.742589 gene_loss:1.181342\n",
      "epoch:786 loss:3.739963 gene_loss:-0.708465\n",
      "epoch:787 loss:3.749747 gene_loss:2.043757\n",
      "epoch:788 loss:3.756974 gene_loss:-0.537451\n",
      "epoch:789 loss:3.753703 gene_loss:1.607026\n",
      "epoch:790 loss:3.758797 gene_loss:0.060470\n",
      "epoch:791 loss:3.754664 gene_loss:-1.053158\n",
      "epoch:792 loss:3.768390 gene_loss:-0.567993\n",
      "epoch:793 loss:3.751160 gene_loss:-1.314301\n",
      "epoch:794 loss:3.765413 gene_loss:-1.265246\n",
      "epoch:795 loss:3.770830 gene_loss:-1.321029\n",
      "epoch:796 loss:3.778069 gene_loss:-1.375532\n",
      "epoch:797 loss:3.765579 gene_loss:-1.387925\n",
      "epoch:798 loss:3.778506 gene_loss:-1.417891\n",
      "epoch:799 loss:3.781543 gene_loss:-1.435049\n",
      "epoch:800 loss:3.793495 gene_loss:-1.437951\n",
      "epoch:801 loss:3.784204 gene_loss:-1.458241\n",
      "epoch:802 loss:3.803154 gene_loss:-1.459436\n",
      "epoch:803 loss:3.784108 gene_loss:-1.453763\n",
      "epoch:804 loss:3.803229 gene_loss:-1.465212\n",
      "epoch:805 loss:3.800514 gene_loss:-1.450193\n",
      "epoch:806 loss:3.801080 gene_loss:-1.459208\n",
      "epoch:807 loss:3.811771 gene_loss:-1.472852\n",
      "epoch:808 loss:3.809067 gene_loss:-1.464612\n",
      "epoch:809 loss:3.810580 gene_loss:-1.469418\n",
      "epoch:810 loss:3.809722 gene_loss:-1.467726\n",
      "epoch:811 loss:3.810887 gene_loss:-1.469686\n",
      "epoch:812 loss:3.807672 gene_loss:-1.474601\n",
      "epoch:813 loss:3.810212 gene_loss:-1.467533\n",
      "epoch:814 loss:3.813644 gene_loss:-1.474890\n",
      "epoch:815 loss:3.810641 gene_loss:-1.473519\n",
      "epoch:816 loss:3.813290 gene_loss:-1.471870\n",
      "epoch:817 loss:3.810615 gene_loss:-1.459538\n",
      "epoch:818 loss:3.815564 gene_loss:-1.477558\n",
      "epoch:819 loss:3.812251 gene_loss:-1.472357\n",
      "epoch:820 loss:3.811714 gene_loss:-1.473371\n",
      "epoch:821 loss:3.815193 gene_loss:-1.477470\n",
      "epoch:822 loss:3.811231 gene_loss:-1.461463\n",
      "epoch:823 loss:3.816844 gene_loss:-1.479038\n",
      "epoch:824 loss:3.817029 gene_loss:-1.476897\n",
      "epoch:825 loss:3.809659 gene_loss:-1.476285\n",
      "epoch:826 loss:3.816514 gene_loss:-1.479342\n",
      "epoch:827 loss:3.814010 gene_loss:-1.475769\n",
      "epoch:828 loss:3.815066 gene_loss:-1.477659\n",
      "epoch:829 loss:3.815693 gene_loss:-1.479607\n",
      "epoch:830 loss:3.815706 gene_loss:-1.476174\n",
      "epoch:831 loss:3.816263 gene_loss:-1.477675\n",
      "epoch:832 loss:3.818405 gene_loss:-1.480391\n",
      "epoch:833 loss:3.813949 gene_loss:-1.475899\n",
      "epoch:834 loss:3.817491 gene_loss:-1.480651\n",
      "epoch:835 loss:3.817763 gene_loss:-1.477641\n",
      "epoch:836 loss:3.815907 gene_loss:-1.479560\n",
      "epoch:837 loss:3.816345 gene_loss:-1.476986\n",
      "epoch:838 loss:3.816867 gene_loss:-1.479527\n",
      "epoch:839 loss:3.817210 gene_loss:-1.480201\n",
      "epoch:840 loss:3.816883 gene_loss:-1.480246\n",
      "epoch:841 loss:3.818797 gene_loss:-1.479546\n",
      "epoch:842 loss:3.819992 gene_loss:-1.480878\n",
      "epoch:843 loss:3.814324 gene_loss:-1.476606\n",
      "epoch:844 loss:3.819950 gene_loss:-1.481654\n",
      "epoch:845 loss:3.814085 gene_loss:-1.479114\n",
      "epoch:846 loss:3.815564 gene_loss:-1.479610\n",
      "epoch:847 loss:3.818238 gene_loss:-1.480491\n",
      "epoch:848 loss:3.816642 gene_loss:-1.478850\n",
      "epoch:849 loss:3.818573 gene_loss:-1.480783\n",
      "epoch:850 loss:3.818820 gene_loss:-1.480462\n",
      "epoch:851 loss:3.818281 gene_loss:-1.479494\n",
      "epoch:852 loss:3.818640 gene_loss:-1.481629\n",
      "epoch:853 loss:3.819917 gene_loss:-1.481189\n",
      "epoch:854 loss:3.821390 gene_loss:-1.481572\n",
      "epoch:855 loss:3.819411 gene_loss:-1.480459\n",
      "epoch:856 loss:3.818115 gene_loss:-1.481657\n",
      "epoch:857 loss:3.818890 gene_loss:-1.480927\n",
      "epoch:858 loss:3.818820 gene_loss:-1.482208\n",
      "epoch:859 loss:3.819716 gene_loss:-1.481534\n",
      "epoch:860 loss:3.816899 gene_loss:-1.480788\n",
      "epoch:861 loss:3.819319 gene_loss:-1.479746\n",
      "epoch:862 loss:3.820357 gene_loss:-1.482649\n",
      "epoch:863 loss:3.819491 gene_loss:-1.480586\n",
      "epoch:864 loss:3.818810 gene_loss:-1.482498\n",
      "epoch:865 loss:3.818999 gene_loss:-1.480613\n",
      "epoch:866 loss:3.821891 gene_loss:-1.482687\n",
      "epoch:867 loss:3.818663 gene_loss:-1.480057\n",
      "epoch:868 loss:3.820828 gene_loss:-1.482195\n",
      "epoch:869 loss:3.819753 gene_loss:-1.482110\n",
      "epoch:870 loss:3.819381 gene_loss:-1.480051\n",
      "epoch:871 loss:3.820065 gene_loss:-1.482879\n",
      "epoch:872 loss:3.820481 gene_loss:-1.481540\n",
      "epoch:873 loss:3.819337 gene_loss:-1.479045\n",
      "epoch:874 loss:3.820767 gene_loss:-1.483159\n",
      "epoch:875 loss:3.821320 gene_loss:-1.480695\n",
      "epoch:876 loss:3.819574 gene_loss:-1.483088\n",
      "epoch:877 loss:3.819460 gene_loss:-1.481208\n",
      "epoch:878 loss:3.820631 gene_loss:-1.482449\n",
      "epoch:879 loss:3.819113 gene_loss:-1.481147\n",
      "epoch:880 loss:3.820307 gene_loss:-1.482172\n",
      "epoch:881 loss:3.822118 gene_loss:-1.482854\n",
      "epoch:882 loss:3.819542 gene_loss:-1.480641\n",
      "epoch:883 loss:3.821048 gene_loss:-1.483108\n",
      "epoch:884 loss:3.820823 gene_loss:-1.481002\n",
      "epoch:885 loss:3.820863 gene_loss:-1.482854\n",
      "epoch:886 loss:3.819934 gene_loss:-1.479773\n",
      "epoch:887 loss:3.821918 gene_loss:-1.482945\n",
      "epoch:888 loss:3.821504 gene_loss:-1.481046\n",
      "epoch:889 loss:3.821857 gene_loss:-1.482473\n",
      "epoch:890 loss:3.820551 gene_loss:-1.482124\n",
      "epoch:891 loss:3.821842 gene_loss:-1.482241\n",
      "epoch:892 loss:3.819547 gene_loss:-1.481437\n",
      "epoch:893 loss:3.819625 gene_loss:-1.482437\n",
      "epoch:894 loss:3.821420 gene_loss:-1.480319\n",
      "epoch:895 loss:3.820149 gene_loss:-1.482225\n",
      "epoch:896 loss:3.819325 gene_loss:-1.479767\n",
      "epoch:897 loss:3.821357 gene_loss:-1.481274\n",
      "epoch:898 loss:3.819810 gene_loss:-1.481336\n",
      "epoch:899 loss:3.819516 gene_loss:-1.478734\n",
      "epoch:900 loss:3.820536 gene_loss:-1.480652\n",
      "epoch:901 loss:3.821155 gene_loss:-1.479273\n",
      "epoch:902 loss:3.820368 gene_loss:-1.478745\n",
      "epoch:903 loss:3.817913 gene_loss:-1.478687\n",
      "epoch:904 loss:3.817300 gene_loss:-1.477906\n",
      "epoch:905 loss:3.818316 gene_loss:-1.475483\n",
      "epoch:906 loss:3.819482 gene_loss:-1.476316\n",
      "epoch:907 loss:3.817389 gene_loss:-1.471302\n",
      "epoch:908 loss:3.817544 gene_loss:-1.472237\n",
      "epoch:909 loss:3.816059 gene_loss:-1.462207\n",
      "epoch:910 loss:3.815042 gene_loss:-1.428681\n",
      "epoch:911 loss:3.808176 gene_loss:-0.839001\n",
      "epoch:912 loss:3.789558 gene_loss:3.362504\n",
      "epoch:913 loss:3.799912 gene_loss:3.764116\n",
      "epoch:914 loss:3.795694 gene_loss:3.961431\n",
      "epoch:915 loss:3.744851 gene_loss:3.983170\n",
      "epoch:916 loss:3.796828 gene_loss:3.903622\n",
      "epoch:917 loss:3.794377 gene_loss:3.904348\n",
      "epoch:918 loss:3.792504 gene_loss:3.918114\n",
      "epoch:919 loss:3.795598 gene_loss:3.894447\n",
      "epoch:920 loss:3.774704 gene_loss:3.856027\n",
      "epoch:921 loss:3.798419 gene_loss:3.931805\n",
      "epoch:922 loss:3.754229 gene_loss:3.821689\n",
      "epoch:923 loss:3.752408 gene_loss:3.820466\n",
      "epoch:924 loss:3.775287 gene_loss:3.923500\n",
      "epoch:925 loss:3.763357 gene_loss:3.967348\n",
      "epoch:926 loss:3.661676 gene_loss:3.919945\n",
      "epoch:927 loss:3.605784 gene_loss:3.885256\n",
      "epoch:928 loss:3.632673 gene_loss:3.674475\n",
      "epoch:929 loss:1.976235 gene_loss:2.369802\n",
      "epoch:930 loss:1.964311 gene_loss:2.835082\n",
      "epoch:931 loss:2.117929 gene_loss:1.886040\n",
      "epoch:932 loss:3.155254 gene_loss:1.575716\n",
      "epoch:933 loss:3.343170 gene_loss:0.492360\n",
      "epoch:934 loss:3.462489 gene_loss:0.878025\n",
      "epoch:935 loss:3.624269 gene_loss:0.677876\n",
      "epoch:936 loss:3.626595 gene_loss:1.977325\n",
      "epoch:937 loss:3.700166 gene_loss:2.753667\n",
      "epoch:938 loss:3.683406 gene_loss:2.701901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:939 loss:3.725879 gene_loss:3.000499\n",
      "epoch:940 loss:3.740181 gene_loss:3.597590\n",
      "epoch:941 loss:3.732969 gene_loss:3.522485\n",
      "epoch:942 loss:3.743046 gene_loss:2.758092\n",
      "epoch:943 loss:3.755360 gene_loss:3.606364\n",
      "epoch:944 loss:3.765016 gene_loss:3.522363\n",
      "epoch:945 loss:3.763556 gene_loss:1.974002\n",
      "epoch:946 loss:3.760409 gene_loss:3.297387\n",
      "epoch:947 loss:3.765330 gene_loss:3.395870\n",
      "epoch:948 loss:3.780151 gene_loss:3.255010\n",
      "epoch:949 loss:3.745528 gene_loss:-0.247847\n",
      "epoch:950 loss:3.769017 gene_loss:-0.377298\n",
      "epoch:951 loss:3.755682 gene_loss:1.465835\n",
      "epoch:952 loss:3.774311 gene_loss:1.985044\n",
      "epoch:953 loss:3.723406 gene_loss:-0.461999\n",
      "epoch:954 loss:3.759938 gene_loss:3.057737\n",
      "epoch:955 loss:3.689037 gene_loss:-1.068154\n",
      "epoch:956 loss:3.767183 gene_loss:0.205434\n",
      "epoch:957 loss:3.752760 gene_loss:-1.068387\n",
      "epoch:958 loss:3.758076 gene_loss:0.037460\n",
      "epoch:959 loss:3.752205 gene_loss:-0.967462\n",
      "epoch:960 loss:3.770689 gene_loss:1.585284\n",
      "epoch:961 loss:3.747769 gene_loss:-0.318208\n",
      "epoch:962 loss:3.755242 gene_loss:2.451458\n",
      "epoch:963 loss:3.740620 gene_loss:-0.198701\n",
      "epoch:964 loss:3.759322 gene_loss:3.553955\n",
      "epoch:965 loss:3.756460 gene_loss:3.228112\n",
      "epoch:966 loss:3.745763 gene_loss:-1.106067\n",
      "epoch:967 loss:3.753415 gene_loss:1.221302\n",
      "epoch:968 loss:3.725967 gene_loss:0.821376\n",
      "epoch:969 loss:3.746178 gene_loss:1.063522\n",
      "epoch:970 loss:3.751682 gene_loss:-0.820472\n",
      "epoch:971 loss:3.749931 gene_loss:-0.617294\n",
      "epoch:972 loss:3.739214 gene_loss:-1.309702\n",
      "epoch:973 loss:3.756635 gene_loss:-1.295835\n",
      "epoch:974 loss:3.763622 gene_loss:-1.337943\n",
      "epoch:975 loss:3.772699 gene_loss:-1.364680\n",
      "epoch:976 loss:3.767638 gene_loss:-1.380760\n",
      "epoch:977 loss:3.779193 gene_loss:-1.436834\n",
      "epoch:978 loss:3.776841 gene_loss:-1.369894\n",
      "epoch:979 loss:3.781042 gene_loss:-1.447380\n",
      "epoch:980 loss:3.797623 gene_loss:-1.436574\n",
      "epoch:981 loss:3.800123 gene_loss:-1.459374\n",
      "epoch:982 loss:3.785951 gene_loss:-1.442628\n",
      "epoch:983 loss:3.801894 gene_loss:-1.465720\n",
      "epoch:984 loss:3.805024 gene_loss:-1.458787\n",
      "epoch:985 loss:3.800975 gene_loss:-1.452627\n",
      "epoch:986 loss:3.809773 gene_loss:-1.471171\n",
      "epoch:987 loss:3.804437 gene_loss:-1.464293\n",
      "epoch:988 loss:3.800262 gene_loss:-1.460313\n",
      "epoch:989 loss:3.808854 gene_loss:-1.469669\n",
      "epoch:990 loss:3.809346 gene_loss:-1.465010\n",
      "epoch:991 loss:3.811584 gene_loss:-1.466198\n",
      "epoch:992 loss:3.809735 gene_loss:-1.468854\n",
      "epoch:993 loss:3.806614 gene_loss:-1.470507\n",
      "epoch:994 loss:3.807702 gene_loss:-1.461243\n",
      "epoch:995 loss:3.814532 gene_loss:-1.476314\n",
      "epoch:996 loss:3.813382 gene_loss:-1.471675\n",
      "epoch:997 loss:3.807430 gene_loss:-1.475157\n",
      "epoch:998 loss:3.814173 gene_loss:-1.474195\n",
      "epoch:999 loss:3.814605 gene_loss:-1.476064\n"
     ]
    }
   ],
   "source": [
    "real_labels = -np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.ones(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(1000):\n",
    "    #============================\n",
    "    #训练一次G就要训练N_CRITIC次D（Discriminator）\n",
    "    #===\n",
    "    critic_i.trainable = True\n",
    "    for layer in critic_i.layers:\n",
    "        layer.trainable = True\n",
    "    #===\n",
    "    for _ in range(N_CRITIC):\n",
    "        \n",
    "        noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "\n",
    "        real_image = load_image()\n",
    "        #训练判别器\n",
    "        fake_image = generator_i.predict(noise)\n",
    "\n",
    "        real_loss = critic_i.train_on_batch(real_image , real_labels)\n",
    "        fake_loss = critic_i.train_on_batch(fake_image , fake_labels)\n",
    "\n",
    "        loss = np.add(real_loss , fake_loss)/2\n",
    "        \n",
    "        #addin\n",
    "        #WGAN的变化 对权重参数进行裁剪\n",
    "        for layer in critic_i.layers:\n",
    "            weights = layer.get_weights()\n",
    "            weights = [np.clip(w , - CLIP_VALUE , CLIP_VALUE) for w in weights]\n",
    "            layer.set_weights(weights)\n",
    "    #============================\n",
    "    #===\n",
    "    critic_i.trainable = False\n",
    "    for layer in critic_i.layers:\n",
    "        layer.trainable = False\n",
    "    #===\n",
    "    \n",
    "    #训练生成器\n",
    "    noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    generator_loss = combined_model_i.train_on_batch(noise2 , real_labels)\n",
    "\n",
    "    print('epoch:%d loss:%f gene_loss:%f' % (i , 1-loss[0] , 1-generator_loss))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        write_image(i)\n",
    "    \n",
    "write_image(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeli = Sequential()\n",
    "\n",
    "modeli.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "modeli.add(Reshape((7, 7, 128)))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(Activation(\"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modeli.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
