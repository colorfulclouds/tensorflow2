{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reference https://github.com/mokemokechicken/keras_BEGAN\n",
    "#reference https://github.com/siddharthalodha/BEGAN_KERAS/blob/master/BEGAN_v1.ipynb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D , Convolution2D\n",
    "from keras.layers.advanced_activations import LeakyReLU , PReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.initializers import truncated_normal , constant , random_normal\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "#残差块使用\n",
    "from keras.layers import Add\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#addin BEGAN\n",
    "from keras.layers import Lambda #可以自己指定特定层 完成特定的功能 discriminator使用\n",
    "from keras.layers import Concatenate #将discriminator的两个输出合并使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from scipy.misc import imread , imsave , imresize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BEGAN使用64*64*3 图像\n",
    "WIDTH = 64\n",
    "HEIGHT = 64\n",
    "CHANNEL = 3\n",
    "\n",
    "SHAPE = (WIDTH , HEIGHT , CHANNEL)\n",
    "\n",
    "LATENT_DIM = 64 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "    PATH = '../dataset/CelebA/img_align_celeba/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 2\n",
    "COL = 2\n",
    "\n",
    "#addin BEGAN\n",
    "N_FILTERS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============\n",
    "IMAGES_PATH = glob(PATH+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202599"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(batch_size = BATCH_SIZE , training = True):\n",
    "    #随机在图片库中挑选\n",
    "    image_path = np.random.choice(IMAGES_PATH , size=batch_size)\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for an_image_path in image_path:\n",
    "        img = imread(an_image_path , mode='RGB').astype(np.float)\n",
    "        \n",
    "        #尽管原图像不是指定的大小 下面将强制将图像resize\n",
    "        img = imresize(img , size=SHAPE) #resize到64*64*3尺寸\n",
    "        \n",
    "        #随机性地对训练样本进行 左右反转\n",
    "        #BEGAN不进行左右反转\n",
    "        #if training and np.random.random()<0.5:\n",
    "        #    img = np.fliplr(img)\n",
    "        \n",
    "        images.append(img)\n",
    "        \n",
    "    images = np.array(images)/255.0\n",
    "    \n",
    "    return images\n",
    "\n",
    "def write_image(epoch):\n",
    "    #生成高分图像时 进行对比显示\n",
    "    z = np.random.uniform(-1 , 1 , size=(ROW*COL , LATENT_DIM))\n",
    "    images = Generator.predict(z)\n",
    "    \n",
    "    images = images*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    count=0\n",
    "    \n",
    "    axes[0][0].imshow(images[0])\n",
    "    #axes[0][0].set_title('original high')\n",
    "    axes[0][0].axis('off')\n",
    "\n",
    "    axes[0][1].imshow(images[1])\n",
    "    #axes[0][1].set_title('generated high')\n",
    "    axes[0][1].axis('off')\n",
    "    \n",
    "\n",
    "    axes[1][0].imshow(images[2])\n",
    "    #axes[1][0].set_title('original high')\n",
    "    axes[1][0].axis('off')\n",
    "\n",
    "    axes[1][1].imshow(images[3])\n",
    "    #axes[1][1].set_title('generated high')\n",
    "    axes[1][1].axis('off')\n",
    "    \n",
    "    fig.savefig('celeba_began/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_new(index_list):\n",
    "    images = []\n",
    "    \n",
    "    for an_image_path in index_list:\n",
    "        img = imread(IMAGES_PATH[an_image_path] , mode='RGB').astype(np.float)\n",
    "        \n",
    "        #尽管原图像不是指定的大小 下面将强制将图像resize\n",
    "        img = imresize(img , size=SHAPE) #resize到64*64*3尺寸\n",
    "        \n",
    "        #随机性地对训练样本进行 左右反转\n",
    "        #BEGAN不进行左右反转\n",
    "        #if training and np.random.random()<0.5:\n",
    "        #    img = np.fliplr(img)\n",
    "        \n",
    "        images.append(img)\n",
    "        \n",
    "    images = np.array(images)/255.0\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def multi_conv2d(x , output_size , strides , n_layer):\n",
    "#    for _ in range(n_layer):\n",
    "#        x = Conv2D(output_size , kernel_size=(3,3) , strides=(1,1) , activation='elu' , padding='same')(x) #1步卷积 保证图像尺寸不变\n",
    "#    \n",
    "#    return Conv2D(output_size , kernel_size=(3,3) , strides=strides , activation='elu' , padding='same')(x) #最后的这个卷积的stride为2 进行降2倍采样\n",
    "\n",
    "def multi_conv2d(x, filters, strides=(1, 1), name=None, n_layer=2):\n",
    "    for i in range(1, n_layer):\n",
    "        x = Convolution2D(filters, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "\n",
    "    x = Convolution2D(filters, (3, 3), activation=\"elu\", padding=\"same\", strides=strides)(x)\n",
    "    return x\n",
    "\n",
    "#def multi_deconv2d(x , output_size , n_layer , upsample):\n",
    "#    for _ in range(n_layer):\n",
    "#        x = Conv2D(output_size , kernel_size=(3,3) , strides=(1,1) , activation='elu' , padding='same')(x)\n",
    "#    \n",
    "#    if upsample:\n",
    "#        x = UpSampling2D()(x)\n",
    "#\n",
    "#    return x\n",
    "\n",
    "def multi_deconv2d(x, filters, upsample=None, name=None, n_layer=2):\n",
    "    for i in range(1, n_layer+1):\n",
    "        x = Convolution2D(filters, (3, 3), activation=\"elu\", padding=\"same\")(x)\n",
    "    if upsample:\n",
    "        x = UpSampling2D()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    image = Input(shape=SHAPE)\n",
    "    \n",
    "    h = multi_conv2d(image , N_FILTERS , strides=(2,2) , n_layer=2)\n",
    "    h = multi_conv2d(h , N_FILTERS*2 , strides=(2,2) , n_layer=2)\n",
    "    h = multi_conv2d(h , N_FILTERS*3 , strides=(2,2) , n_layer=2)\n",
    "    h = multi_conv2d(h , N_FILTERS*4 , strides=(1,1) , n_layer=2)\n",
    "    \n",
    "    h = Flatten()(h)\n",
    "    \n",
    "    feature = Dense(units=LATENT_DIM , activation='linear')(h)\n",
    "    \n",
    "    return Model(image , feature)\n",
    "    \n",
    "def decoder():\n",
    "    feature = Input(shape=(LATENT_DIM , ))\n",
    "    \n",
    "    h = Dense(units=(8*8*N_FILTERS) , activation='linear')(feature)\n",
    "    h = Reshape(target_shape=(8,8,N_FILTERS))(h)\n",
    "    \n",
    "    h = multi_deconv2d(h , N_FILTERS , upsample=True , n_layer=2 )\n",
    "    h = multi_deconv2d(h , N_FILTERS , upsample=True , n_layer=2 )\n",
    "    h = multi_deconv2d(h , N_FILTERS , upsample=True , n_layer=2 )\n",
    "    h = multi_deconv2d(h , N_FILTERS , upsample=False , n_layer=2)\n",
    "    \n",
    "    image = Convolution2D(3 , kernel_size=(3,3) , strides=(1,1) , activation='linear' , padding='same')(h)\n",
    "    \n",
    "    return Model(feature , image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoencoder():\n",
    "    image = Input(shape=(SHAPE))\n",
    "    \n",
    "    Encoder = encoder()\n",
    "    Decoder = decoder()\n",
    "    \n",
    "    feature = Encoder(image)\n",
    "    image_hat = Decoder(feature)\n",
    "    \n",
    "    return Model(image , image_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    input_data = Input(shape=(HEIGHT , WIDTH , CHANNEL*2)) #因为discriminator的输入是两幅图像 所以输入的通道翻一倍\n",
    "    \n",
    "    real_image = Lambda(lambda x: x[:,:,:,0:3] , output_shape=SHAPE)(input_data)\n",
    "    generator_image = Lambda(lambda x: x[:,:,:,3:6] , output_shape=SHAPE)(input_data)\n",
    "    \n",
    "    Autoencoder = autoencoder()\n",
    "    \n",
    "    real_image_hat = Autoencoder(real_image)\n",
    "    generator_image_hat = Autoencoder(generator_image)\n",
    "\n",
    "    output_data = Concatenate(axis=-1)([real_image_hat , generator_image_hat])\n",
    "\n",
    "    return Model(input_data , output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    return decoder() #feature to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class D_loss(object):\n",
    "    __name__ = 'discriminator_loss'\n",
    "    \n",
    "    def __init__(self , init_k_var=0.0 , init_lambda_k = 0.001 , init_gamma=0.5):\n",
    "        self.lambda_k = init_lambda_k\n",
    "        self.gamma = init_gamma\n",
    "        self.k_var = K.variable(init_k_var , dtype=K.floatx())\n",
    "        \n",
    "        self.m_global_var = K.variable(0.0 , dtype=K.floatx())\n",
    "        self.loss_real_x_var = K.variable(0)\n",
    "        self.loss_gene_x_var = K.variable(0)\n",
    "        \n",
    "        self.updates = []\n",
    "\n",
    "    def __call__(self , y_true , y_pred):\n",
    "        real_image_hat_true , generator_image_hat_true = y_true[:,:,:,:3] , y_true[:,:,:,3:]\n",
    "        real_image_hat_pred ,generator_image_hat_pred = y_pred[:,:,:,:3] , y_true[:,:,:,3:]\n",
    "\n",
    "        #下面的平均在batch维上还没有平均 所以得到的是1维张量\n",
    "        loss_real_image_hat = K.mean(K.abs(real_image_hat_true-real_image_hat_pred) , axis=[1,2,3])\n",
    "        loss_generator_image_hat = K.mean(K.abs(generator_image_hat_true-generator_image_hat_pred) , axis=[1,2,3])\n",
    "\n",
    "        #paper equation\n",
    "        discriminator_loss = loss_real_image_hat - self.k_var*loss_generator_image_hat\n",
    "\n",
    "        mean_loss_real_image_hat = K.mean(loss_real_image_hat)\n",
    "        mean_loss_generator_image_hat = K.mean(loss_generator_image_hat)\n",
    "        #paper equation\n",
    "        new_k = self.k_var + self.lambda_k*(self.gamma*mean_loss_real_image_hat-mean_loss_generator_image_hat)\n",
    "        new_k = K.clip(new_k , 0 , 1)\n",
    "        self.updates.append(K.update(self.k_var , new_k))\n",
    "        \n",
    "        m_global = mean_loss_real_image_hat + K.abs(self.gamma*mean_loss_real_image_hat-mean_loss_generator_image_hat)\n",
    "        self.updates.append(K.update(self.m_global_var , m_global))\n",
    "        \n",
    "        self.updates.append(K.update(self.loss_real_x_var , mean_loss_real_image_hat))\n",
    "        self.updates.append(K.update(self.loss_gene_x_var , mean_loss_generator_image_hat))\n",
    "        \n",
    "        return discriminator_loss\n",
    "\n",
    "    @property\n",
    "    def k(self):\n",
    "        return K.get_value(self.k_var)\n",
    "    \n",
    "    @property\n",
    "    def m_global(self):\n",
    "        return K.get_value(self.m_global_var)\n",
    "    \n",
    "    @property\n",
    "    def loss_real_x(self):\n",
    "        return K.get_value(self.loss_real_x_var)\n",
    "    \n",
    "    @property\n",
    "    def loss_gene_x(self):\n",
    "        return K.get_value(self.loss_gene_x_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_generator_loss(Autoencoder):\n",
    "    def generator_loss(y_true , y_pred):\n",
    "        y_pred_dash = Autoencoder(y_pred)\n",
    "        \n",
    "        return K.mean(K.abs(y_pred - y_pred_dash) , axis=[1,2,3])\n",
    "    \n",
    "    return generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder = autoencoder()\n",
    "Generator = generator()\n",
    "Discrimminator = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_discriminator = D_loss()\n",
    "Discrimminator.compile(optimizer=Adam() , loss=loss_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Generator.compile(optimizer=Adam() , loss=build_generator_loss(Autoencoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss_D:0.418878 loss_G:0.045258\n",
      "epoch:1 loss_D:0.443488 loss_G:0.037838\n",
      "epoch:2 loss_D:0.392980 loss_G:0.033248\n",
      "epoch:3 loss_D:0.300696 loss_G:0.029937\n",
      "epoch:4 loss_D:0.361292 loss_G:0.027289\n",
      "epoch:5 loss_D:0.293150 loss_G:0.025801\n",
      "epoch:6 loss_D:0.233563 loss_G:0.024543\n",
      "epoch:7 loss_D:0.248045 loss_G:0.023060\n",
      "epoch:8 loss_D:0.293889 loss_G:0.022275\n",
      "epoch:9 loss_D:0.273142 loss_G:0.021573\n",
      "epoch:10 loss_D:0.225354 loss_G:0.020987\n",
      "epoch:11 loss_D:0.250578 loss_G:0.020224\n",
      "epoch:12 loss_D:0.241314 loss_G:0.019485\n",
      "epoch:13 loss_D:0.239344 loss_G:0.019324\n",
      "epoch:14 loss_D:0.240600 loss_G:0.018877\n",
      "epoch:15 loss_D:0.232112 loss_G:0.018133\n",
      "epoch:16 loss_D:0.237394 loss_G:0.018214\n",
      "epoch:17 loss_D:0.231723 loss_G:0.017812\n",
      "epoch:18 loss_D:0.228199 loss_G:0.017803\n",
      "epoch:19 loss_D:0.230269 loss_G:0.017756\n",
      "epoch:20 loss_D:0.221120 loss_G:0.017484\n",
      "epoch:21 loss_D:0.227154 loss_G:0.017124\n",
      "epoch:22 loss_D:0.217813 loss_G:0.017081\n",
      "epoch:23 loss_D:0.236953 loss_G:0.016824\n",
      "epoch:24 loss_D:0.233071 loss_G:0.017130\n",
      "epoch:25 loss_D:0.235239 loss_G:0.016668\n",
      "epoch:26 loss_D:0.220154 loss_G:0.016640\n",
      "epoch:27 loss_D:0.215834 loss_G:0.016504\n",
      "epoch:28 loss_D:0.211903 loss_G:0.016483\n",
      "epoch:29 loss_D:0.210777 loss_G:0.016221\n",
      "epoch:30 loss_D:0.219320 loss_G:0.016267\n",
      "epoch:31 loss_D:0.220981 loss_G:0.015986\n",
      "epoch:32 loss_D:0.224742 loss_G:0.016104\n",
      "epoch:33 loss_D:0.221201 loss_G:0.015876\n",
      "epoch:34 loss_D:0.206278 loss_G:0.015801\n",
      "epoch:35 loss_D:0.210815 loss_G:0.015908\n",
      "epoch:36 loss_D:0.210381 loss_G:0.015560\n",
      "epoch:37 loss_D:0.217016 loss_G:0.015365\n",
      "epoch:38 loss_D:0.200638 loss_G:0.015181\n",
      "epoch:39 loss_D:0.226293 loss_G:0.015226\n",
      "epoch:40 loss_D:0.216821 loss_G:0.015081\n",
      "epoch:41 loss_D:0.200381 loss_G:0.014833\n",
      "epoch:42 loss_D:0.208893 loss_G:0.014878\n",
      "epoch:43 loss_D:0.201028 loss_G:0.014715\n",
      "epoch:44 loss_D:0.192407 loss_G:0.014559\n",
      "epoch:45 loss_D:0.210780 loss_G:0.014594\n",
      "epoch:46 loss_D:0.200396 loss_G:0.014618\n",
      "epoch:47 loss_D:0.205000 loss_G:0.014421\n",
      "epoch:48 loss_D:0.199693 loss_G:0.014468\n",
      "epoch:49 loss_D:0.191864 loss_G:0.014395\n",
      "epoch:50 loss_D:0.209846 loss_G:0.014255\n",
      "epoch:51 loss_D:0.201707 loss_G:0.014027\n",
      "epoch:52 loss_D:0.206826 loss_G:0.014128\n",
      "epoch:53 loss_D:0.203767 loss_G:0.013971\n",
      "epoch:54 loss_D:0.196691 loss_G:0.013878\n",
      "epoch:55 loss_D:0.210443 loss_G:0.013500\n",
      "epoch:56 loss_D:0.198782 loss_G:0.013859\n",
      "epoch:57 loss_D:0.191308 loss_G:0.013470\n",
      "epoch:58 loss_D:0.195563 loss_G:0.013607\n",
      "epoch:59 loss_D:0.185048 loss_G:0.013349\n",
      "epoch:60 loss_D:0.191179 loss_G:0.013233\n",
      "epoch:61 loss_D:0.184073 loss_G:0.013270\n",
      "epoch:62 loss_D:0.179732 loss_G:0.012918\n",
      "epoch:63 loss_D:0.185814 loss_G:0.013120\n",
      "epoch:64 loss_D:0.180723 loss_G:0.013189\n",
      "epoch:65 loss_D:0.174630 loss_G:0.012954\n",
      "epoch:66 loss_D:0.182854 loss_G:0.012712\n",
      "epoch:67 loss_D:0.196712 loss_G:0.012806\n",
      "epoch:68 loss_D:0.188763 loss_G:0.012585\n",
      "epoch:69 loss_D:0.179005 loss_G:0.012582\n",
      "epoch:70 loss_D:0.168798 loss_G:0.012597\n",
      "epoch:71 loss_D:0.161376 loss_G:0.012439\n",
      "epoch:72 loss_D:0.186687 loss_G:0.012516\n",
      "epoch:73 loss_D:0.175389 loss_G:0.012151\n",
      "epoch:74 loss_D:0.188940 loss_G:0.011955\n",
      "epoch:75 loss_D:0.170317 loss_G:0.011993\n",
      "epoch:76 loss_D:0.170570 loss_G:0.012152\n",
      "epoch:77 loss_D:0.189124 loss_G:0.011961\n",
      "epoch:78 loss_D:0.171354 loss_G:0.011851\n",
      "epoch:79 loss_D:0.168897 loss_G:0.011905\n",
      "epoch:80 loss_D:0.173650 loss_G:0.011738\n",
      "epoch:81 loss_D:0.178858 loss_G:0.011884\n",
      "epoch:82 loss_D:0.186372 loss_G:0.011794\n",
      "epoch:83 loss_D:0.174488 loss_G:0.011555\n",
      "epoch:84 loss_D:0.174272 loss_G:0.011569\n",
      "epoch:85 loss_D:0.162678 loss_G:0.011484\n",
      "epoch:86 loss_D:0.168579 loss_G:0.011376\n",
      "epoch:87 loss_D:0.170617 loss_G:0.011492\n",
      "epoch:88 loss_D:0.163161 loss_G:0.011258\n",
      "epoch:89 loss_D:0.166965 loss_G:0.011236\n",
      "epoch:90 loss_D:0.158882 loss_G:0.011054\n",
      "epoch:91 loss_D:0.177255 loss_G:0.011050\n",
      "epoch:92 loss_D:0.177369 loss_G:0.011169\n",
      "epoch:93 loss_D:0.164488 loss_G:0.011192\n",
      "epoch:94 loss_D:0.165319 loss_G:0.011211\n",
      "epoch:95 loss_D:0.159684 loss_G:0.011017\n",
      "epoch:96 loss_D:0.155513 loss_G:0.010723\n",
      "epoch:97 loss_D:0.159010 loss_G:0.010857\n",
      "epoch:98 loss_D:0.160893 loss_G:0.010711\n",
      "epoch:99 loss_D:0.160317 loss_G:0.010750\n",
      "epoch:100 loss_D:0.157482 loss_G:0.010717\n",
      "epoch:101 loss_D:0.152839 loss_G:0.010586\n",
      "epoch:102 loss_D:0.163665 loss_G:0.010391\n",
      "epoch:103 loss_D:0.182911 loss_G:0.010470\n",
      "epoch:104 loss_D:0.153101 loss_G:0.010588\n",
      "epoch:105 loss_D:0.166598 loss_G:0.010411\n",
      "epoch:106 loss_D:0.166361 loss_G:0.010407\n",
      "epoch:107 loss_D:0.163594 loss_G:0.010141\n",
      "epoch:108 loss_D:0.159748 loss_G:0.010143\n",
      "epoch:109 loss_D:0.170229 loss_G:0.010340\n",
      "epoch:110 loss_D:0.154056 loss_G:0.010042\n",
      "epoch:111 loss_D:0.173926 loss_G:0.010020\n",
      "epoch:112 loss_D:0.162673 loss_G:0.010066\n",
      "epoch:113 loss_D:0.152420 loss_G:0.009894\n",
      "epoch:114 loss_D:0.168085 loss_G:0.009975\n",
      "epoch:115 loss_D:0.158455 loss_G:0.009819\n",
      "epoch:116 loss_D:0.159580 loss_G:0.009875\n",
      "epoch:117 loss_D:0.161764 loss_G:0.009766\n",
      "epoch:118 loss_D:0.152199 loss_G:0.009644\n",
      "epoch:119 loss_D:0.141208 loss_G:0.009730\n",
      "epoch:120 loss_D:0.155603 loss_G:0.009617\n",
      "epoch:121 loss_D:0.146494 loss_G:0.009540\n",
      "epoch:122 loss_D:0.152389 loss_G:0.009603\n",
      "epoch:123 loss_D:0.154007 loss_G:0.009466\n",
      "epoch:124 loss_D:0.157778 loss_G:0.009317\n",
      "epoch:125 loss_D:0.155104 loss_G:0.009438\n",
      "epoch:126 loss_D:0.151795 loss_G:0.009281\n",
      "epoch:127 loss_D:0.149624 loss_G:0.009420\n",
      "epoch:128 loss_D:0.157052 loss_G:0.009163\n",
      "epoch:129 loss_D:0.159496 loss_G:0.009279\n",
      "epoch:130 loss_D:0.146498 loss_G:0.009123\n",
      "epoch:131 loss_D:0.146994 loss_G:0.009095\n",
      "epoch:132 loss_D:0.145292 loss_G:0.009141\n",
      "epoch:133 loss_D:0.162459 loss_G:0.009077\n",
      "epoch:134 loss_D:0.158894 loss_G:0.009029\n",
      "epoch:135 loss_D:0.153245 loss_G:0.009003\n",
      "epoch:136 loss_D:0.148078 loss_G:0.009007\n",
      "epoch:137 loss_D:0.150481 loss_G:0.008842\n",
      "epoch:138 loss_D:0.152890 loss_G:0.008868\n",
      "epoch:139 loss_D:0.140028 loss_G:0.008806\n",
      "epoch:140 loss_D:0.143749 loss_G:0.008817\n",
      "epoch:141 loss_D:0.154986 loss_G:0.008716\n",
      "epoch:142 loss_D:0.145519 loss_G:0.008673\n",
      "epoch:143 loss_D:0.144656 loss_G:0.008680\n",
      "epoch:144 loss_D:0.144353 loss_G:0.008619\n",
      "epoch:145 loss_D:0.143146 loss_G:0.008619\n",
      "epoch:146 loss_D:0.151142 loss_G:0.008509\n",
      "epoch:147 loss_D:0.143787 loss_G:0.008614\n",
      "epoch:148 loss_D:0.140352 loss_G:0.008503\n",
      "epoch:149 loss_D:0.153410 loss_G:0.008478\n",
      "epoch:150 loss_D:0.139387 loss_G:0.008335\n",
      "epoch:151 loss_D:0.145347 loss_G:0.008345\n",
      "epoch:152 loss_D:0.146332 loss_G:0.008098\n",
      "epoch:153 loss_D:0.141050 loss_G:0.008302\n",
      "epoch:154 loss_D:0.152293 loss_G:0.008178\n",
      "epoch:155 loss_D:0.144795 loss_G:0.008204\n",
      "epoch:156 loss_D:0.141824 loss_G:0.008217\n",
      "epoch:157 loss_D:0.149103 loss_G:0.008207\n",
      "epoch:158 loss_D:0.146919 loss_G:0.008057\n",
      "epoch:159 loss_D:0.138851 loss_G:0.008009\n",
      "epoch:160 loss_D:0.136168 loss_G:0.008080\n",
      "epoch:161 loss_D:0.137182 loss_G:0.007940\n",
      "epoch:162 loss_D:0.149367 loss_G:0.007897\n",
      "epoch:163 loss_D:0.136695 loss_G:0.007917\n",
      "epoch:164 loss_D:0.145311 loss_G:0.007934\n",
      "epoch:165 loss_D:0.140425 loss_G:0.007909\n",
      "epoch:166 loss_D:0.145626 loss_G:0.007710\n",
      "epoch:167 loss_D:0.141939 loss_G:0.007846\n",
      "epoch:168 loss_D:0.142926 loss_G:0.007808\n",
      "epoch:169 loss_D:0.143745 loss_G:0.007673\n",
      "epoch:170 loss_D:0.142837 loss_G:0.007706\n",
      "epoch:171 loss_D:0.134656 loss_G:0.007629\n",
      "epoch:172 loss_D:0.145119 loss_G:0.007698\n",
      "epoch:173 loss_D:0.151516 loss_G:0.007578\n",
      "epoch:174 loss_D:0.140586 loss_G:0.007682\n",
      "epoch:175 loss_D:0.136693 loss_G:0.007703\n",
      "epoch:176 loss_D:0.139711 loss_G:0.007487\n",
      "epoch:177 loss_D:0.132734 loss_G:0.007555\n",
      "epoch:178 loss_D:0.140591 loss_G:0.007442\n",
      "epoch:179 loss_D:0.131165 loss_G:0.007375\n",
      "epoch:180 loss_D:0.134351 loss_G:0.007339\n",
      "epoch:181 loss_D:0.149011 loss_G:0.007322\n",
      "epoch:182 loss_D:0.136619 loss_G:0.007448\n",
      "epoch:183 loss_D:0.138482 loss_G:0.007410\n",
      "epoch:184 loss_D:0.130498 loss_G:0.007375\n",
      "epoch:185 loss_D:0.134508 loss_G:0.007244\n",
      "epoch:186 loss_D:0.136551 loss_G:0.007207\n",
      "epoch:187 loss_D:0.132930 loss_G:0.007239\n",
      "epoch:188 loss_D:0.149739 loss_G:0.007230\n",
      "epoch:189 loss_D:0.144779 loss_G:0.007216\n",
      "epoch:190 loss_D:0.133117 loss_G:0.007086\n",
      "epoch:191 loss_D:0.134242 loss_G:0.007118\n",
      "epoch:192 loss_D:0.138320 loss_G:0.007030\n",
      "epoch:193 loss_D:0.132691 loss_G:0.007044\n",
      "epoch:194 loss_D:0.137986 loss_G:0.006926\n",
      "epoch:195 loss_D:0.145467 loss_G:0.006947\n",
      "epoch:196 loss_D:0.131088 loss_G:0.006992\n",
      "epoch:197 loss_D:0.139943 loss_G:0.006957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:198 loss_D:0.131197 loss_G:0.006844\n",
      "epoch:199 loss_D:0.141513 loss_G:0.006905\n",
      "epoch:200 loss_D:0.138327 loss_G:0.006848\n",
      "epoch:201 loss_D:0.131111 loss_G:0.006886\n",
      "epoch:202 loss_D:0.127584 loss_G:0.006837\n",
      "epoch:203 loss_D:0.130179 loss_G:0.006900\n",
      "epoch:204 loss_D:0.128798 loss_G:0.006763\n",
      "epoch:205 loss_D:0.128042 loss_G:0.006661\n",
      "epoch:206 loss_D:0.132492 loss_G:0.006888\n",
      "epoch:207 loss_D:0.137565 loss_G:0.006807\n",
      "epoch:208 loss_D:0.124384 loss_G:0.006570\n",
      "epoch:209 loss_D:0.125838 loss_G:0.006660\n",
      "epoch:210 loss_D:0.130285 loss_G:0.006533\n",
      "epoch:211 loss_D:0.131025 loss_G:0.006708\n",
      "epoch:212 loss_D:0.129272 loss_G:0.006647\n",
      "epoch:213 loss_D:0.138353 loss_G:0.006510\n",
      "epoch:214 loss_D:0.136552 loss_G:0.006621\n",
      "epoch:215 loss_D:0.127353 loss_G:0.006479\n",
      "epoch:216 loss_D:0.123801 loss_G:0.006459\n",
      "epoch:217 loss_D:0.126151 loss_G:0.006557\n",
      "epoch:218 loss_D:0.122483 loss_G:0.006371\n",
      "epoch:219 loss_D:0.130405 loss_G:0.006480\n",
      "epoch:220 loss_D:0.135506 loss_G:0.006332\n",
      "epoch:221 loss_D:0.133041 loss_G:0.006412\n",
      "epoch:222 loss_D:0.126594 loss_G:0.006410\n",
      "epoch:223 loss_D:0.137240 loss_G:0.006348\n",
      "epoch:224 loss_D:0.122862 loss_G:0.006279\n",
      "epoch:225 loss_D:0.120058 loss_G:0.006263\n",
      "epoch:226 loss_D:0.128598 loss_G:0.006414\n",
      "epoch:227 loss_D:0.127444 loss_G:0.006325\n",
      "epoch:228 loss_D:0.124436 loss_G:0.006199\n",
      "epoch:229 loss_D:0.128593 loss_G:0.006134\n",
      "epoch:230 loss_D:0.131465 loss_G:0.006125\n",
      "epoch:231 loss_D:0.128917 loss_G:0.006078\n",
      "epoch:232 loss_D:0.130638 loss_G:0.006147\n",
      "epoch:233 loss_D:0.126213 loss_G:0.006128\n",
      "epoch:234 loss_D:0.131516 loss_G:0.006188\n",
      "epoch:235 loss_D:0.126163 loss_G:0.006069\n",
      "epoch:236 loss_D:0.129410 loss_G:0.006044\n",
      "epoch:237 loss_D:0.124044 loss_G:0.005989\n",
      "epoch:238 loss_D:0.125419 loss_G:0.005950\n",
      "epoch:239 loss_D:0.123865 loss_G:0.005948\n",
      "epoch:240 loss_D:0.124558 loss_G:0.005996\n",
      "epoch:241 loss_D:0.134535 loss_G:0.005940\n",
      "epoch:242 loss_D:0.131379 loss_G:0.005976\n",
      "epoch:243 loss_D:0.119394 loss_G:0.005879\n",
      "epoch:244 loss_D:0.137532 loss_G:0.005954\n",
      "epoch:245 loss_D:0.119190 loss_G:0.005861\n",
      "epoch:246 loss_D:0.133069 loss_G:0.005963\n",
      "epoch:247 loss_D:0.131455 loss_G:0.005834\n",
      "epoch:248 loss_D:0.121937 loss_G:0.005860\n",
      "epoch:249 loss_D:0.131569 loss_G:0.005859\n",
      "epoch:250 loss_D:0.119400 loss_G:0.005834\n",
      "epoch:251 loss_D:0.124659 loss_G:0.005717\n",
      "epoch:252 loss_D:0.117908 loss_G:0.005691\n",
      "epoch:253 loss_D:0.129839 loss_G:0.005692\n",
      "epoch:254 loss_D:0.131482 loss_G:0.005723\n",
      "epoch:255 loss_D:0.130759 loss_G:0.005746\n",
      "epoch:256 loss_D:0.123161 loss_G:0.005586\n",
      "epoch:257 loss_D:0.124690 loss_G:0.005640\n",
      "epoch:258 loss_D:0.127670 loss_G:0.005550\n",
      "epoch:259 loss_D:0.132918 loss_G:0.005562\n",
      "epoch:260 loss_D:0.128950 loss_G:0.005704\n",
      "epoch:261 loss_D:0.126371 loss_G:0.005643\n",
      "epoch:262 loss_D:0.121839 loss_G:0.005485\n",
      "epoch:263 loss_D:0.123012 loss_G:0.005533\n",
      "epoch:264 loss_D:0.125616 loss_G:0.005553\n",
      "epoch:265 loss_D:0.129053 loss_G:0.005505\n",
      "epoch:266 loss_D:0.125055 loss_G:0.005478\n",
      "epoch:267 loss_D:0.126736 loss_G:0.005552\n",
      "epoch:268 loss_D:0.121165 loss_G:0.005477\n",
      "epoch:269 loss_D:0.126980 loss_G:0.005344\n",
      "epoch:270 loss_D:0.119537 loss_G:0.005486\n",
      "epoch:271 loss_D:0.126722 loss_G:0.005482\n",
      "epoch:272 loss_D:0.128059 loss_G:0.005351\n",
      "epoch:273 loss_D:0.115212 loss_G:0.005412\n",
      "epoch:274 loss_D:0.119234 loss_G:0.005357\n",
      "epoch:275 loss_D:0.116846 loss_G:0.005350\n",
      "epoch:276 loss_D:0.118248 loss_G:0.005343\n",
      "epoch:277 loss_D:0.130542 loss_G:0.005368\n",
      "epoch:278 loss_D:0.115423 loss_G:0.005349\n",
      "epoch:279 loss_D:0.126559 loss_G:0.005234\n",
      "epoch:280 loss_D:0.116450 loss_G:0.005255\n",
      "epoch:281 loss_D:0.121540 loss_G:0.005279\n",
      "epoch:282 loss_D:0.118730 loss_G:0.005252\n",
      "epoch:283 loss_D:0.117870 loss_G:0.005199\n",
      "epoch:284 loss_D:0.116731 loss_G:0.005188\n",
      "epoch:285 loss_D:0.125610 loss_G:0.005193\n",
      "epoch:286 loss_D:0.117421 loss_G:0.005167\n",
      "epoch:287 loss_D:0.119793 loss_G:0.005213\n",
      "epoch:288 loss_D:0.113949 loss_G:0.005066\n",
      "epoch:289 loss_D:0.119265 loss_G:0.005160\n",
      "epoch:290 loss_D:0.124174 loss_G:0.005119\n",
      "epoch:291 loss_D:0.127661 loss_G:0.005123\n",
      "epoch:292 loss_D:0.120371 loss_G:0.005066\n",
      "epoch:293 loss_D:0.117971 loss_G:0.005043\n",
      "epoch:294 loss_D:0.115795 loss_G:0.005078\n",
      "epoch:295 loss_D:0.123373 loss_G:0.005009\n",
      "epoch:296 loss_D:0.111885 loss_G:0.005048\n",
      "epoch:297 loss_D:0.119890 loss_G:0.005015\n",
      "epoch:298 loss_D:0.115019 loss_G:0.005027\n",
      "epoch:299 loss_D:0.115883 loss_G:0.004986\n",
      "epoch:300 loss_D:0.110388 loss_G:0.005008\n",
      "epoch:301 loss_D:0.112270 loss_G:0.004980\n",
      "epoch:302 loss_D:0.115959 loss_G:0.004938\n",
      "epoch:303 loss_D:0.119040 loss_G:0.004856\n",
      "epoch:304 loss_D:0.124334 loss_G:0.004882\n",
      "epoch:305 loss_D:0.121012 loss_G:0.004843\n",
      "epoch:306 loss_D:0.113830 loss_G:0.004941\n",
      "epoch:307 loss_D:0.119962 loss_G:0.004891\n",
      "epoch:308 loss_D:0.129565 loss_G:0.004903\n",
      "epoch:309 loss_D:0.120177 loss_G:0.004750\n",
      "epoch:310 loss_D:0.119510 loss_G:0.004856\n",
      "epoch:311 loss_D:0.112531 loss_G:0.004779\n",
      "epoch:312 loss_D:0.126130 loss_G:0.004763\n",
      "epoch:313 loss_D:0.120583 loss_G:0.004810\n",
      "epoch:314 loss_D:0.116714 loss_G:0.004786\n",
      "epoch:315 loss_D:0.107982 loss_G:0.004708\n",
      "epoch:316 loss_D:0.118983 loss_G:0.004736\n",
      "epoch:317 loss_D:0.120730 loss_G:0.004733\n",
      "epoch:318 loss_D:0.120936 loss_G:0.004706\n",
      "epoch:319 loss_D:0.119380 loss_G:0.004821\n",
      "epoch:320 loss_D:0.122996 loss_G:0.004645\n",
      "epoch:321 loss_D:0.119516 loss_G:0.004675\n",
      "epoch:322 loss_D:0.106342 loss_G:0.004638\n",
      "epoch:323 loss_D:0.114932 loss_G:0.004643\n",
      "epoch:324 loss_D:0.123841 loss_G:0.004734\n",
      "epoch:325 loss_D:0.116702 loss_G:0.004654\n",
      "epoch:326 loss_D:0.114016 loss_G:0.004680\n",
      "epoch:327 loss_D:0.117152 loss_G:0.004602\n",
      "epoch:328 loss_D:0.111820 loss_G:0.004561\n",
      "epoch:329 loss_D:0.120643 loss_G:0.004505\n",
      "epoch:330 loss_D:0.110466 loss_G:0.004549\n",
      "epoch:331 loss_D:0.118065 loss_G:0.004588\n",
      "epoch:332 loss_D:0.116723 loss_G:0.004637\n",
      "epoch:333 loss_D:0.116581 loss_G:0.004581\n",
      "epoch:334 loss_D:0.110942 loss_G:0.004501\n",
      "epoch:335 loss_D:0.106971 loss_G:0.004526\n",
      "epoch:336 loss_D:0.120686 loss_G:0.004517\n",
      "epoch:337 loss_D:0.118629 loss_G:0.004522\n",
      "epoch:338 loss_D:0.112785 loss_G:0.004418\n",
      "epoch:339 loss_D:0.120942 loss_G:0.004435\n",
      "epoch:340 loss_D:0.116779 loss_G:0.004473\n",
      "epoch:341 loss_D:0.121226 loss_G:0.004514\n",
      "epoch:342 loss_D:0.117896 loss_G:0.004364\n",
      "epoch:343 loss_D:0.119316 loss_G:0.004448\n",
      "epoch:344 loss_D:0.116805 loss_G:0.004448\n",
      "epoch:345 loss_D:0.117272 loss_G:0.004371\n",
      "epoch:346 loss_D:0.119297 loss_G:0.004409\n",
      "epoch:347 loss_D:0.118032 loss_G:0.004489\n",
      "epoch:348 loss_D:0.117875 loss_G:0.004485\n",
      "epoch:349 loss_D:0.117972 loss_G:0.004335\n",
      "epoch:350 loss_D:0.117814 loss_G:0.004298\n",
      "epoch:351 loss_D:0.113077 loss_G:0.004293\n",
      "epoch:352 loss_D:0.116561 loss_G:0.004372\n",
      "epoch:353 loss_D:0.111550 loss_G:0.004293\n",
      "epoch:354 loss_D:0.110500 loss_G:0.004266\n",
      "epoch:355 loss_D:0.117779 loss_G:0.004409\n",
      "epoch:356 loss_D:0.111869 loss_G:0.004240\n",
      "epoch:357 loss_D:0.119866 loss_G:0.004256\n",
      "epoch:358 loss_D:0.109754 loss_G:0.004178\n",
      "epoch:359 loss_D:0.104355 loss_G:0.004219\n",
      "epoch:360 loss_D:0.116799 loss_G:0.004190\n",
      "epoch:361 loss_D:0.108304 loss_G:0.004213\n",
      "epoch:362 loss_D:0.116901 loss_G:0.004167\n",
      "epoch:363 loss_D:0.116697 loss_G:0.004151\n",
      "epoch:364 loss_D:0.111287 loss_G:0.004125\n",
      "epoch:365 loss_D:0.109317 loss_G:0.004128\n",
      "epoch:366 loss_D:0.112044 loss_G:0.004142\n",
      "epoch:367 loss_D:0.112449 loss_G:0.004251\n",
      "epoch:368 loss_D:0.112957 loss_G:0.004165\n",
      "epoch:369 loss_D:0.109760 loss_G:0.004157\n",
      "epoch:370 loss_D:0.114067 loss_G:0.004096\n",
      "epoch:371 loss_D:0.110311 loss_G:0.004076\n",
      "epoch:372 loss_D:0.111297 loss_G:0.004110\n",
      "epoch:373 loss_D:0.115086 loss_G:0.004155\n",
      "epoch:374 loss_D:0.113805 loss_G:0.004082\n",
      "epoch:375 loss_D:0.110759 loss_G:0.004099\n",
      "epoch:376 loss_D:0.108652 loss_G:0.004154\n",
      "epoch:377 loss_D:0.116161 loss_G:0.004095\n",
      "epoch:378 loss_D:0.109874 loss_G:0.004029\n",
      "epoch:379 loss_D:0.110765 loss_G:0.004037\n",
      "epoch:380 loss_D:0.102495 loss_G:0.003955\n",
      "epoch:381 loss_D:0.104633 loss_G:0.004013\n",
      "epoch:382 loss_D:0.116812 loss_G:0.004089\n",
      "epoch:383 loss_D:0.115906 loss_G:0.003996\n",
      "epoch:384 loss_D:0.115249 loss_G:0.003920\n",
      "epoch:385 loss_D:0.114180 loss_G:0.003990\n",
      "epoch:386 loss_D:0.108158 loss_G:0.003951\n",
      "epoch:387 loss_D:0.116043 loss_G:0.003996\n",
      "epoch:388 loss_D:0.111785 loss_G:0.003965\n",
      "epoch:389 loss_D:0.113065 loss_G:0.003942\n",
      "epoch:390 loss_D:0.102733 loss_G:0.003987\n",
      "epoch:391 loss_D:0.111645 loss_G:0.003904\n",
      "epoch:392 loss_D:0.106957 loss_G:0.003956\n",
      "epoch:393 loss_D:0.112654 loss_G:0.003903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:394 loss_D:0.112139 loss_G:0.003904\n",
      "epoch:395 loss_D:0.113133 loss_G:0.003884\n",
      "epoch:396 loss_D:0.116346 loss_G:0.003895\n",
      "epoch:397 loss_D:0.126030 loss_G:0.003841\n",
      "epoch:398 loss_D:0.110545 loss_G:0.003861\n",
      "epoch:399 loss_D:0.113972 loss_G:0.003791\n",
      "epoch:400 loss_D:0.111488 loss_G:0.003879\n",
      "epoch:401 loss_D:0.112030 loss_G:0.003909\n",
      "epoch:402 loss_D:0.114749 loss_G:0.003832\n",
      "epoch:403 loss_D:0.114312 loss_G:0.003800\n",
      "epoch:404 loss_D:0.105869 loss_G:0.003751\n",
      "epoch:405 loss_D:0.104407 loss_G:0.003699\n",
      "epoch:406 loss_D:0.109372 loss_G:0.003848\n",
      "epoch:407 loss_D:0.121311 loss_G:0.003781\n",
      "epoch:408 loss_D:0.108516 loss_G:0.003812\n",
      "epoch:409 loss_D:0.110904 loss_G:0.003791\n",
      "epoch:410 loss_D:0.107545 loss_G:0.003747\n",
      "epoch:411 loss_D:0.111380 loss_G:0.003776\n",
      "epoch:412 loss_D:0.105992 loss_G:0.003757\n",
      "epoch:413 loss_D:0.118349 loss_G:0.003713\n",
      "epoch:414 loss_D:0.112594 loss_G:0.003743\n",
      "epoch:415 loss_D:0.108859 loss_G:0.003745\n",
      "epoch:416 loss_D:0.110121 loss_G:0.003677\n",
      "epoch:417 loss_D:0.112306 loss_G:0.003667\n",
      "epoch:418 loss_D:0.113477 loss_G:0.003650\n",
      "epoch:419 loss_D:0.112078 loss_G:0.003653\n",
      "epoch:420 loss_D:0.111849 loss_G:0.003648\n",
      "epoch:421 loss_D:0.099848 loss_G:0.003675\n",
      "epoch:422 loss_D:0.100874 loss_G:0.003695\n",
      "epoch:423 loss_D:0.108103 loss_G:0.003614\n",
      "epoch:424 loss_D:0.103080 loss_G:0.003607\n",
      "epoch:425 loss_D:0.102816 loss_G:0.003581\n",
      "epoch:426 loss_D:0.111189 loss_G:0.003574\n",
      "epoch:427 loss_D:0.105189 loss_G:0.003590\n",
      "epoch:428 loss_D:0.108536 loss_G:0.003579\n",
      "epoch:429 loss_D:0.109261 loss_G:0.003597\n",
      "epoch:430 loss_D:0.114587 loss_G:0.003600\n",
      "epoch:431 loss_D:0.104957 loss_G:0.003595\n",
      "epoch:432 loss_D:0.103473 loss_G:0.003554\n",
      "epoch:433 loss_D:0.117895 loss_G:0.003529\n",
      "epoch:434 loss_D:0.108467 loss_G:0.003535\n",
      "epoch:435 loss_D:0.105258 loss_G:0.003517\n",
      "epoch:436 loss_D:0.110841 loss_G:0.003528\n",
      "epoch:437 loss_D:0.109649 loss_G:0.003506\n",
      "epoch:438 loss_D:0.106631 loss_G:0.003562\n",
      "epoch:439 loss_D:0.109005 loss_G:0.003547\n",
      "epoch:440 loss_D:0.100896 loss_G:0.003481\n",
      "epoch:441 loss_D:0.105044 loss_G:0.003532\n",
      "epoch:442 loss_D:0.108312 loss_G:0.003458\n",
      "epoch:443 loss_D:0.108591 loss_G:0.003447\n",
      "epoch:444 loss_D:0.108810 loss_G:0.003419\n",
      "epoch:445 loss_D:0.106757 loss_G:0.003414\n",
      "epoch:446 loss_D:0.109514 loss_G:0.003493\n",
      "epoch:447 loss_D:0.100551 loss_G:0.003404\n",
      "epoch:448 loss_D:0.107562 loss_G:0.003453\n",
      "epoch:449 loss_D:0.109355 loss_G:0.003443\n",
      "epoch:450 loss_D:0.110886 loss_G:0.003474\n",
      "epoch:451 loss_D:0.107545 loss_G:0.003375\n",
      "epoch:452 loss_D:0.105600 loss_G:0.003394\n",
      "epoch:453 loss_D:0.108960 loss_G:0.003363\n",
      "epoch:454 loss_D:0.110592 loss_G:0.003401\n",
      "epoch:455 loss_D:0.103389 loss_G:0.003400\n",
      "epoch:456 loss_D:0.101372 loss_G:0.003375\n",
      "epoch:457 loss_D:0.103558 loss_G:0.003323\n",
      "epoch:458 loss_D:0.104428 loss_G:0.003386\n",
      "epoch:459 loss_D:0.108598 loss_G:0.003310\n",
      "epoch:460 loss_D:0.110252 loss_G:0.003259\n",
      "epoch:461 loss_D:0.113594 loss_G:0.003331\n",
      "epoch:462 loss_D:0.101063 loss_G:0.003370\n",
      "epoch:463 loss_D:0.108570 loss_G:0.003295\n",
      "epoch:464 loss_D:0.107761 loss_G:0.003295\n",
      "epoch:465 loss_D:0.107910 loss_G:0.003323\n",
      "epoch:466 loss_D:0.110841 loss_G:0.003289\n",
      "epoch:467 loss_D:0.104839 loss_G:0.003320\n",
      "epoch:468 loss_D:0.109534 loss_G:0.003245\n",
      "epoch:469 loss_D:0.103267 loss_G:0.003211\n",
      "epoch:470 loss_D:0.098277 loss_G:0.003277\n",
      "epoch:471 loss_D:0.109897 loss_G:0.003220\n",
      "epoch:472 loss_D:0.101593 loss_G:0.003229\n",
      "epoch:473 loss_D:0.100622 loss_G:0.003213\n",
      "epoch:474 loss_D:0.107225 loss_G:0.003236\n",
      "epoch:475 loss_D:0.105650 loss_G:0.003283\n",
      "epoch:476 loss_D:0.108949 loss_G:0.003232\n",
      "epoch:477 loss_D:0.106013 loss_G:0.003215\n",
      "epoch:478 loss_D:0.105802 loss_G:0.003156\n",
      "epoch:479 loss_D:0.103200 loss_G:0.003206\n",
      "epoch:480 loss_D:0.099675 loss_G:0.003220\n",
      "epoch:481 loss_D:0.109136 loss_G:0.003258\n",
      "epoch:482 loss_D:0.103531 loss_G:0.003222\n",
      "epoch:483 loss_D:0.105149 loss_G:0.003208\n",
      "epoch:484 loss_D:0.107796 loss_G:0.003163\n",
      "epoch:485 loss_D:0.114146 loss_G:0.003140\n",
      "epoch:486 loss_D:0.106203 loss_G:0.003216\n",
      "epoch:487 loss_D:0.102466 loss_G:0.003108\n",
      "epoch:488 loss_D:0.111602 loss_G:0.003168\n",
      "epoch:489 loss_D:0.103620 loss_G:0.003097\n",
      "epoch:490 loss_D:0.101596 loss_G:0.003178\n",
      "epoch:491 loss_D:0.115524 loss_G:0.003119\n",
      "epoch:492 loss_D:0.104812 loss_G:0.003143\n",
      "epoch:493 loss_D:0.097440 loss_G:0.003098\n",
      "epoch:494 loss_D:0.102428 loss_G:0.003099\n",
      "epoch:495 loss_D:0.107800 loss_G:0.003066\n",
      "epoch:496 loss_D:0.111056 loss_G:0.003111\n",
      "epoch:497 loss_D:0.111358 loss_G:0.003109\n",
      "epoch:498 loss_D:0.099355 loss_G:0.003100\n",
      "epoch:499 loss_D:0.107264 loss_G:0.003085\n",
      "epoch:500 loss_D:0.102846 loss_G:0.003130\n",
      "epoch:501 loss_D:0.109697 loss_G:0.003059\n",
      "epoch:502 loss_D:0.104672 loss_G:0.003047\n",
      "epoch:503 loss_D:0.101850 loss_G:0.003052\n",
      "epoch:504 loss_D:0.112956 loss_G:0.003059\n",
      "epoch:505 loss_D:0.100272 loss_G:0.003045\n",
      "epoch:506 loss_D:0.106766 loss_G:0.003045\n",
      "epoch:507 loss_D:0.108472 loss_G:0.003038\n",
      "epoch:508 loss_D:0.106479 loss_G:0.003027\n",
      "epoch:509 loss_D:0.102142 loss_G:0.003002\n",
      "epoch:510 loss_D:0.103487 loss_G:0.003041\n",
      "epoch:511 loss_D:0.105890 loss_G:0.003008\n",
      "epoch:512 loss_D:0.102085 loss_G:0.002989\n",
      "epoch:513 loss_D:0.107321 loss_G:0.002975\n",
      "epoch:514 loss_D:0.111480 loss_G:0.002938\n",
      "epoch:515 loss_D:0.100413 loss_G:0.002950\n",
      "epoch:516 loss_D:0.103959 loss_G:0.002962\n",
      "epoch:517 loss_D:0.112673 loss_G:0.002984\n",
      "epoch:518 loss_D:0.109304 loss_G:0.002940\n",
      "epoch:519 loss_D:0.106997 loss_G:0.002887\n",
      "epoch:520 loss_D:0.099657 loss_G:0.002925\n",
      "epoch:521 loss_D:0.102241 loss_G:0.002936\n",
      "epoch:522 loss_D:0.110371 loss_G:0.002915\n",
      "epoch:523 loss_D:0.108177 loss_G:0.002977\n",
      "epoch:524 loss_D:0.097177 loss_G:0.002927\n",
      "epoch:525 loss_D:0.107928 loss_G:0.002911\n",
      "epoch:526 loss_D:0.096285 loss_G:0.002940\n",
      "epoch:527 loss_D:0.100311 loss_G:0.002926\n",
      "epoch:528 loss_D:0.108572 loss_G:0.002915\n",
      "epoch:529 loss_D:0.098945 loss_G:0.002948\n",
      "epoch:530 loss_D:0.097736 loss_G:0.002906\n",
      "epoch:531 loss_D:0.106312 loss_G:0.002860\n",
      "epoch:532 loss_D:0.102320 loss_G:0.002872\n",
      "epoch:533 loss_D:0.107372 loss_G:0.002906\n",
      "epoch:534 loss_D:0.102319 loss_G:0.002871\n",
      "epoch:535 loss_D:0.097665 loss_G:0.002907\n",
      "epoch:536 loss_D:0.102531 loss_G:0.002836\n",
      "epoch:537 loss_D:0.104814 loss_G:0.002838\n",
      "epoch:538 loss_D:0.102398 loss_G:0.002865\n",
      "epoch:539 loss_D:0.100015 loss_G:0.002844\n",
      "epoch:540 loss_D:0.101501 loss_G:0.002853\n",
      "epoch:541 loss_D:0.100698 loss_G:0.002833\n",
      "epoch:542 loss_D:0.099658 loss_G:0.002848\n",
      "epoch:543 loss_D:0.103070 loss_G:0.002819\n",
      "epoch:544 loss_D:0.105872 loss_G:0.002810\n",
      "epoch:545 loss_D:0.104345 loss_G:0.002768\n",
      "epoch:546 loss_D:0.101631 loss_G:0.002842\n",
      "epoch:547 loss_D:0.100442 loss_G:0.002781\n",
      "epoch:548 loss_D:0.094876 loss_G:0.002769\n",
      "epoch:549 loss_D:0.101738 loss_G:0.002773\n",
      "epoch:550 loss_D:0.100479 loss_G:0.002732\n",
      "epoch:551 loss_D:0.104167 loss_G:0.002762\n",
      "epoch:552 loss_D:0.102072 loss_G:0.002792\n",
      "epoch:553 loss_D:0.103260 loss_G:0.002739\n",
      "epoch:554 loss_D:0.102248 loss_G:0.002750\n",
      "epoch:555 loss_D:0.106842 loss_G:0.002760\n",
      "epoch:556 loss_D:0.105133 loss_G:0.002771\n",
      "epoch:557 loss_D:0.103388 loss_G:0.002760\n",
      "epoch:558 loss_D:0.100958 loss_G:0.002741\n",
      "epoch:559 loss_D:0.099110 loss_G:0.002782\n",
      "epoch:560 loss_D:0.097770 loss_G:0.002747\n",
      "epoch:561 loss_D:0.103117 loss_G:0.002750\n",
      "epoch:562 loss_D:0.104561 loss_G:0.002739\n",
      "epoch:563 loss_D:0.104435 loss_G:0.002705\n",
      "epoch:564 loss_D:0.099633 loss_G:0.002689\n",
      "epoch:565 loss_D:0.095913 loss_G:0.002683\n",
      "epoch:566 loss_D:0.106038 loss_G:0.002719\n",
      "epoch:567 loss_D:0.105880 loss_G:0.002738\n",
      "epoch:568 loss_D:0.095851 loss_G:0.002692\n",
      "epoch:569 loss_D:0.099207 loss_G:0.002674\n",
      "epoch:570 loss_D:0.095603 loss_G:0.002640\n",
      "epoch:571 loss_D:0.098576 loss_G:0.002678\n",
      "epoch:572 loss_D:0.098178 loss_G:0.002645\n",
      "epoch:573 loss_D:0.099416 loss_G:0.002665\n",
      "epoch:574 loss_D:0.094195 loss_G:0.002629\n",
      "epoch:575 loss_D:0.105255 loss_G:0.002676\n",
      "epoch:576 loss_D:0.096377 loss_G:0.002636\n",
      "epoch:577 loss_D:0.106520 loss_G:0.002664\n",
      "epoch:578 loss_D:0.102743 loss_G:0.002617\n",
      "epoch:579 loss_D:0.102233 loss_G:0.002662\n",
      "epoch:580 loss_D:0.097120 loss_G:0.002656\n",
      "epoch:581 loss_D:0.099537 loss_G:0.002628\n",
      "epoch:582 loss_D:0.098675 loss_G:0.002591\n",
      "epoch:583 loss_D:0.101688 loss_G:0.002610\n",
      "epoch:584 loss_D:0.097976 loss_G:0.002639\n",
      "epoch:585 loss_D:0.103567 loss_G:0.002596\n",
      "epoch:586 loss_D:0.101946 loss_G:0.002587\n",
      "epoch:587 loss_D:0.104618 loss_G:0.002619\n",
      "epoch:588 loss_D:0.103139 loss_G:0.002609\n",
      "epoch:589 loss_D:0.095430 loss_G:0.002591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:590 loss_D:0.108587 loss_G:0.002590\n",
      "epoch:591 loss_D:0.094692 loss_G:0.002560\n",
      "epoch:592 loss_D:0.100704 loss_G:0.002582\n",
      "epoch:593 loss_D:0.100682 loss_G:0.002591\n",
      "epoch:594 loss_D:0.093396 loss_G:0.002546\n",
      "epoch:595 loss_D:0.102624 loss_G:0.002555\n",
      "epoch:596 loss_D:0.104266 loss_G:0.002547\n",
      "epoch:597 loss_D:0.098732 loss_G:0.002588\n",
      "epoch:598 loss_D:0.098669 loss_G:0.002536\n",
      "epoch:599 loss_D:0.102227 loss_G:0.002552\n",
      "epoch:600 loss_D:0.100574 loss_G:0.002500\n",
      "epoch:601 loss_D:0.098564 loss_G:0.002526\n",
      "epoch:602 loss_D:0.106293 loss_G:0.002517\n",
      "epoch:603 loss_D:0.101007 loss_G:0.002461\n",
      "epoch:604 loss_D:0.102762 loss_G:0.002500\n",
      "epoch:605 loss_D:0.095993 loss_G:0.002547\n",
      "epoch:606 loss_D:0.103725 loss_G:0.002459\n",
      "epoch:607 loss_D:0.101004 loss_G:0.002518\n",
      "epoch:608 loss_D:0.098070 loss_G:0.002499\n",
      "epoch:609 loss_D:0.106334 loss_G:0.002480\n",
      "epoch:610 loss_D:0.104363 loss_G:0.002477\n",
      "epoch:611 loss_D:0.095376 loss_G:0.002478\n",
      "epoch:612 loss_D:0.103375 loss_G:0.002473\n",
      "epoch:613 loss_D:0.101281 loss_G:0.002487\n",
      "epoch:614 loss_D:0.102144 loss_G:0.002495\n",
      "epoch:615 loss_D:0.102258 loss_G:0.002465\n",
      "epoch:616 loss_D:0.108405 loss_G:0.002432\n",
      "epoch:617 loss_D:0.101864 loss_G:0.002468\n",
      "epoch:618 loss_D:0.099310 loss_G:0.002472\n",
      "epoch:619 loss_D:0.096188 loss_G:0.002463\n",
      "epoch:620 loss_D:0.101067 loss_G:0.002411\n",
      "epoch:621 loss_D:0.100393 loss_G:0.002428\n",
      "epoch:622 loss_D:0.100946 loss_G:0.002447\n",
      "epoch:623 loss_D:0.107838 loss_G:0.002445\n",
      "epoch:624 loss_D:0.100602 loss_G:0.002458\n",
      "epoch:625 loss_D:0.100416 loss_G:0.002392\n",
      "epoch:626 loss_D:0.103454 loss_G:0.002422\n",
      "epoch:627 loss_D:0.095759 loss_G:0.002422\n",
      "epoch:628 loss_D:0.102202 loss_G:0.002437\n",
      "epoch:629 loss_D:0.099833 loss_G:0.002439\n",
      "epoch:630 loss_D:0.097564 loss_G:0.002348\n",
      "epoch:631 loss_D:0.102479 loss_G:0.002378\n",
      "epoch:632 loss_D:0.095657 loss_G:0.002406\n",
      "epoch:633 loss_D:0.100275 loss_G:0.002404\n",
      "epoch:634 loss_D:0.094938 loss_G:0.002351\n",
      "epoch:635 loss_D:0.097709 loss_G:0.002385\n",
      "epoch:636 loss_D:0.105129 loss_G:0.002376\n",
      "epoch:637 loss_D:0.100105 loss_G:0.002382\n",
      "epoch:638 loss_D:0.099256 loss_G:0.002357\n",
      "epoch:639 loss_D:0.101812 loss_G:0.002361\n",
      "epoch:640 loss_D:0.095186 loss_G:0.002356\n",
      "epoch:641 loss_D:0.103782 loss_G:0.002381\n",
      "epoch:642 loss_D:0.100329 loss_G:0.002366\n",
      "epoch:643 loss_D:0.106280 loss_G:0.002349\n",
      "epoch:644 loss_D:0.101466 loss_G:0.002328\n",
      "epoch:645 loss_D:0.096114 loss_G:0.002343\n",
      "epoch:646 loss_D:0.099325 loss_G:0.002333\n",
      "epoch:647 loss_D:0.102596 loss_G:0.002297\n",
      "epoch:648 loss_D:0.098409 loss_G:0.002299\n",
      "epoch:649 loss_D:0.103756 loss_G:0.002355\n",
      "epoch:650 loss_D:0.098193 loss_G:0.002335\n",
      "epoch:651 loss_D:0.095343 loss_G:0.002309\n",
      "epoch:652 loss_D:0.099775 loss_G:0.002295\n",
      "epoch:653 loss_D:0.098453 loss_G:0.002322\n",
      "epoch:654 loss_D:0.101658 loss_G:0.002348\n",
      "epoch:655 loss_D:0.096913 loss_G:0.002270\n",
      "epoch:656 loss_D:0.094463 loss_G:0.002274\n",
      "epoch:657 loss_D:0.099083 loss_G:0.002272\n",
      "epoch:658 loss_D:0.095345 loss_G:0.002273\n",
      "epoch:659 loss_D:0.101674 loss_G:0.002280\n",
      "epoch:660 loss_D:0.094944 loss_G:0.002261\n",
      "epoch:661 loss_D:0.100461 loss_G:0.002303\n",
      "epoch:662 loss_D:0.104647 loss_G:0.002277\n",
      "epoch:663 loss_D:0.101580 loss_G:0.002286\n",
      "epoch:664 loss_D:0.105056 loss_G:0.002229\n",
      "epoch:665 loss_D:0.093983 loss_G:0.002239\n",
      "epoch:666 loss_D:0.104805 loss_G:0.002220\n",
      "epoch:667 loss_D:0.100981 loss_G:0.002276\n",
      "epoch:668 loss_D:0.102691 loss_G:0.002223\n",
      "epoch:669 loss_D:0.097918 loss_G:0.002275\n",
      "epoch:670 loss_D:0.094474 loss_G:0.002267\n",
      "epoch:671 loss_D:0.108799 loss_G:0.002243\n",
      "epoch:672 loss_D:0.096666 loss_G:0.002221\n",
      "epoch:673 loss_D:0.102024 loss_G:0.002179\n",
      "epoch:674 loss_D:0.099311 loss_G:0.002242\n",
      "epoch:675 loss_D:0.099507 loss_G:0.002226\n",
      "epoch:676 loss_D:0.098545 loss_G:0.002227\n",
      "epoch:677 loss_D:0.095554 loss_G:0.002218\n",
      "epoch:678 loss_D:0.093415 loss_G:0.002201\n",
      "epoch:679 loss_D:0.092795 loss_G:0.002245\n",
      "epoch:680 loss_D:0.096960 loss_G:0.002204\n",
      "epoch:681 loss_D:0.101947 loss_G:0.002159\n",
      "epoch:682 loss_D:0.102964 loss_G:0.002196\n",
      "epoch:683 loss_D:0.096258 loss_G:0.002180\n",
      "epoch:684 loss_D:0.096533 loss_G:0.002161\n",
      "epoch:685 loss_D:0.101004 loss_G:0.002187\n",
      "epoch:686 loss_D:0.101623 loss_G:0.002199\n",
      "epoch:687 loss_D:0.097922 loss_G:0.002164\n",
      "epoch:688 loss_D:0.101414 loss_G:0.002160\n",
      "epoch:689 loss_D:0.098337 loss_G:0.002155\n",
      "epoch:690 loss_D:0.093710 loss_G:0.002163\n",
      "epoch:691 loss_D:0.096992 loss_G:0.002161\n",
      "epoch:692 loss_D:0.093168 loss_G:0.002138\n",
      "epoch:693 loss_D:0.097596 loss_G:0.002160\n",
      "epoch:694 loss_D:0.096376 loss_G:0.002150\n",
      "epoch:695 loss_D:0.090791 loss_G:0.002156\n",
      "epoch:696 loss_D:0.101994 loss_G:0.002119\n",
      "epoch:697 loss_D:0.092593 loss_G:0.002143\n",
      "epoch:698 loss_D:0.100956 loss_G:0.002177\n",
      "epoch:699 loss_D:0.091619 loss_G:0.002101\n",
      "epoch:700 loss_D:0.095734 loss_G:0.002137\n",
      "epoch:701 loss_D:0.103150 loss_G:0.002143\n",
      "epoch:702 loss_D:0.088181 loss_G:0.002160\n",
      "epoch:703 loss_D:0.101710 loss_G:0.002093\n",
      "epoch:704 loss_D:0.098399 loss_G:0.002147\n",
      "epoch:705 loss_D:0.092071 loss_G:0.002074\n",
      "epoch:706 loss_D:0.097725 loss_G:0.002137\n",
      "epoch:707 loss_D:0.104002 loss_G:0.002097\n",
      "epoch:708 loss_D:0.098117 loss_G:0.002101\n",
      "epoch:709 loss_D:0.098785 loss_G:0.002103\n",
      "epoch:710 loss_D:0.093422 loss_G:0.002102\n",
      "epoch:711 loss_D:0.102659 loss_G:0.002096\n",
      "epoch:712 loss_D:0.096543 loss_G:0.002083\n",
      "epoch:713 loss_D:0.096036 loss_G:0.002122\n",
      "epoch:714 loss_D:0.097123 loss_G:0.002093\n",
      "epoch:715 loss_D:0.088611 loss_G:0.002068\n",
      "epoch:716 loss_D:0.102445 loss_G:0.002067\n",
      "epoch:717 loss_D:0.091109 loss_G:0.002078\n",
      "epoch:718 loss_D:0.098773 loss_G:0.002065\n",
      "epoch:719 loss_D:0.092153 loss_G:0.002041\n",
      "epoch:720 loss_D:0.099289 loss_G:0.002064\n",
      "epoch:721 loss_D:0.095298 loss_G:0.002059\n",
      "epoch:722 loss_D:0.094987 loss_G:0.002038\n",
      "epoch:723 loss_D:0.105912 loss_G:0.002050\n",
      "epoch:724 loss_D:0.095272 loss_G:0.002055\n",
      "epoch:725 loss_D:0.101534 loss_G:0.002058\n",
      "epoch:726 loss_D:0.097283 loss_G:0.002035\n",
      "epoch:727 loss_D:0.097927 loss_G:0.002048\n",
      "epoch:728 loss_D:0.086938 loss_G:0.002061\n",
      "epoch:729 loss_D:0.102396 loss_G:0.002028\n",
      "epoch:730 loss_D:0.095282 loss_G:0.002042\n",
      "epoch:731 loss_D:0.094277 loss_G:0.002039\n",
      "epoch:732 loss_D:0.094890 loss_G:0.002028\n",
      "epoch:733 loss_D:0.104801 loss_G:0.002016\n",
      "epoch:734 loss_D:0.095937 loss_G:0.002015\n",
      "epoch:735 loss_D:0.096803 loss_G:0.002005\n",
      "epoch:736 loss_D:0.100847 loss_G:0.002026\n",
      "epoch:737 loss_D:0.096472 loss_G:0.001999\n",
      "epoch:738 loss_D:0.095747 loss_G:0.002013\n",
      "epoch:739 loss_D:0.100076 loss_G:0.001995\n",
      "epoch:740 loss_D:0.095957 loss_G:0.002005\n",
      "epoch:741 loss_D:0.094803 loss_G:0.002029\n",
      "epoch:742 loss_D:0.093251 loss_G:0.002013\n",
      "epoch:743 loss_D:0.090940 loss_G:0.002017\n",
      "epoch:744 loss_D:0.101459 loss_G:0.001993\n",
      "epoch:745 loss_D:0.089842 loss_G:0.001972\n",
      "epoch:746 loss_D:0.092544 loss_G:0.001981\n",
      "epoch:747 loss_D:0.088042 loss_G:0.001940\n",
      "epoch:748 loss_D:0.104170 loss_G:0.001978\n",
      "epoch:749 loss_D:0.091359 loss_G:0.001975\n",
      "epoch:750 loss_D:0.089859 loss_G:0.001965\n",
      "epoch:751 loss_D:0.095662 loss_G:0.001957\n",
      "epoch:752 loss_D:0.092941 loss_G:0.001963\n",
      "epoch:753 loss_D:0.094978 loss_G:0.001944\n",
      "epoch:754 loss_D:0.096030 loss_G:0.001966\n",
      "epoch:755 loss_D:0.093344 loss_G:0.001938\n",
      "epoch:756 loss_D:0.096955 loss_G:0.001937\n",
      "epoch:757 loss_D:0.097790 loss_G:0.001959\n",
      "epoch:758 loss_D:0.101394 loss_G:0.001917\n",
      "epoch:759 loss_D:0.097880 loss_G:0.001959\n",
      "epoch:760 loss_D:0.094494 loss_G:0.001934\n",
      "epoch:761 loss_D:0.091547 loss_G:0.001938\n",
      "epoch:762 loss_D:0.096343 loss_G:0.001906\n",
      "epoch:763 loss_D:0.095641 loss_G:0.001904\n",
      "epoch:764 loss_D:0.096636 loss_G:0.001938\n",
      "epoch:765 loss_D:0.093990 loss_G:0.001922\n",
      "epoch:766 loss_D:0.101056 loss_G:0.001917\n",
      "epoch:767 loss_D:0.095162 loss_G:0.001924\n",
      "epoch:768 loss_D:0.097680 loss_G:0.001925\n",
      "epoch:769 loss_D:0.096129 loss_G:0.001918\n",
      "epoch:770 loss_D:0.095637 loss_G:0.001892\n",
      "epoch:771 loss_D:0.094673 loss_G:0.001894\n",
      "epoch:772 loss_D:0.092477 loss_G:0.001881\n",
      "epoch:773 loss_D:0.094291 loss_G:0.001872\n",
      "epoch:774 loss_D:0.088442 loss_G:0.001915\n",
      "epoch:775 loss_D:0.089743 loss_G:0.001877\n",
      "epoch:776 loss_D:0.094486 loss_G:0.001896\n",
      "epoch:777 loss_D:0.092551 loss_G:0.001878\n",
      "epoch:778 loss_D:0.095495 loss_G:0.001868\n",
      "epoch:779 loss_D:0.099370 loss_G:0.001896\n",
      "epoch:780 loss_D:0.094949 loss_G:0.001903\n",
      "epoch:781 loss_D:0.094781 loss_G:0.001876\n",
      "epoch:782 loss_D:0.089935 loss_G:0.001907\n",
      "epoch:783 loss_D:0.096844 loss_G:0.001850\n",
      "epoch:784 loss_D:0.094547 loss_G:0.001879\n",
      "epoch:785 loss_D:0.099673 loss_G:0.001856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:786 loss_D:0.102410 loss_G:0.001838\n",
      "epoch:787 loss_D:0.093282 loss_G:0.001824\n",
      "epoch:788 loss_D:0.091094 loss_G:0.001843\n",
      "epoch:789 loss_D:0.090603 loss_G:0.001853\n",
      "epoch:790 loss_D:0.095700 loss_G:0.001840\n",
      "epoch:791 loss_D:0.090215 loss_G:0.001881\n",
      "epoch:792 loss_D:0.094849 loss_G:0.001835\n",
      "epoch:793 loss_D:0.093798 loss_G:0.001825\n",
      "epoch:794 loss_D:0.091493 loss_G:0.001847\n",
      "epoch:795 loss_D:0.092975 loss_G:0.001812\n",
      "epoch:796 loss_D:0.090450 loss_G:0.001844\n",
      "epoch:797 loss_D:0.095579 loss_G:0.001851\n",
      "epoch:798 loss_D:0.097631 loss_G:0.001849\n",
      "epoch:799 loss_D:0.097484 loss_G:0.001845\n",
      "epoch:800 loss_D:0.089971 loss_G:0.001786\n",
      "epoch:801 loss_D:0.095923 loss_G:0.001783\n",
      "epoch:802 loss_D:0.091206 loss_G:0.001821\n",
      "epoch:803 loss_D:0.098190 loss_G:0.001814\n",
      "epoch:804 loss_D:0.086182 loss_G:0.001795\n",
      "epoch:805 loss_D:0.092913 loss_G:0.001793\n",
      "epoch:806 loss_D:0.101296 loss_G:0.001821\n",
      "epoch:807 loss_D:0.094094 loss_G:0.001788\n",
      "epoch:808 loss_D:0.095503 loss_G:0.001802\n",
      "epoch:809 loss_D:0.091857 loss_G:0.001804\n",
      "epoch:810 loss_D:0.093218 loss_G:0.001793\n",
      "epoch:811 loss_D:0.093638 loss_G:0.001815\n",
      "epoch:812 loss_D:0.092145 loss_G:0.001790\n",
      "epoch:813 loss_D:0.093809 loss_G:0.001786\n",
      "epoch:814 loss_D:0.096347 loss_G:0.001777\n",
      "epoch:815 loss_D:0.093729 loss_G:0.001796\n",
      "epoch:816 loss_D:0.099077 loss_G:0.001779\n",
      "epoch:817 loss_D:0.093736 loss_G:0.001809\n",
      "epoch:818 loss_D:0.092767 loss_G:0.001780\n",
      "epoch:819 loss_D:0.089617 loss_G:0.001779\n",
      "epoch:820 loss_D:0.096131 loss_G:0.001781\n",
      "epoch:821 loss_D:0.092572 loss_G:0.001764\n",
      "epoch:822 loss_D:0.092388 loss_G:0.001748\n",
      "epoch:823 loss_D:0.093929 loss_G:0.001761\n",
      "epoch:824 loss_D:0.089257 loss_G:0.001766\n",
      "epoch:825 loss_D:0.095736 loss_G:0.001775\n",
      "epoch:826 loss_D:0.092428 loss_G:0.001754\n",
      "epoch:827 loss_D:0.095090 loss_G:0.001742\n",
      "epoch:828 loss_D:0.088304 loss_G:0.001731\n",
      "epoch:829 loss_D:0.094571 loss_G:0.001768\n",
      "epoch:830 loss_D:0.097312 loss_G:0.001751\n",
      "epoch:831 loss_D:0.099189 loss_G:0.001765\n",
      "epoch:832 loss_D:0.089372 loss_G:0.001741\n",
      "epoch:833 loss_D:0.095092 loss_G:0.001740\n",
      "epoch:834 loss_D:0.094984 loss_G:0.001707\n",
      "epoch:835 loss_D:0.092734 loss_G:0.001730\n",
      "epoch:836 loss_D:0.100415 loss_G:0.001751\n",
      "epoch:837 loss_D:0.092792 loss_G:0.001735\n",
      "epoch:838 loss_D:0.091968 loss_G:0.001741\n",
      "epoch:839 loss_D:0.093189 loss_G:0.001742\n",
      "epoch:840 loss_D:0.093234 loss_G:0.001730\n",
      "epoch:841 loss_D:0.090841 loss_G:0.001716\n",
      "epoch:842 loss_D:0.095941 loss_G:0.001670\n",
      "epoch:843 loss_D:0.087773 loss_G:0.001720\n",
      "epoch:844 loss_D:0.092399 loss_G:0.001713\n",
      "epoch:845 loss_D:0.092665 loss_G:0.001721\n",
      "epoch:846 loss_D:0.099733 loss_G:0.001706\n",
      "epoch:847 loss_D:0.097608 loss_G:0.001699\n",
      "epoch:848 loss_D:0.096466 loss_G:0.001676\n",
      "epoch:849 loss_D:0.098319 loss_G:0.001722\n",
      "epoch:850 loss_D:0.096182 loss_G:0.001693\n",
      "epoch:851 loss_D:0.087500 loss_G:0.001693\n",
      "epoch:852 loss_D:0.097212 loss_G:0.001669\n",
      "epoch:853 loss_D:0.096336 loss_G:0.001673\n",
      "epoch:854 loss_D:0.091524 loss_G:0.001734\n",
      "epoch:855 loss_D:0.092450 loss_G:0.001683\n",
      "epoch:856 loss_D:0.096763 loss_G:0.001664\n",
      "epoch:857 loss_D:0.096263 loss_G:0.001685\n",
      "epoch:858 loss_D:0.090532 loss_G:0.001674\n",
      "epoch:859 loss_D:0.094003 loss_G:0.001663\n",
      "epoch:860 loss_D:0.093846 loss_G:0.001665\n",
      "epoch:861 loss_D:0.086771 loss_G:0.001674\n",
      "epoch:862 loss_D:0.092825 loss_G:0.001648\n",
      "epoch:863 loss_D:0.087824 loss_G:0.001664\n",
      "epoch:864 loss_D:0.094284 loss_G:0.001677\n",
      "epoch:865 loss_D:0.090580 loss_G:0.001659\n",
      "epoch:866 loss_D:0.095658 loss_G:0.001642\n",
      "epoch:867 loss_D:0.094762 loss_G:0.001640\n",
      "epoch:868 loss_D:0.094042 loss_G:0.001693\n",
      "epoch:869 loss_D:0.093061 loss_G:0.001688\n",
      "epoch:870 loss_D:0.086554 loss_G:0.001638\n",
      "epoch:871 loss_D:0.098622 loss_G:0.001636\n",
      "epoch:872 loss_D:0.094146 loss_G:0.001641\n",
      "epoch:873 loss_D:0.095235 loss_G:0.001632\n",
      "epoch:874 loss_D:0.088516 loss_G:0.001657\n",
      "epoch:875 loss_D:0.088683 loss_G:0.001619\n",
      "epoch:876 loss_D:0.090693 loss_G:0.001619\n",
      "epoch:877 loss_D:0.086890 loss_G:0.001627\n",
      "epoch:878 loss_D:0.095731 loss_G:0.001608\n",
      "epoch:879 loss_D:0.097553 loss_G:0.001642\n",
      "epoch:880 loss_D:0.087528 loss_G:0.001602\n",
      "epoch:881 loss_D:0.098315 loss_G:0.001642\n",
      "epoch:882 loss_D:0.092791 loss_G:0.001622\n",
      "epoch:883 loss_D:0.091295 loss_G:0.001623\n",
      "epoch:884 loss_D:0.092921 loss_G:0.001617\n",
      "epoch:885 loss_D:0.090370 loss_G:0.001615\n",
      "epoch:886 loss_D:0.094226 loss_G:0.001618\n",
      "epoch:887 loss_D:0.090783 loss_G:0.001621\n",
      "epoch:888 loss_D:0.085212 loss_G:0.001598\n",
      "epoch:889 loss_D:0.091332 loss_G:0.001598\n",
      "epoch:890 loss_D:0.090215 loss_G:0.001580\n",
      "epoch:891 loss_D:0.090453 loss_G:0.001609\n",
      "epoch:892 loss_D:0.093741 loss_G:0.001595\n",
      "epoch:893 loss_D:0.094089 loss_G:0.001575\n",
      "epoch:894 loss_D:0.090116 loss_G:0.001574\n",
      "epoch:895 loss_D:0.088986 loss_G:0.001595\n",
      "epoch:896 loss_D:0.087531 loss_G:0.001580\n",
      "epoch:897 loss_D:0.091767 loss_G:0.001602\n",
      "epoch:898 loss_D:0.089061 loss_G:0.001600\n",
      "epoch:899 loss_D:0.087940 loss_G:0.001596\n",
      "epoch:900 loss_D:0.096882 loss_G:0.001568\n",
      "epoch:901 loss_D:0.091653 loss_G:0.001604\n",
      "epoch:902 loss_D:0.092829 loss_G:0.001574\n",
      "epoch:903 loss_D:0.090963 loss_G:0.001588\n",
      "epoch:904 loss_D:0.091523 loss_G:0.001573\n",
      "epoch:905 loss_D:0.093355 loss_G:0.001559\n",
      "epoch:906 loss_D:0.084902 loss_G:0.001567\n",
      "epoch:907 loss_D:0.088600 loss_G:0.001567\n",
      "epoch:908 loss_D:0.091445 loss_G:0.001566\n",
      "epoch:909 loss_D:0.091087 loss_G:0.001540\n",
      "epoch:910 loss_D:0.089459 loss_G:0.001553\n",
      "epoch:911 loss_D:0.087745 loss_G:0.001530\n",
      "epoch:912 loss_D:0.096501 loss_G:0.001518\n",
      "epoch:913 loss_D:0.094283 loss_G:0.001563\n",
      "epoch:914 loss_D:0.087548 loss_G:0.001546\n",
      "epoch:915 loss_D:0.094549 loss_G:0.001535\n",
      "epoch:916 loss_D:0.089521 loss_G:0.001557\n",
      "epoch:917 loss_D:0.089705 loss_G:0.001545\n",
      "epoch:918 loss_D:0.094509 loss_G:0.001535\n",
      "epoch:919 loss_D:0.097274 loss_G:0.001535\n",
      "epoch:920 loss_D:0.091233 loss_G:0.001502\n",
      "epoch:921 loss_D:0.092637 loss_G:0.001541\n",
      "epoch:922 loss_D:0.095560 loss_G:0.001532\n",
      "epoch:923 loss_D:0.085220 loss_G:0.001537\n",
      "epoch:924 loss_D:0.088107 loss_G:0.001531\n",
      "epoch:925 loss_D:0.092421 loss_G:0.001529\n",
      "epoch:926 loss_D:0.092525 loss_G:0.001529\n",
      "epoch:927 loss_D:0.093545 loss_G:0.001527\n",
      "epoch:928 loss_D:0.097879 loss_G:0.001526\n",
      "epoch:929 loss_D:0.090195 loss_G:0.001496\n",
      "epoch:930 loss_D:0.090579 loss_G:0.001499\n",
      "epoch:931 loss_D:0.091195 loss_G:0.001526\n",
      "epoch:932 loss_D:0.089121 loss_G:0.001517\n",
      "epoch:933 loss_D:0.089193 loss_G:0.001535\n",
      "epoch:934 loss_D:0.096336 loss_G:0.001538\n",
      "epoch:935 loss_D:0.096422 loss_G:0.001517\n",
      "epoch:936 loss_D:0.089279 loss_G:0.001502\n",
      "epoch:937 loss_D:0.090176 loss_G:0.001486\n",
      "epoch:938 loss_D:0.084021 loss_G:0.001502\n",
      "epoch:939 loss_D:0.086633 loss_G:0.001492\n",
      "epoch:940 loss_D:0.087361 loss_G:0.001498\n",
      "epoch:941 loss_D:0.083536 loss_G:0.001503\n",
      "epoch:942 loss_D:0.086294 loss_G:0.001508\n",
      "epoch:943 loss_D:0.084912 loss_G:0.001490\n",
      "epoch:944 loss_D:0.091769 loss_G:0.001510\n",
      "epoch:945 loss_D:0.089817 loss_G:0.001488\n",
      "epoch:946 loss_D:0.090151 loss_G:0.001454\n",
      "epoch:947 loss_D:0.087186 loss_G:0.001452\n",
      "epoch:948 loss_D:0.092450 loss_G:0.001486\n",
      "epoch:949 loss_D:0.091318 loss_G:0.001496\n",
      "epoch:950 loss_D:0.089796 loss_G:0.001484\n",
      "epoch:951 loss_D:0.092567 loss_G:0.001514\n",
      "epoch:952 loss_D:0.093697 loss_G:0.001484\n",
      "epoch:953 loss_D:0.091611 loss_G:0.001480\n",
      "epoch:954 loss_D:0.101093 loss_G:0.001472\n",
      "epoch:955 loss_D:0.087450 loss_G:0.001465\n",
      "epoch:956 loss_D:0.093111 loss_G:0.001486\n",
      "epoch:957 loss_D:0.095869 loss_G:0.001456\n",
      "epoch:958 loss_D:0.093524 loss_G:0.001451\n",
      "epoch:959 loss_D:0.092457 loss_G:0.001460\n",
      "epoch:960 loss_D:0.094013 loss_G:0.001456\n",
      "epoch:961 loss_D:0.092033 loss_G:0.001472\n",
      "epoch:962 loss_D:0.095535 loss_G:0.001460\n",
      "epoch:963 loss_D:0.093278 loss_G:0.001449\n",
      "epoch:964 loss_D:0.090836 loss_G:0.001444\n",
      "epoch:965 loss_D:0.087150 loss_G:0.001444\n",
      "epoch:966 loss_D:0.091671 loss_G:0.001400\n",
      "epoch:967 loss_D:0.085331 loss_G:0.001457\n",
      "epoch:968 loss_D:0.088452 loss_G:0.001469\n",
      "epoch:969 loss_D:0.094081 loss_G:0.001421\n",
      "epoch:970 loss_D:0.092160 loss_G:0.001448\n",
      "epoch:971 loss_D:0.085829 loss_G:0.001403\n",
      "epoch:972 loss_D:0.092756 loss_G:0.001409\n",
      "epoch:973 loss_D:0.095163 loss_G:0.001463\n",
      "epoch:974 loss_D:0.087774 loss_G:0.001441\n",
      "epoch:975 loss_D:0.092660 loss_G:0.001436\n",
      "epoch:976 loss_D:0.094198 loss_G:0.001418\n",
      "epoch:977 loss_D:0.088413 loss_G:0.001386\n",
      "epoch:978 loss_D:0.091775 loss_G:0.001412\n",
      "epoch:979 loss_D:0.090196 loss_G:0.001430\n",
      "epoch:980 loss_D:0.089505 loss_G:0.001419\n",
      "epoch:981 loss_D:0.092009 loss_G:0.001416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:982 loss_D:0.088893 loss_G:0.001417\n",
      "epoch:983 loss_D:0.087907 loss_G:0.001406\n",
      "epoch:984 loss_D:0.088518 loss_G:0.001402\n",
      "epoch:985 loss_D:0.089701 loss_G:0.001415\n",
      "epoch:986 loss_D:0.087453 loss_G:0.001369\n",
      "epoch:987 loss_D:0.092119 loss_G:0.001409\n",
      "epoch:988 loss_D:0.096502 loss_G:0.001420\n",
      "epoch:989 loss_D:0.095783 loss_G:0.001396\n",
      "epoch:990 loss_D:0.092756 loss_G:0.001401\n",
      "epoch:991 loss_D:0.093843 loss_G:0.001399\n",
      "epoch:992 loss_D:0.092321 loss_G:0.001365\n",
      "epoch:993 loss_D:0.096396 loss_G:0.001414\n",
      "epoch:994 loss_D:0.091100 loss_G:0.001401\n",
      "epoch:995 loss_D:0.088427 loss_G:0.001394\n",
      "epoch:996 loss_D:0.091272 loss_G:0.001361\n",
      "epoch:997 loss_D:0.090497 loss_G:0.001399\n",
      "epoch:998 loss_D:0.091405 loss_G:0.001383\n",
      "epoch:999 loss_D:0.090404 loss_G:0.001396\n",
      "epoch:1000 loss_D:0.087295 loss_G:0.001386\n"
     ]
    }
   ],
   "source": [
    "lr_decay_step = 0\n",
    "\n",
    "#last_m_global = np.Inf\n",
    "#\n",
    "#test_use = len(IMAGES_PATH)//1000\n",
    "#\n",
    "#for ep in range(1 , 1002):\n",
    "#    np.random.seed(ep*100)\n",
    "#    \n",
    "#    zd = np.random.uniform(-1, 1, (test_use , LATENT_DIM) )\n",
    "#    zg = np.random.uniform(-1, 1, (test_use , LATENT_DIM) )\n",
    "#\n",
    "#    index_order = np.arange(test_use)\n",
    "#    np.random.shuffle(index_order)\n",
    "#    \n",
    "#    lr = max(0.0001*(0.9**lr_decay_step) , 0.00001)\n",
    "#    \n",
    "#    K.set_value(Generator.optimizer.lr , lr)\n",
    "#    K.set_value(Discrimminator.optimizer.lr , lr)\n",
    "#    \n",
    "#    m_global_history = []\n",
    "#    \n",
    "#    batch_len = test_use//BATCH_SIZE\n",
    "#    \n",
    "#    for b_idx in range(batch_len):\n",
    "#        index_list = index_order[b_idx*BATCH_SIZE : (b_idx+1)*BATCH_SIZE]\n",
    "#\n",
    "#        # training discriminator\n",
    "#        in_x1 = load_image_new(index_list)  # (bs, row, col, ch)\n",
    "#        in_x2 = Generator.predict_on_batch(zd[index_list])\n",
    "#        \n",
    "#        in_x = np.concatenate([in_x1, in_x2], axis=-1)  # (bs, row, col, ch*2)\n",
    "#        loss_discriminator = Discrimminator.train_on_batch(in_x, in_x)\n",
    "#\n",
    "#        # training generator\n",
    "#        in_x1 = zg[index_list]\n",
    "#        loss_generator = Generator.train_on_batch(in_x1, np.zeros_like(in_x2))  # y_true is meaningless\n",
    "#\n",
    "#        print('epoch %d' % b_idx)\n",
    "#\n",
    "#    write_image(ep)\n",
    "#    lr_decay_step += 1\n",
    "    \n",
    "    \n",
    "for i in range(1001):\n",
    "    \n",
    "    lr = max(0.0001*(0.9**lr_decay_step) , 0.00001)\n",
    "    \n",
    "    K.set_value(Generator.optimizer.lr , lr)\n",
    "    K.set_value(Discrimminator.optimizer.lr , lr)\n",
    "    \n",
    "    #训练discriminator\n",
    "    input_x1 = load_image() #真实图像\n",
    "    input_x2 = Generator.predict(np.random.uniform(-1,1, size=(BATCH_SIZE , LATENT_DIM)))\n",
    "    input_x = np.concatenate((input_x1 , input_x2) , axis=-1)\n",
    "    \n",
    "    loss_d = Discrimminator.train_on_batch(input_x , input_x)\n",
    "\n",
    "    #训练generator\n",
    "    input_x3 = np.random.uniform(-1,1 , size=(BATCH_SIZE , LATENT_DIM))\n",
    "    loss_g = Generator.train_on_batch(input_x3 , np.zeros_like(input_x2)) #G向全0映射\n",
    "    \n",
    "    print('epoch:%d loss_D:%f loss_G:%f' % (i , loss_d , loss_g))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        write_image(i)\n",
    "    \n",
    "    \n",
    "    lr_decay_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dataset/CelebA/img_align_celeba\\\\122405.jpg'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGES_PATH[index_list[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IMAGES_PATH)//1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import google.protobuf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
