{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reference https://github.com/mokemokechicken/keras_BEGAN\n",
    "#reference https://github.com/siddharthalodha/BEGAN_KERAS/blob/master/BEGAN_v1.ipynb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU , PReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.initializers import truncated_normal , constant , random_normal\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "#残差块使用\n",
    "from keras.layers import Add\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#addin BEGAN\n",
    "from keras.layers import Lambda #可以自己指定特定层 完成特定的功能 discriminator使用\n",
    "from keras.layers import Concatenate #将discriminator的两个输出合并使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 64\n",
    "HEIGHT = 64\n",
    "CHANNEL = 3\n",
    "\n",
    "SHAPE = (WIDTH , HEIGHT , CHANNEL)\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 16 #crazy!!! slow turtle\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = '../dataset/CelebA/img_align_celeba/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 2\n",
    "COL = 3\n",
    "\n",
    "#addin BEGAN\n",
    "N_FILTERS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============\n",
    "IMAGES_PATH = glob(PATH+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(batch_size = BATCH_SIZE , training = True):\n",
    "    #随机在图片库中挑选\n",
    "    images = np.random.choice(IMAGES_PATH , size=batch_size)\n",
    "    \n",
    "    images_high_resolution = []\n",
    "    images_low_resolution = []\n",
    "    \n",
    "    for image in images:\n",
    "        img = scipy.misc.imread(image , mode='RGB').astype(np.float)\n",
    "        \n",
    "        #尽管原图像不是指定的大小 下面将强制将图像resize\n",
    "        img_high_resolution = scipy.misc.imresize(img , size=HIGH_RESOLUTION_SHAPE)\n",
    "        img_low_resolution = scipy.misc.imresize(img , size=LOW_RESOLUTION_SHAPE)\n",
    "        \n",
    "        #随机性地对训练样本进行 左右反转\n",
    "        if training and np.random.random()<0.5:\n",
    "            img_high_resolution = np.fliplr(img_high_resolution)\n",
    "            img_low_resolution = np.fliplr(img_low_resolution)\n",
    "        \n",
    "        images_high_resolution.append(img_high_resolution)\n",
    "        images_low_resolution.append(img_low_resolution)\n",
    "        \n",
    "    images_high_resolution = np.array(images_high_resolution)/127.5 - 1\n",
    "    images_low_resolution = np.array(images_low_resolution)/127.5 - 1\n",
    "    \n",
    "    return images_high_resolution , images_low_resolution\n",
    "\n",
    "\n",
    "def write_image(epoch):\n",
    "    #生成高分图像时 进行对比显示\n",
    "    high_resolution_image , low_resolution_image = load_image(batch_size=2 , training=False)\n",
    "    fake_high_resolution_image = generator_i.predict(low_resolution_image) #使用G来生成高分图像 使用低分图像生成原始的高分图像 但是难免有偏差 细节表现\n",
    "    \n",
    "    low_resolution_image = low_resolution_image*0.5+0.5\n",
    "    high_resolution_image = high_resolution_image*0.5+0.5\n",
    "    fake_high_resolution_image = fake_high_resolution_image*0.5+0.5\n",
    "    \n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    count=0\n",
    "    \n",
    "    axes[0][0].imshow(high_resolution_image[0])\n",
    "    axes[0][0].set_title('original high')\n",
    "    axes[0][0].axis('off')\n",
    "\n",
    "    axes[0][1].imshow(fake_high_resolution_image[0])\n",
    "    axes[0][1].set_title('generated high')\n",
    "    axes[0][1].axis('off')\n",
    "    \n",
    "    axes[0][2].imshow(low_resolution_image[0])\n",
    "    axes[0][2].set_title('original low')\n",
    "    axes[0][2].axis('off')\n",
    "\n",
    "    axes[1][0].imshow(high_resolution_image[1])\n",
    "    axes[1][0].set_title('original high')\n",
    "    axes[1][0].axis('off')\n",
    "\n",
    "    axes[1][1].imshow(fake_high_resolution_image[1])\n",
    "    axes[1][1].set_title('generated high')\n",
    "    axes[1][1].axis('off')\n",
    "    \n",
    "    axes[1][2].imshow(low_resolution_image[1])\n",
    "    axes[1][2].set_title('original low')\n",
    "    axes[1][2].axis('off')\n",
    "\n",
    "            \n",
    "    fig.savefig('celeba_began/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_conv2d(x , output_size , strides , n_layer):\n",
    "    for _ in range(n_layer):\n",
    "        x = Conv2D(output_size , kernel_size=(3,3) , strides=(1,1) , activation='elu' , padding='same')(x) #1步卷积 保证图像尺寸不变\n",
    "    \n",
    "    return Conv2D(output_size , kernel_size=(3,3) , strides=strides , activation='elu' , padding='same')(x) #最后的这个卷积的stride为2 进行降2倍采样\n",
    "\n",
    "\n",
    "def multi_deconv2d(x , output_size , n_layer , upsample):\n",
    "    for _ in range(n_layer):\n",
    "        x = Conv2D(output_size , kernel_size=(3,3) , strides=(1,1) , activation='elu' , padding='same')(x)\n",
    "    \n",
    "    if upsample:\n",
    "        x = UpSampling2D()(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    image = Input(shape=SHAPE)\n",
    "    \n",
    "    h = multi_conv2d(image , output_size=N_FILTERS , strides=(2,2) , n_layer=2)\n",
    "    h = multi_conv2d(h , output_size=N_FILTERS*2 , strides=(2,2) , n_layer=2)\n",
    "    h = multi_conv2d(h , output_size=N_FILTERS*3 , strides=(2,2) , n_layer=2)\n",
    "    h = multi_conv2d(h , output_size=N_FILTERS*4 , strides=(1,1) , n_layer=2)\n",
    "    \n",
    "    h = Flatten()(h)\n",
    "    \n",
    "    feature = Dense(units=64 , activation='linear')(h)\n",
    "    \n",
    "    return Model(image , feature)\n",
    "    \n",
    "def decoder():\n",
    "    feature = Input(shape=(64 , ))\n",
    "    \n",
    "    h = Dense(units=8*8*N_FILTERS , activation='linear')(feature)\n",
    "    h = Reshape(target_shape=(8,8,N_FILTERS))(h)\n",
    "    \n",
    "    h = multi_deconv2d(h , output_size=N_FILTERS , n_layer=2 , upsample=True)\n",
    "    h = multi_deconv2d(h , output_size=N_FILTERS , n_layer=2 , upsample=True)\n",
    "    h = multi_deconv2d(h , output_size=N_FILTERS , n_layer=2 , upsample=True)\n",
    "    h = multi_deconv2d(h , output_size=N_FILTERS , n_layer=2 , upsample=False)\n",
    "    \n",
    "    image = Conv2D(3 , kernel_size=(3,3) , strides=(1,1) , activation='linear' , padding='same')(h)\n",
    "    \n",
    "    return Model(feature , image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoencoder():\n",
    "    image = Input(shape=(SHAPE))\n",
    "    \n",
    "    Encoder = encoder()\n",
    "    Decoder = decoder()\n",
    "    \n",
    "    feature = Encoder(image)\n",
    "    image_hat = Decoder(feature)\n",
    "    \n",
    "    return Model(image , image_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    input_data = Input(shape=(HEIGHT , WIDTH , CHANNEL*2)) #因为discriminator的输入是两幅图像 所以输入的通道翻一倍\n",
    "    \n",
    "    real_image = Lambda(lambda x: x[:,:,:,:3] , output_shape=SHAPE)(input_data)\n",
    "    generator_image = Lambda(lambda x: x[:,:,:,3:] , output_shape=SHAPE)(input_data)\n",
    "    \n",
    "    Autoencoder = autoencoder()\n",
    "    \n",
    "    real_image_hat = Autoencoder(real_image)\n",
    "    generator_image_hat = Autoencoder(generator_image)\n",
    "\n",
    "    output_data = Concatenate()([real_image_hat , generator_image_hat])\n",
    "\n",
    "    return Model(input_data , output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    return decoder() #feature to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class D_loss(object):\n",
    "    def __init__(self , init_k_var=0.0 , init_lambda_k = 0.001 , init_gamma=0.5):\n",
    "        self.lambda_k = init_lambda_k\n",
    "        self.gamma = init_gamma\n",
    "        self.k_var = K.variable(init_k_var , dtype=K.floatx())\n",
    "        \n",
    "        self.m_global_var = K.variable(0.0 , dtype=K.floatx())\n",
    "        self.loss_real_x_var = K.variable(0)\n",
    "        self.loss_gene_x_var = K.variable(0)\n",
    "        \n",
    "        self.updates = []\n",
    "\n",
    "    def D_loss(y_true , y_pred):\n",
    "        real_image_hat_true , generator_image_hat_true = y_true[:,:,:,:3] , y_true[:,:,:,3:]\n",
    "        real_image_hat_pred ,generator_image_hat_pred = y_pred[:,:,:,:3] , y_true[:,:,:,3:]\n",
    "\n",
    "        #下面的平均在batch维上还没有平均 所以得到的是1维张量\n",
    "        loss_real_image_hat = K.mean(K.abs(real_image_hat_true-real_image_hat_pred) , axis=[1,2,3])\n",
    "        loss_generator_image_hat = K.mean(K.abs(generator_image_hat_true-generator_image_hat_pred) , axis=[1,2,3])\n",
    "\n",
    "        #paper equation\n",
    "        discriminator_loss = loss_real_image_hat - self.k_var*loss_generator_image_hat\n",
    "\n",
    "        mean_loss_real_image_hat = K.mean(loss_real_image_hat)\n",
    "        mean_loss_generator_image_hat = K.mean(loss_generator_image_hat)\n",
    "        #paper equation\n",
    "        new_k = self.k_var + self.lambda_k*(self.gamma*mean_loss_real_image_hat-mean_loss_generator_image_hat)\n",
    "        new_k = K.clip(new_k , 0 , 1)\n",
    "        self.updates.append(K.update(self.k_var , new_k))\n",
    "        \n",
    "        m_global = mean_loss_real_image_hat + K.abs(self.gamma*mean_loss_real_image_hat-mean_loss_generator_image_hat)\n",
    "        self.updates.append(K.update(self.m_global_var , m_global))\n",
    "        \n",
    "        self.updates.append(K.update(self.loss_real_x_var , mean_loss_real_image_hat))\n",
    "        self.updates.append(K.update(self.loss_gene_x_var , mean_loss_generator_image_hat))\n",
    "        \n",
    "        return discriminator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch = int(HIGH_HEIGHT/(2**4)) #16\n",
    "disc_patch = (patch , patch , 1) #16*16*1\n",
    "\n",
    "G_filters = 64\n",
    "D_filters = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator_i = discriminator(D_filters)\n",
    "discriminator_i.compile(optimizer=adam , loss='mse' , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator(G_filters)\n",
    "\n",
    "#image_high_resolution = Input(shape=HIGH_RESOLUTION_SHAPE) #不需要参与combined_model的整体构建 但是在训练的时候 是需要的\n",
    "#在训练的时候\n",
    "#来自真实样本的低分图像和高分图像\n",
    "#高分样本经过VGG后的 低维特征和real_labels 作为训练generator时的labels 训练数据是低分图像\n",
    "#具体过程为 将低分图像使用generator变为高分图像 然后经过VGG得到低维特征与训练样本中的低维样本(上面一句话中的低维特征)进行mse validity进行binary_crossentropy\n",
    "image_low_resolution = Input(shape=LOW_RESOLUTION_SHAPE)\n",
    "\n",
    "fake_image_high_resolution = generator_i(image_low_resolution) #低分 图像经过G后 生成高分图像\n",
    "fake_image_high_resolution_feature = vgg(fake_image_high_resolution) #生成的高分图像经过VGG16得到的特征值\n",
    "\n",
    "discriminator_i.trainable = False\n",
    "validity = discriminator_i(fake_image_high_resolution) #判别器对生辰高分图像的validity值\n",
    "\n",
    "combined_model_i = Model(image_low_resolution , [validity , fake_image_high_resolution_feature])\n",
    "\n",
    "combined_model_i.compile(optimizer=adam , loss=['binary_crossentropy' , 'mse'] , loss_weights=[1e-3 , 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\misc\\pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\misc\\pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n",
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:0.289170 accu:0.364258 gene_loss[x_entropy]:35.002327 gene_loss[mse]:0.726299\n",
      "epoch:1 loss:0.240119 accu:0.560547 gene_loss[x_entropy]:37.199436 gene_loss[mse]:0.993910\n",
      "epoch:2 loss:0.193222 accu:0.729980 gene_loss[x_entropy]:28.870689 gene_loss[mse]:1.238815\n",
      "epoch:3 loss:0.188500 accu:0.707031 gene_loss[x_entropy]:21.937769 gene_loss[mse]:1.285119\n",
      "epoch:4 loss:0.238619 accu:0.590820 gene_loss[x_entropy]:19.904377 gene_loss[mse]:1.846873\n",
      "epoch:5 loss:0.178662 accu:0.746582 gene_loss[x_entropy]:16.685862 gene_loss[mse]:2.045166\n",
      "epoch:6 loss:0.182008 accu:0.762695 gene_loss[x_entropy]:19.846888 gene_loss[mse]:1.548349\n",
      "epoch:7 loss:0.082215 accu:0.955566 gene_loss[x_entropy]:15.399577 gene_loss[mse]:1.685131\n",
      "epoch:8 loss:0.072267 accu:0.963379 gene_loss[x_entropy]:15.132298 gene_loss[mse]:2.216459\n",
      "epoch:9 loss:0.044728 accu:0.997559 gene_loss[x_entropy]:16.553905 gene_loss[mse]:2.506667\n",
      "epoch:10 loss:0.034918 accu:0.996094 gene_loss[x_entropy]:12.300821 gene_loss[mse]:2.559145\n",
      "epoch:11 loss:0.010695 accu:1.000000 gene_loss[x_entropy]:14.384052 gene_loss[mse]:2.746194\n",
      "epoch:12 loss:0.007794 accu:1.000000 gene_loss[x_entropy]:17.770962 gene_loss[mse]:2.834547\n",
      "epoch:13 loss:0.011743 accu:1.000000 gene_loss[x_entropy]:15.218203 gene_loss[mse]:2.821018\n",
      "epoch:14 loss:0.014468 accu:1.000000 gene_loss[x_entropy]:12.792607 gene_loss[mse]:2.840961\n",
      "epoch:15 loss:0.005747 accu:1.000000 gene_loss[x_entropy]:14.908691 gene_loss[mse]:2.834152\n",
      "epoch:16 loss:0.011099 accu:1.000000 gene_loss[x_entropy]:12.229841 gene_loss[mse]:2.922507\n",
      "epoch:17 loss:0.005023 accu:1.000000 gene_loss[x_entropy]:13.702356 gene_loss[mse]:3.016535\n",
      "epoch:18 loss:0.004679 accu:1.000000 gene_loss[x_entropy]:13.242436 gene_loss[mse]:2.996542\n",
      "epoch:19 loss:0.003443 accu:1.000000 gene_loss[x_entropy]:13.039419 gene_loss[mse]:2.991160\n",
      "epoch:20 loss:0.002869 accu:1.000000 gene_loss[x_entropy]:11.712291 gene_loss[mse]:3.231628\n",
      "epoch:21 loss:0.002639 accu:1.000000 gene_loss[x_entropy]:14.697947 gene_loss[mse]:3.181434\n",
      "epoch:22 loss:0.004416 accu:1.000000 gene_loss[x_entropy]:11.091394 gene_loss[mse]:3.206154\n",
      "epoch:23 loss:0.002877 accu:1.000000 gene_loss[x_entropy]:12.936701 gene_loss[mse]:3.240411\n",
      "epoch:24 loss:0.002056 accu:1.000000 gene_loss[x_entropy]:9.927262 gene_loss[mse]:3.385791\n",
      "epoch:25 loss:0.002788 accu:1.000000 gene_loss[x_entropy]:11.118559 gene_loss[mse]:3.364545\n",
      "epoch:26 loss:0.001601 accu:1.000000 gene_loss[x_entropy]:15.556676 gene_loss[mse]:3.375094\n",
      "epoch:27 loss:0.001919 accu:1.000000 gene_loss[x_entropy]:13.061799 gene_loss[mse]:3.340191\n",
      "epoch:28 loss:0.001438 accu:1.000000 gene_loss[x_entropy]:14.171734 gene_loss[mse]:3.450312\n",
      "epoch:29 loss:0.001706 accu:1.000000 gene_loss[x_entropy]:11.380783 gene_loss[mse]:3.491345\n",
      "epoch:30 loss:0.001397 accu:1.000000 gene_loss[x_entropy]:11.420233 gene_loss[mse]:3.599173\n",
      "epoch:31 loss:0.001449 accu:1.000000 gene_loss[x_entropy]:10.782272 gene_loss[mse]:3.673297\n",
      "epoch:32 loss:0.001198 accu:1.000000 gene_loss[x_entropy]:14.904538 gene_loss[mse]:3.457456\n",
      "epoch:33 loss:0.001232 accu:1.000000 gene_loss[x_entropy]:11.340000 gene_loss[mse]:3.521873\n",
      "epoch:34 loss:0.000973 accu:1.000000 gene_loss[x_entropy]:11.274271 gene_loss[mse]:3.541243\n",
      "epoch:35 loss:0.001241 accu:1.000000 gene_loss[x_entropy]:17.121899 gene_loss[mse]:3.665363\n",
      "epoch:36 loss:0.001142 accu:1.000000 gene_loss[x_entropy]:13.188661 gene_loss[mse]:3.575759\n",
      "epoch:37 loss:0.000696 accu:1.000000 gene_loss[x_entropy]:10.252524 gene_loss[mse]:3.665430\n",
      "epoch:38 loss:0.000730 accu:1.000000 gene_loss[x_entropy]:14.253963 gene_loss[mse]:3.726540\n",
      "epoch:39 loss:0.000849 accu:1.000000 gene_loss[x_entropy]:11.796575 gene_loss[mse]:3.525303\n",
      "epoch:40 loss:0.001601 accu:1.000000 gene_loss[x_entropy]:12.909141 gene_loss[mse]:3.720265\n",
      "epoch:41 loss:0.000658 accu:1.000000 gene_loss[x_entropy]:11.430274 gene_loss[mse]:3.552065\n",
      "epoch:42 loss:0.000975 accu:1.000000 gene_loss[x_entropy]:12.149856 gene_loss[mse]:3.825294\n",
      "epoch:43 loss:0.000564 accu:1.000000 gene_loss[x_entropy]:13.946235 gene_loss[mse]:3.796694\n",
      "epoch:44 loss:0.001200 accu:1.000000 gene_loss[x_entropy]:10.583405 gene_loss[mse]:3.811347\n",
      "epoch:45 loss:0.000783 accu:1.000000 gene_loss[x_entropy]:11.168609 gene_loss[mse]:3.792313\n",
      "epoch:46 loss:0.000698 accu:1.000000 gene_loss[x_entropy]:10.299287 gene_loss[mse]:3.863245\n",
      "epoch:47 loss:0.000849 accu:1.000000 gene_loss[x_entropy]:11.007903 gene_loss[mse]:3.827122\n",
      "epoch:48 loss:0.001032 accu:1.000000 gene_loss[x_entropy]:9.637421 gene_loss[mse]:3.715722\n",
      "epoch:49 loss:0.000708 accu:1.000000 gene_loss[x_entropy]:10.614463 gene_loss[mse]:3.771852\n",
      "epoch:50 loss:0.000631 accu:1.000000 gene_loss[x_entropy]:10.741340 gene_loss[mse]:3.803871\n",
      "epoch:51 loss:0.000846 accu:1.000000 gene_loss[x_entropy]:9.891498 gene_loss[mse]:3.981736\n",
      "epoch:52 loss:0.000668 accu:1.000000 gene_loss[x_entropy]:13.121091 gene_loss[mse]:4.014389\n",
      "epoch:53 loss:0.000595 accu:1.000000 gene_loss[x_entropy]:10.706151 gene_loss[mse]:3.930475\n",
      "epoch:54 loss:0.000633 accu:1.000000 gene_loss[x_entropy]:12.300611 gene_loss[mse]:3.774392\n",
      "epoch:55 loss:0.000479 accu:1.000000 gene_loss[x_entropy]:11.868226 gene_loss[mse]:3.816279\n",
      "epoch:56 loss:0.000520 accu:1.000000 gene_loss[x_entropy]:8.882808 gene_loss[mse]:4.014106\n",
      "epoch:57 loss:0.000312 accu:1.000000 gene_loss[x_entropy]:11.553020 gene_loss[mse]:4.014466\n",
      "epoch:58 loss:0.002172 accu:1.000000 gene_loss[x_entropy]:11.589459 gene_loss[mse]:3.933499\n",
      "epoch:59 loss:0.000448 accu:1.000000 gene_loss[x_entropy]:13.467387 gene_loss[mse]:3.809486\n",
      "epoch:60 loss:0.000572 accu:1.000000 gene_loss[x_entropy]:11.741427 gene_loss[mse]:3.903003\n",
      "epoch:61 loss:0.000397 accu:1.000000 gene_loss[x_entropy]:9.420569 gene_loss[mse]:4.121049\n",
      "epoch:62 loss:0.000369 accu:1.000000 gene_loss[x_entropy]:8.566490 gene_loss[mse]:4.068172\n",
      "epoch:63 loss:0.000473 accu:1.000000 gene_loss[x_entropy]:9.383451 gene_loss[mse]:4.010601\n",
      "epoch:64 loss:0.000302 accu:1.000000 gene_loss[x_entropy]:8.610619 gene_loss[mse]:4.145651\n",
      "epoch:65 loss:0.000319 accu:1.000000 gene_loss[x_entropy]:13.382843 gene_loss[mse]:4.059084\n",
      "epoch:66 loss:0.000239 accu:1.000000 gene_loss[x_entropy]:10.573621 gene_loss[mse]:3.974066\n",
      "epoch:67 loss:0.000430 accu:1.000000 gene_loss[x_entropy]:9.080382 gene_loss[mse]:4.070538\n",
      "epoch:68 loss:0.000284 accu:1.000000 gene_loss[x_entropy]:8.057586 gene_loss[mse]:4.199189\n",
      "epoch:69 loss:0.000274 accu:1.000000 gene_loss[x_entropy]:8.947175 gene_loss[mse]:4.268271\n",
      "epoch:70 loss:0.000287 accu:1.000000 gene_loss[x_entropy]:9.478367 gene_loss[mse]:4.148812\n",
      "epoch:71 loss:0.000335 accu:1.000000 gene_loss[x_entropy]:10.389582 gene_loss[mse]:4.195083\n",
      "epoch:72 loss:0.000249 accu:1.000000 gene_loss[x_entropy]:11.253580 gene_loss[mse]:3.956529\n",
      "epoch:73 loss:0.000438 accu:1.000000 gene_loss[x_entropy]:11.817612 gene_loss[mse]:4.117732\n",
      "epoch:74 loss:0.000476 accu:1.000000 gene_loss[x_entropy]:11.893345 gene_loss[mse]:4.079391\n",
      "epoch:75 loss:0.000308 accu:1.000000 gene_loss[x_entropy]:9.810868 gene_loss[mse]:4.157089\n",
      "epoch:76 loss:0.000360 accu:1.000000 gene_loss[x_entropy]:9.634523 gene_loss[mse]:4.128768\n",
      "epoch:77 loss:0.000370 accu:1.000000 gene_loss[x_entropy]:10.122890 gene_loss[mse]:4.154045\n",
      "epoch:78 loss:0.000242 accu:1.000000 gene_loss[x_entropy]:8.636357 gene_loss[mse]:4.187480\n",
      "epoch:79 loss:0.000433 accu:1.000000 gene_loss[x_entropy]:10.229603 gene_loss[mse]:4.233442\n",
      "epoch:80 loss:0.000559 accu:1.000000 gene_loss[x_entropy]:8.311719 gene_loss[mse]:4.207431\n",
      "epoch:81 loss:0.000248 accu:1.000000 gene_loss[x_entropy]:9.087221 gene_loss[mse]:4.230489\n",
      "epoch:82 loss:0.000292 accu:1.000000 gene_loss[x_entropy]:10.594801 gene_loss[mse]:4.295256\n",
      "epoch:83 loss:0.000465 accu:1.000000 gene_loss[x_entropy]:10.246889 gene_loss[mse]:4.273818\n",
      "epoch:84 loss:0.000761 accu:1.000000 gene_loss[x_entropy]:10.108431 gene_loss[mse]:4.332898\n",
      "epoch:85 loss:0.000460 accu:1.000000 gene_loss[x_entropy]:10.358922 gene_loss[mse]:4.393818\n",
      "epoch:86 loss:0.000425 accu:1.000000 gene_loss[x_entropy]:9.935510 gene_loss[mse]:4.360838\n",
      "epoch:87 loss:0.000443 accu:1.000000 gene_loss[x_entropy]:7.710073 gene_loss[mse]:4.378292\n",
      "epoch:88 loss:0.000170 accu:1.000000 gene_loss[x_entropy]:9.810650 gene_loss[mse]:4.267108\n",
      "epoch:89 loss:0.000305 accu:1.000000 gene_loss[x_entropy]:9.212428 gene_loss[mse]:4.162896\n",
      "epoch:90 loss:0.000130 accu:1.000000 gene_loss[x_entropy]:9.448153 gene_loss[mse]:4.312959\n",
      "epoch:91 loss:0.000223 accu:1.000000 gene_loss[x_entropy]:9.804703 gene_loss[mse]:4.449502\n",
      "epoch:92 loss:0.000217 accu:1.000000 gene_loss[x_entropy]:6.716733 gene_loss[mse]:4.583884\n",
      "epoch:93 loss:0.000327 accu:1.000000 gene_loss[x_entropy]:8.234782 gene_loss[mse]:4.382513\n",
      "epoch:94 loss:0.000195 accu:1.000000 gene_loss[x_entropy]:10.530398 gene_loss[mse]:4.486005\n",
      "epoch:95 loss:0.000257 accu:1.000000 gene_loss[x_entropy]:11.005095 gene_loss[mse]:4.198820\n",
      "epoch:96 loss:0.000243 accu:1.000000 gene_loss[x_entropy]:10.559433 gene_loss[mse]:4.516956\n",
      "epoch:97 loss:0.000147 accu:1.000000 gene_loss[x_entropy]:11.293322 gene_loss[mse]:4.495035\n",
      "epoch:98 loss:0.000199 accu:1.000000 gene_loss[x_entropy]:10.266997 gene_loss[mse]:4.448599\n",
      "epoch:99 loss:0.000180 accu:1.000000 gene_loss[x_entropy]:11.849442 gene_loss[mse]:4.331164\n",
      "epoch:100 loss:0.000304 accu:1.000000 gene_loss[x_entropy]:8.402928 gene_loss[mse]:4.256933\n",
      "epoch:101 loss:0.000182 accu:1.000000 gene_loss[x_entropy]:7.347625 gene_loss[mse]:4.607534\n",
      "epoch:102 loss:0.000412 accu:1.000000 gene_loss[x_entropy]:7.289384 gene_loss[mse]:4.581587\n",
      "epoch:103 loss:0.000248 accu:1.000000 gene_loss[x_entropy]:10.277214 gene_loss[mse]:4.497077\n",
      "epoch:104 loss:0.000126 accu:1.000000 gene_loss[x_entropy]:8.646091 gene_loss[mse]:4.472847\n",
      "epoch:105 loss:0.000123 accu:1.000000 gene_loss[x_entropy]:10.111553 gene_loss[mse]:4.385536\n",
      "epoch:106 loss:0.000240 accu:1.000000 gene_loss[x_entropy]:8.918358 gene_loss[mse]:4.598171\n",
      "epoch:107 loss:0.000118 accu:1.000000 gene_loss[x_entropy]:7.067586 gene_loss[mse]:4.490736\n",
      "epoch:108 loss:0.000154 accu:1.000000 gene_loss[x_entropy]:9.349298 gene_loss[mse]:4.517538\n",
      "epoch:109 loss:0.000308 accu:1.000000 gene_loss[x_entropy]:7.406329 gene_loss[mse]:4.463702\n",
      "epoch:110 loss:0.000177 accu:1.000000 gene_loss[x_entropy]:7.910836 gene_loss[mse]:4.641898\n",
      "epoch:111 loss:0.000392 accu:1.000000 gene_loss[x_entropy]:8.376008 gene_loss[mse]:4.567235\n",
      "epoch:112 loss:0.000181 accu:1.000000 gene_loss[x_entropy]:9.429541 gene_loss[mse]:4.548109\n",
      "epoch:113 loss:0.000185 accu:1.000000 gene_loss[x_entropy]:6.898166 gene_loss[mse]:4.599503\n",
      "epoch:114 loss:0.000162 accu:1.000000 gene_loss[x_entropy]:8.452863 gene_loss[mse]:4.563319\n",
      "epoch:115 loss:0.000103 accu:1.000000 gene_loss[x_entropy]:9.459397 gene_loss[mse]:4.681113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e9e0fa26101a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfake_high_resolution_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow_resolution_image\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#使用G生成真低分样本的高分样本\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#训练判别器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mreal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhigh_resolution_image\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mreal_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#使用真实的高分图像 训练 label全1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mfake_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_high_resolution_image\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#使用G生成的假的高分图像 训练 label全0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tuple类型相加 相当于cat连接\n",
    "real_labels = np.ones(shape=(BATCH_SIZE , )+disc_patch) #真实样本label为1\n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , )+disc_patch) #假样本label为0\n",
    "\n",
    "for i in range(1001):\n",
    "    \n",
    "    high_resolution_image , low_resolution_image = load_image() #真实的高分图像和低分图像都是来自真实样本\n",
    "    \n",
    "    fake_high_resolution_image = generator_i.predict(low_resolution_image) #使用G生成真低分样本的高分样本\n",
    "    #训练判别器\n",
    "    real_loss = discriminator_i.train_on_batch(high_resolution_image , real_labels) #使用真实的高分图像 训练 label全1\n",
    "    fake_loss = discriminator_i.train_on_batch(fake_high_resolution_image , fake_labels) #使用G生成的假的高分图像 训练 label全0 \n",
    "\n",
    "    loss = np.add(real_loss , fake_loss)/2\n",
    "\n",
    "    #训练生成器\n",
    "    high_resolution_image , low_resolution_image = load_image() #真实的高分图像和低分图像都是来自真实样本\n",
    "    \n",
    "    feature_high_resolution_image = vgg.predict(high_resolution_image)\n",
    "    \n",
    "    generator_loss = combined_model_i.train_on_batch(low_resolution_image , [real_labels , feature_high_resolution_image])\n",
    "\n",
    "    print('epoch:%d loss:%f accu:%f gene_loss[x_entropy]:%f gene_loss[mse]:%f' % (i , loss[0] , loss[1] , generator_loss[0] , generator_loss[1]))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        write_image(i+1000)\n",
    "    #write_image_mnist(i)\n",
    "    \n",
    "write_image(999)\n",
    "#write_image_mnist(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of cbe5617147190e668d6c5d5026f83318 so we will re-download the data.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5\n",
      "574717952/574710816 [==============================] - 729s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x1ddbc5e7a58>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG19(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
