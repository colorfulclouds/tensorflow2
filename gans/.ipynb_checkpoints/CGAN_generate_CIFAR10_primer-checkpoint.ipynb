{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers import Multiply\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.initializers import truncated_normal , random_normal , constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CIFAR10 dataset\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNEL = 3\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = '../dataset/train/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n",
    "\n",
    "#=========\n",
    "#=========\n",
    "#add new FLAG(s)\n",
    "CLASS_NUM = 10 #mnist=10 CIFAR10=10 CIFAR100=100 CIFAR1000=1000\n",
    "\n",
    "LABEL2INDEX = {'frog':0 , 'truck':1 , 'deer':2 , 'automobile':3 , 'bird':4 , 'horse':5 , 'ship':6 , 'cat':7 , 'dog':8 , 'airplane':9}\n",
    "INDEX2LABEL = {value:key for key , value in LABEL2INDEX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'frog',\n",
       " 1: 'truck',\n",
       " 2: 'deer',\n",
       " 3: 'automobile',\n",
       " 4: 'bird',\n",
       " 5: 'horse',\n",
       " 6: 'ship',\n",
       " 7: 'cat',\n",
       " 8: 'dog',\n",
       " 9: 'airplane'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX2LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('../dataset/trainLabels.csv')\n",
    "#每张图片对应的类别标号\n",
    "train_labels = train_labels['label'].map(LABEL2INDEX).get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "        labels.append(train_labels[(load_index + i) % IMAGES_COUNT])\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1 , np.array(labels)\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    labels = np.random.randint(low=0 , high=CLASS_NUM , size=(ROW*COL , 1))\n",
    "    \n",
    "    generated_image = generator_i.predict([noise , labels])\n",
    "    \n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    fig.subplots_adjust(hspace=0.9 , wspace=0.9)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            axes[i][j].set_title(INDEX2LABEL[labels[i*ROW+j][0]])\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('CIFAR10_cgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(output_size):\n",
    "    return Conv2D(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def dense(output_size):\n",
    "    return Dense(output_size , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def deconv2d(output_size):\n",
    "    return Conv2DTranspose(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def batch_norm():\n",
    "    return BatchNormalization(momentum=0.9 , epsilon=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "    \n",
    "    #CIFAR10 图像使用 32*32*3\n",
    "    model.add(Dense(2 * 2 * 64*8, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "    model.add(Reshape((2, 2, 64*8)))\n",
    "    \n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #4\n",
    "    model.add(deconv2d(64*4))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #8\n",
    "    model.add(deconv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #16\n",
    "    model.add(deconv2d(64*1))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #32\n",
    "    model.add(deconv2d(3))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    label = Input(shape=(1,) , dtype='int32')\n",
    "    \n",
    "    _ = Embedding(input_dim=CLASS_NUM , output_dim=LATENT_DIM)(label)\n",
    "    embedding_label = Flatten()(_)\n",
    "    \n",
    "    noise_embedding_label = Multiply()([noise , embedding_label]) #(None , LATENT_DIM)\n",
    "    \n",
    "    image = model(noise_embedding_label)\n",
    "    \n",
    "    return Model([noise , label] , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='discriminator')\n",
    "    \n",
    "    model.add(Conv2D(filters=64 , kernel_size=(5,5) , strides=(2,2) , padding='same' , input_shape=(WIDTH , HEIGHT , CHANNEL) , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0) , name='conv1'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(conv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(conv2d(64*4))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(conv2d(64*8))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #=====\n",
    "    #addin\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #=====\n",
    "    model.add(Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    flatten_feature = Flatten()(image)\n",
    "    \n",
    "    label = Input(shape=(1,))\n",
    "    embedding_label = Embedding(input_dim=CLASS_NUM , output_dim=WIDTH*HEIGHT*CHANNEL)(label)\n",
    "    flatten_embedding_label = Flatten()(embedding_label)\n",
    "    \n",
    "\n",
    "    input_ = Multiply()([flatten_feature , flatten_embedding_label])\n",
    "    input_reshape = Reshape(target_shape=(WIDTH , HEIGHT , CHANNEL))(input_)\n",
    "    \n",
    "    #FC层 多加了一层\n",
    "    #_ = Dense(128)(flatten_feature)\n",
    "    #_ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    validity = model(input_reshape)\n",
    "        \n",
    "    return Model([image , label] , validity , name='discriminator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_model(generator_i , discriminator_i):\n",
    "    #生成器和判别器组合成整体\n",
    "    z = Input(shape=(LATENT_DIM , ) , name='z')\n",
    "    label = Input(shape=(1,) , dtype='int32')\n",
    "    \n",
    "    image = generator_i([z , label])\n",
    "    \n",
    "    #print(image.shape)\n",
    "    \n",
    "    discriminator_i.trainable = False\n",
    "    validity = discriminator_i([image , label])\n",
    "    \n",
    "    return Model([z , label] , validity , name='combined_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 2, 2, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,572,545\n",
      "Trainable params: 4,570,753\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i = discriminator()\n",
    "discriminator_i.compile(optimizer=adam , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator()\n",
    "\n",
    "combined_model_i = combined_model(generator_i , discriminator_i)\n",
    "\n",
    "\n",
    "combined_model_i.compile(optimizer=adam , loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch:52 loss:3.358416 accu:0.203125 gene_loss:[validity:0.066407]\n",
      "epoch:1 batch:52 loss:1.712920 accu:0.500000 gene_loss:[validity:1.204314]\n",
      "epoch:2 batch:52 loss:0.158450 accu:0.960938 gene_loss:[validity:3.792040]\n",
      "epoch:3 batch:52 loss:0.124455 accu:1.000000 gene_loss:[validity:3.621857]\n",
      "epoch:4 batch:52 loss:0.092235 accu:1.000000 gene_loss:[validity:3.650307]\n",
      "epoch:5 batch:52 loss:0.073095 accu:1.000000 gene_loss:[validity:4.086358]\n",
      "epoch:6 batch:52 loss:0.057453 accu:1.000000 gene_loss:[validity:4.536630]\n",
      "epoch:7 batch:52 loss:0.067552 accu:1.000000 gene_loss:[validity:4.186734]\n",
      "epoch:8 batch:52 loss:0.084976 accu:0.992188 gene_loss:[validity:5.099353]\n",
      "epoch:9 batch:52 loss:0.137509 accu:1.000000 gene_loss:[validity:5.509458]\n",
      "epoch:10 batch:52 loss:0.232126 accu:0.953125 gene_loss:[validity:5.590363]\n",
      "epoch:11 batch:52 loss:0.230709 accu:0.921875 gene_loss:[validity:4.790498]\n",
      "epoch:12 batch:52 loss:0.098422 accu:1.000000 gene_loss:[validity:5.259956]\n",
      "epoch:13 batch:52 loss:0.155099 accu:0.992188 gene_loss:[validity:6.163752]\n",
      "epoch:14 batch:52 loss:0.824704 accu:0.671875 gene_loss:[validity:6.247040]\n",
      "epoch:15 batch:52 loss:2.092676 accu:0.390625 gene_loss:[validity:3.673781]\n",
      "epoch:16 batch:52 loss:0.208837 accu:0.953125 gene_loss:[validity:3.203051]\n",
      "epoch:17 batch:52 loss:0.383181 accu:0.812500 gene_loss:[validity:3.738345]\n",
      "epoch:18 batch:52 loss:0.784834 accu:0.601562 gene_loss:[validity:4.526595]\n",
      "epoch:19 batch:52 loss:1.302601 accu:0.484375 gene_loss:[validity:3.300833]\n",
      "epoch:20 batch:52 loss:0.507480 accu:0.781250 gene_loss:[validity:3.378458]\n",
      "epoch:21 batch:52 loss:0.702470 accu:0.648438 gene_loss:[validity:3.374012]\n",
      "epoch:22 batch:52 loss:1.067281 accu:0.484375 gene_loss:[validity:2.811809]\n",
      "epoch:23 batch:52 loss:0.537037 accu:0.640625 gene_loss:[validity:3.840829]\n",
      "epoch:24 batch:52 loss:1.210753 accu:0.273438 gene_loss:[validity:4.104119]\n",
      "epoch:25 batch:52 loss:0.805198 accu:0.632812 gene_loss:[validity:1.998257]\n",
      "epoch:26 batch:52 loss:1.086890 accu:0.414062 gene_loss:[validity:3.704731]\n",
      "epoch:27 batch:52 loss:1.255277 accu:0.500000 gene_loss:[validity:3.003497]\n",
      "epoch:28 batch:52 loss:0.935998 accu:0.492188 gene_loss:[validity:2.209971]\n",
      "epoch:29 batch:52 loss:0.442454 accu:0.835938 gene_loss:[validity:2.606567]\n",
      "epoch:30 batch:52 loss:0.881190 accu:0.484375 gene_loss:[validity:3.354993]\n",
      "epoch:31 batch:52 loss:1.384148 accu:0.203125 gene_loss:[validity:2.127566]\n",
      "epoch:32 batch:52 loss:0.810250 accu:0.445312 gene_loss:[validity:2.775404]\n",
      "epoch:33 batch:52 loss:1.118347 accu:0.156250 gene_loss:[validity:2.432282]\n",
      "epoch:34 batch:52 loss:0.693273 accu:0.648438 gene_loss:[validity:2.530046]\n",
      "epoch:35 batch:52 loss:0.970450 accu:0.398438 gene_loss:[validity:2.042977]\n",
      "epoch:36 batch:52 loss:1.006569 accu:0.437500 gene_loss:[validity:1.962329]\n",
      "epoch:37 batch:52 loss:0.966403 accu:0.367188 gene_loss:[validity:2.223161]\n",
      "epoch:38 batch:52 loss:0.777981 accu:0.546875 gene_loss:[validity:2.543952]\n",
      "epoch:39 batch:52 loss:1.027737 accu:0.281250 gene_loss:[validity:1.712875]\n",
      "epoch:40 batch:52 loss:0.837590 accu:0.359375 gene_loss:[validity:2.537724]\n",
      "epoch:41 batch:52 loss:0.956387 accu:0.312500 gene_loss:[validity:2.129468]\n",
      "epoch:42 batch:52 loss:0.621292 accu:0.609375 gene_loss:[validity:1.952981]\n",
      "epoch:43 batch:52 loss:0.966464 accu:0.171875 gene_loss:[validity:2.163753]\n",
      "epoch:44 batch:52 loss:0.663146 accu:0.679688 gene_loss:[validity:1.967369]\n",
      "epoch:45 batch:52 loss:1.212948 accu:0.164062 gene_loss:[validity:2.072050]\n",
      "epoch:46 batch:52 loss:0.993971 accu:0.296875 gene_loss:[validity:1.969443]\n",
      "epoch:47 batch:52 loss:0.892381 accu:0.421875 gene_loss:[validity:1.650054]\n",
      "epoch:48 batch:52 loss:0.934403 accu:0.335938 gene_loss:[validity:2.003486]\n",
      "epoch:49 batch:52 loss:0.732283 accu:0.445312 gene_loss:[validity:2.377309]\n",
      "epoch:50 batch:52 loss:1.018157 accu:0.359375 gene_loss:[validity:1.237368]\n",
      "epoch:51 batch:52 loss:0.733017 accu:0.437500 gene_loss:[validity:2.315602]\n",
      "epoch:52 batch:52 loss:0.711525 accu:0.570312 gene_loss:[validity:2.405991]\n",
      "epoch:53 batch:52 loss:1.199821 accu:0.109375 gene_loss:[validity:1.918980]\n",
      "epoch:54 batch:52 loss:0.996856 accu:0.296875 gene_loss:[validity:2.021735]\n",
      "epoch:55 batch:52 loss:1.043449 accu:0.335938 gene_loss:[validity:1.580150]\n",
      "epoch:56 batch:52 loss:0.853392 accu:0.218750 gene_loss:[validity:1.896917]\n",
      "epoch:57 batch:52 loss:0.861678 accu:0.421875 gene_loss:[validity:1.913829]\n",
      "epoch:58 batch:52 loss:0.916115 accu:0.382812 gene_loss:[validity:1.175451]\n",
      "epoch:59 batch:52 loss:1.095247 accu:0.093750 gene_loss:[validity:1.599369]\n",
      "epoch:60 batch:52 loss:0.876361 accu:0.429688 gene_loss:[validity:1.764893]\n",
      "epoch:61 batch:52 loss:0.997851 accu:0.242188 gene_loss:[validity:2.113287]\n",
      "epoch:62 batch:52 loss:1.147486 accu:0.171875 gene_loss:[validity:1.301290]\n",
      "epoch:63 batch:52 loss:1.158080 accu:0.242188 gene_loss:[validity:1.365941]\n",
      "epoch:64 batch:52 loss:0.927232 accu:0.406250 gene_loss:[validity:1.362473]\n",
      "epoch:65 batch:52 loss:0.805483 accu:0.445312 gene_loss:[validity:1.654978]\n",
      "epoch:66 batch:52 loss:0.827331 accu:0.359375 gene_loss:[validity:1.201416]\n",
      "epoch:67 batch:52 loss:0.768644 accu:0.351562 gene_loss:[validity:1.432299]\n",
      "epoch:68 batch:52 loss:0.728179 accu:0.476562 gene_loss:[validity:1.237470]\n",
      "epoch:69 batch:52 loss:0.955557 accu:0.476562 gene_loss:[validity:1.312842]\n",
      "epoch:70 batch:52 loss:1.056026 accu:0.250000 gene_loss:[validity:1.430441]\n",
      "epoch:71 batch:52 loss:1.029959 accu:0.210938 gene_loss:[validity:1.028888]\n",
      "epoch:72 batch:52 loss:1.053212 accu:0.125000 gene_loss:[validity:1.191236]\n",
      "epoch:73 batch:52 loss:0.822850 accu:0.390625 gene_loss:[validity:1.304048]\n",
      "epoch:74 batch:52 loss:0.833208 accu:0.492188 gene_loss:[validity:0.909590]\n",
      "epoch:75 batch:52 loss:0.854901 accu:0.335938 gene_loss:[validity:1.294671]\n",
      "epoch:76 batch:52 loss:0.953844 accu:0.226562 gene_loss:[validity:1.217960]\n",
      "epoch:77 batch:52 loss:0.757926 accu:0.367188 gene_loss:[validity:1.004866]\n",
      "epoch:78 batch:52 loss:0.823238 accu:0.421875 gene_loss:[validity:1.059294]\n",
      "epoch:79 batch:52 loss:0.702858 accu:0.585938 gene_loss:[validity:1.723804]\n",
      "epoch:80 batch:52 loss:1.309701 accu:0.132812 gene_loss:[validity:1.169694]\n",
      "epoch:81 batch:52 loss:0.932211 accu:0.343750 gene_loss:[validity:1.182058]\n",
      "epoch:82 batch:52 loss:0.812633 accu:0.320312 gene_loss:[validity:1.141611]\n",
      "epoch:83 batch:52 loss:0.712155 accu:0.539062 gene_loss:[validity:1.283707]\n",
      "epoch:84 batch:52 loss:0.980053 accu:0.132812 gene_loss:[validity:1.127079]\n",
      "epoch:85 batch:52 loss:0.730684 accu:0.453125 gene_loss:[validity:1.319960]\n",
      "epoch:86 batch:52 loss:0.823815 accu:0.460938 gene_loss:[validity:1.336170]\n",
      "epoch:87 batch:52 loss:0.875596 accu:0.390625 gene_loss:[validity:1.126549]\n",
      "epoch:88 batch:52 loss:0.944898 accu:0.054688 gene_loss:[validity:1.055622]\n",
      "epoch:89 batch:52 loss:0.866767 accu:0.359375 gene_loss:[validity:1.183660]\n",
      "epoch:90 batch:52 loss:0.790574 accu:0.398438 gene_loss:[validity:0.821163]\n",
      "epoch:91 batch:52 loss:0.872002 accu:0.375000 gene_loss:[validity:0.985328]\n",
      "epoch:92 batch:52 loss:0.807690 accu:0.367188 gene_loss:[validity:0.993251]\n",
      "epoch:93 batch:52 loss:1.085109 accu:0.179688 gene_loss:[validity:1.055945]\n",
      "epoch:94 batch:52 loss:0.835930 accu:0.390625 gene_loss:[validity:1.110244]\n",
      "epoch:95 batch:52 loss:0.963962 accu:0.179688 gene_loss:[validity:1.509281]\n",
      "epoch:96 batch:52 loss:0.747885 accu:0.445312 gene_loss:[validity:0.969837]\n",
      "epoch:97 batch:52 loss:0.819531 accu:0.445312 gene_loss:[validity:0.956715]\n",
      "epoch:98 batch:52 loss:0.916428 accu:0.234375 gene_loss:[validity:1.185765]\n",
      "epoch:99 batch:52 loss:0.897265 accu:0.335938 gene_loss:[validity:1.167039]\n",
      "epoch:100 batch:52 loss:0.905872 accu:0.226562 gene_loss:[validity:1.214700]\n",
      "epoch:101 batch:52 loss:0.951764 accu:0.265625 gene_loss:[validity:0.975385]\n",
      "epoch:102 batch:52 loss:0.954505 accu:0.109375 gene_loss:[validity:1.009718]\n",
      "epoch:103 batch:52 loss:0.867376 accu:0.320312 gene_loss:[validity:1.068768]\n",
      "epoch:104 batch:52 loss:0.887595 accu:0.429688 gene_loss:[validity:1.141282]\n",
      "epoch:105 batch:52 loss:0.847833 accu:0.406250 gene_loss:[validity:1.267490]\n",
      "epoch:106 batch:52 loss:1.023134 accu:0.226562 gene_loss:[validity:1.095534]\n",
      "epoch:107 batch:52 loss:0.883101 accu:0.234375 gene_loss:[validity:0.981942]\n",
      "epoch:108 batch:52 loss:0.823722 accu:0.453125 gene_loss:[validity:1.069171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:109 batch:52 loss:0.877941 accu:0.257812 gene_loss:[validity:1.162117]\n",
      "epoch:110 batch:52 loss:0.822776 accu:0.210938 gene_loss:[validity:0.996902]\n",
      "epoch:111 batch:52 loss:0.855451 accu:0.414062 gene_loss:[validity:1.009773]\n",
      "epoch:112 batch:52 loss:0.843760 accu:0.203125 gene_loss:[validity:0.971719]\n",
      "epoch:113 batch:52 loss:0.771110 accu:0.453125 gene_loss:[validity:1.040673]\n",
      "epoch:114 batch:52 loss:0.834051 accu:0.234375 gene_loss:[validity:0.921108]\n",
      "epoch:115 batch:52 loss:0.821202 accu:0.265625 gene_loss:[validity:0.847026]\n",
      "epoch:116 batch:52 loss:0.805212 accu:0.273438 gene_loss:[validity:0.995336]\n",
      "epoch:117 batch:52 loss:0.865465 accu:0.375000 gene_loss:[validity:0.905387]\n",
      "epoch:118 batch:52 loss:0.820668 accu:0.398438 gene_loss:[validity:1.014947]\n",
      "epoch:119 batch:52 loss:0.913017 accu:0.195312 gene_loss:[validity:1.028235]\n",
      "epoch:120 batch:52 loss:0.879429 accu:0.312500 gene_loss:[validity:0.993874]\n",
      "epoch:121 batch:52 loss:0.855253 accu:0.289062 gene_loss:[validity:0.974659]\n",
      "epoch:122 batch:52 loss:0.779597 accu:0.421875 gene_loss:[validity:1.081636]\n",
      "epoch:123 batch:52 loss:0.893822 accu:0.156250 gene_loss:[validity:0.917193]\n",
      "epoch:124 batch:52 loss:0.832527 accu:0.328125 gene_loss:[validity:1.000208]\n",
      "epoch:125 batch:52 loss:0.786424 accu:0.156250 gene_loss:[validity:0.863261]\n",
      "epoch:126 batch:52 loss:0.896526 accu:0.257812 gene_loss:[validity:1.096032]\n",
      "epoch:127 batch:52 loss:0.753409 accu:0.500000 gene_loss:[validity:0.968961]\n",
      "epoch:128 batch:52 loss:0.779024 accu:0.476562 gene_loss:[validity:0.922591]\n",
      "epoch:129 batch:52 loss:0.771528 accu:0.351562 gene_loss:[validity:1.078225]\n",
      "epoch:130 batch:52 loss:0.778245 accu:0.406250 gene_loss:[validity:0.898106]\n",
      "epoch:131 batch:52 loss:0.940366 accu:0.257812 gene_loss:[validity:0.707769]\n",
      "epoch:132 batch:52 loss:0.684653 accu:0.570312 gene_loss:[validity:0.714855]\n",
      "epoch:133 batch:52 loss:0.823134 accu:0.296875 gene_loss:[validity:1.011498]\n",
      "epoch:134 batch:52 loss:0.873179 accu:0.257812 gene_loss:[validity:0.941868]\n",
      "epoch:135 batch:52 loss:0.975239 accu:0.132812 gene_loss:[validity:1.004105]\n",
      "epoch:136 batch:52 loss:0.842219 accu:0.398438 gene_loss:[validity:0.941496]\n",
      "epoch:137 batch:52 loss:0.766761 accu:0.375000 gene_loss:[validity:0.963744]\n",
      "epoch:138 batch:52 loss:0.937284 accu:0.250000 gene_loss:[validity:0.975874]\n",
      "epoch:139 batch:52 loss:0.800530 accu:0.265625 gene_loss:[validity:0.937975]\n",
      "epoch:140 batch:52 loss:0.782486 accu:0.289062 gene_loss:[validity:1.075501]\n",
      "epoch:141 batch:52 loss:0.765617 accu:0.437500 gene_loss:[validity:0.728459]\n",
      "epoch:142 batch:52 loss:0.793370 accu:0.367188 gene_loss:[validity:0.834490]\n",
      "epoch:143 batch:52 loss:0.908775 accu:0.273438 gene_loss:[validity:0.936006]\n",
      "epoch:144 batch:52 loss:0.778853 accu:0.375000 gene_loss:[validity:1.118630]\n",
      "epoch:145 batch:52 loss:0.884227 accu:0.218750 gene_loss:[validity:1.026795]\n",
      "epoch:146 batch:52 loss:0.863611 accu:0.226562 gene_loss:[validity:0.897114]\n",
      "epoch:147 batch:52 loss:0.821183 accu:0.492188 gene_loss:[validity:1.036175]\n",
      "epoch:148 batch:52 loss:0.821127 accu:0.289062 gene_loss:[validity:0.903260]\n",
      "epoch:149 batch:52 loss:0.838794 accu:0.195312 gene_loss:[validity:0.757653]\n",
      "epoch:150 batch:52 loss:0.910721 accu:0.101562 gene_loss:[validity:0.808472]\n",
      "epoch:151 batch:52 loss:0.799971 accu:0.312500 gene_loss:[validity:0.721701]\n",
      "epoch:152 batch:52 loss:0.834849 accu:0.117188 gene_loss:[validity:0.870337]\n",
      "epoch:153 batch:52 loss:0.855854 accu:0.031250 gene_loss:[validity:0.921814]\n",
      "epoch:154 batch:52 loss:0.749898 accu:0.320312 gene_loss:[validity:0.802046]\n",
      "epoch:155 batch:52 loss:0.787200 accu:0.367188 gene_loss:[validity:1.041968]\n",
      "epoch:156 batch:52 loss:0.799505 accu:0.398438 gene_loss:[validity:0.955604]\n",
      "epoch:157 batch:52 loss:0.906250 accu:0.281250 gene_loss:[validity:0.800707]\n",
      "epoch:158 batch:52 loss:0.783418 accu:0.453125 gene_loss:[validity:0.807991]\n",
      "epoch:159 batch:52 loss:0.850424 accu:0.195312 gene_loss:[validity:0.928054]\n",
      "epoch:160 batch:52 loss:0.751910 accu:0.390625 gene_loss:[validity:0.915811]\n",
      "epoch:161 batch:52 loss:0.892848 accu:0.187500 gene_loss:[validity:0.774063]\n",
      "epoch:162 batch:52 loss:0.779717 accu:0.179688 gene_loss:[validity:0.882007]\n",
      "epoch:163 batch:52 loss:0.744204 accu:0.335938 gene_loss:[validity:0.817656]\n",
      "epoch:164 batch:52 loss:0.809723 accu:0.273438 gene_loss:[validity:0.789805]\n",
      "epoch:165 batch:52 loss:0.702383 accu:0.585938 gene_loss:[validity:0.769153]\n",
      "epoch:166 batch:52 loss:0.772205 accu:0.289062 gene_loss:[validity:0.763950]\n",
      "epoch:167 batch:52 loss:0.603003 accu:0.765625 gene_loss:[validity:0.894368]\n",
      "epoch:168 batch:52 loss:0.908451 accu:0.078125 gene_loss:[validity:0.595458]\n",
      "epoch:169 batch:52 loss:0.695931 accu:0.539062 gene_loss:[validity:1.161567]\n",
      "epoch:170 batch:52 loss:0.846074 accu:0.257812 gene_loss:[validity:0.831197]\n",
      "epoch:171 batch:52 loss:0.812289 accu:0.210938 gene_loss:[validity:0.735133]\n",
      "epoch:172 batch:52 loss:0.959124 accu:0.148438 gene_loss:[validity:0.898975]\n",
      "epoch:173 batch:52 loss:0.740326 accu:0.531250 gene_loss:[validity:0.891103]\n",
      "epoch:174 batch:52 loss:0.800705 accu:0.406250 gene_loss:[validity:0.681736]\n",
      "epoch:175 batch:52 loss:0.719565 accu:0.445312 gene_loss:[validity:0.984892]\n",
      "epoch:176 batch:52 loss:0.833338 accu:0.164062 gene_loss:[validity:0.881655]\n",
      "epoch:177 batch:52 loss:0.582100 accu:0.867188 gene_loss:[validity:0.898280]\n",
      "epoch:178 batch:52 loss:0.920362 accu:0.015625 gene_loss:[validity:1.022972]\n",
      "epoch:179 batch:52 loss:0.944287 accu:0.078125 gene_loss:[validity:0.936606]\n",
      "epoch:180 batch:52 loss:0.850258 accu:0.367188 gene_loss:[validity:0.865368]\n",
      "epoch:181 batch:52 loss:0.748356 accu:0.460938 gene_loss:[validity:0.773725]\n",
      "epoch:182 batch:52 loss:0.825908 accu:0.296875 gene_loss:[validity:0.694685]\n",
      "epoch:183 batch:52 loss:0.835200 accu:0.335938 gene_loss:[validity:1.030803]\n",
      "epoch:184 batch:52 loss:0.723379 accu:0.578125 gene_loss:[validity:0.862479]\n",
      "epoch:185 batch:52 loss:0.956929 accu:0.398438 gene_loss:[validity:1.083249]\n",
      "epoch:186 batch:52 loss:0.807579 accu:0.429688 gene_loss:[validity:1.048961]\n",
      "epoch:187 batch:52 loss:0.844112 accu:0.242188 gene_loss:[validity:0.745460]\n",
      "epoch:188 batch:52 loss:0.893257 accu:0.265625 gene_loss:[validity:0.853959]\n",
      "epoch:189 batch:52 loss:0.739855 accu:0.398438 gene_loss:[validity:0.965509]\n",
      "epoch:190 batch:52 loss:0.941507 accu:0.085938 gene_loss:[validity:0.942965]\n",
      "epoch:191 batch:52 loss:0.820096 accu:0.320312 gene_loss:[validity:0.937126]\n",
      "epoch:192 batch:52 loss:0.771453 accu:0.539062 gene_loss:[validity:0.774265]\n",
      "epoch:193 batch:52 loss:0.879757 accu:0.265625 gene_loss:[validity:0.825450]\n",
      "epoch:194 batch:52 loss:0.844690 accu:0.226562 gene_loss:[validity:0.791818]\n",
      "epoch:195 batch:52 loss:0.797157 accu:0.367188 gene_loss:[validity:0.916437]\n",
      "epoch:196 batch:52 loss:0.833051 accu:0.148438 gene_loss:[validity:0.888599]\n",
      "epoch:197 batch:52 loss:0.757105 accu:0.398438 gene_loss:[validity:0.927374]\n",
      "epoch:198 batch:52 loss:0.812954 accu:0.273438 gene_loss:[validity:0.859010]\n",
      "epoch:199 batch:52 loss:0.768299 accu:0.367188 gene_loss:[validity:0.982655]\n",
      "epoch:200 batch:52 loss:0.779191 accu:0.359375 gene_loss:[validity:0.836657]\n",
      "epoch:201 batch:52 loss:0.779802 accu:0.429688 gene_loss:[validity:0.821633]\n",
      "epoch:202 batch:52 loss:0.814043 accu:0.242188 gene_loss:[validity:0.870647]\n",
      "epoch:203 batch:52 loss:0.740265 accu:0.335938 gene_loss:[validity:0.840315]\n",
      "epoch:204 batch:52 loss:0.846986 accu:0.203125 gene_loss:[validity:0.830815]\n",
      "epoch:205 batch:52 loss:0.798654 accu:0.312500 gene_loss:[validity:0.652996]\n",
      "epoch:206 batch:52 loss:0.782564 accu:0.296875 gene_loss:[validity:0.811969]\n",
      "epoch:207 batch:52 loss:0.883478 accu:0.351562 gene_loss:[validity:0.824970]\n",
      "epoch:208 batch:52 loss:0.788495 accu:0.390625 gene_loss:[validity:0.900864]\n",
      "epoch:209 batch:52 loss:0.784596 accu:0.265625 gene_loss:[validity:0.840299]\n",
      "epoch:210 batch:52 loss:0.836426 accu:0.234375 gene_loss:[validity:0.883286]\n",
      "epoch:211 batch:52 loss:0.748441 accu:0.406250 gene_loss:[validity:0.867986]\n",
      "epoch:212 batch:52 loss:0.860448 accu:0.156250 gene_loss:[validity:0.905632]\n",
      "epoch:213 batch:52 loss:0.791800 accu:0.234375 gene_loss:[validity:0.702570]\n",
      "epoch:214 batch:52 loss:0.825460 accu:0.179688 gene_loss:[validity:0.704716]\n",
      "epoch:215 batch:52 loss:0.811832 accu:0.250000 gene_loss:[validity:0.724334]\n",
      "epoch:216 batch:52 loss:0.733967 accu:0.468750 gene_loss:[validity:0.756632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:217 batch:52 loss:0.725539 accu:0.484375 gene_loss:[validity:0.845774]\n",
      "epoch:218 batch:52 loss:0.834516 accu:0.273438 gene_loss:[validity:0.894530]\n",
      "epoch:219 batch:52 loss:0.792093 accu:0.281250 gene_loss:[validity:0.799598]\n",
      "epoch:220 batch:52 loss:0.746984 accu:0.398438 gene_loss:[validity:0.922387]\n",
      "epoch:221 batch:52 loss:0.760470 accu:0.390625 gene_loss:[validity:0.840444]\n",
      "epoch:222 batch:52 loss:0.808584 accu:0.375000 gene_loss:[validity:0.856440]\n",
      "epoch:223 batch:52 loss:0.788391 accu:0.429688 gene_loss:[validity:0.910090]\n",
      "epoch:224 batch:52 loss:0.859570 accu:0.304688 gene_loss:[validity:0.829862]\n",
      "epoch:225 batch:52 loss:0.796993 accu:0.414062 gene_loss:[validity:0.861992]\n",
      "epoch:226 batch:52 loss:0.728416 accu:0.570312 gene_loss:[validity:0.763873]\n",
      "epoch:227 batch:52 loss:0.818009 accu:0.164062 gene_loss:[validity:0.726887]\n",
      "epoch:228 batch:52 loss:0.672452 accu:0.656250 gene_loss:[validity:0.780823]\n",
      "epoch:229 batch:52 loss:0.791970 accu:0.273438 gene_loss:[validity:0.812225]\n",
      "epoch:230 batch:52 loss:0.845310 accu:0.218750 gene_loss:[validity:0.833535]\n",
      "epoch:231 batch:52 loss:0.761984 accu:0.265625 gene_loss:[validity:0.875052]\n",
      "epoch:232 batch:52 loss:0.762414 accu:0.359375 gene_loss:[validity:0.702072]\n",
      "epoch:233 batch:52 loss:0.774034 accu:0.273438 gene_loss:[validity:0.824852]\n",
      "epoch:234 batch:52 loss:0.886317 accu:0.031250 gene_loss:[validity:0.817945]\n",
      "epoch:235 batch:52 loss:0.827084 accu:0.195312 gene_loss:[validity:0.862728]\n",
      "epoch:236 batch:52 loss:0.706048 accu:0.398438 gene_loss:[validity:0.907869]\n",
      "epoch:237 batch:52 loss:0.788771 accu:0.429688 gene_loss:[validity:0.717216]\n",
      "epoch:238 batch:52 loss:0.851400 accu:0.187500 gene_loss:[validity:0.810564]\n",
      "epoch:239 batch:52 loss:0.723320 accu:0.414062 gene_loss:[validity:0.840181]\n",
      "epoch:240 batch:52 loss:0.747929 accu:0.398438 gene_loss:[validity:0.820666]\n",
      "epoch:241 batch:52 loss:0.673971 accu:0.578125 gene_loss:[validity:0.816646]\n",
      "epoch:242 batch:52 loss:0.878468 accu:0.062500 gene_loss:[validity:0.850890]\n",
      "epoch:243 batch:52 loss:0.656956 accu:0.734375 gene_loss:[validity:0.735134]\n",
      "epoch:244 batch:52 loss:0.767449 accu:0.273438 gene_loss:[validity:0.754972]\n",
      "epoch:245 batch:52 loss:0.741630 accu:0.351562 gene_loss:[validity:0.857509]\n",
      "epoch:246 batch:52 loss:0.769059 accu:0.375000 gene_loss:[validity:0.786833]\n",
      "epoch:247 batch:52 loss:0.807389 accu:0.039062 gene_loss:[validity:0.877995]\n",
      "epoch:248 batch:52 loss:0.714583 accu:0.554688 gene_loss:[validity:0.681011]\n",
      "epoch:249 batch:52 loss:0.849201 accu:0.148438 gene_loss:[validity:0.714400]\n",
      "epoch:250 batch:52 loss:0.815919 accu:0.273438 gene_loss:[validity:0.724049]\n",
      "epoch:251 batch:52 loss:0.703678 accu:0.523438 gene_loss:[validity:0.943674]\n",
      "epoch:252 batch:52 loss:0.786757 accu:0.304688 gene_loss:[validity:0.907476]\n",
      "epoch:253 batch:52 loss:0.865507 accu:0.203125 gene_loss:[validity:0.804770]\n",
      "epoch:254 batch:52 loss:0.772309 accu:0.437500 gene_loss:[validity:0.916070]\n",
      "epoch:255 batch:52 loss:0.796393 accu:0.140625 gene_loss:[validity:0.684633]\n",
      "epoch:256 batch:52 loss:0.850341 accu:0.171875 gene_loss:[validity:0.689814]\n",
      "epoch:257 batch:52 loss:0.735017 accu:0.460938 gene_loss:[validity:0.778491]\n",
      "epoch:258 batch:52 loss:0.817018 accu:0.265625 gene_loss:[validity:0.810965]\n",
      "epoch:259 batch:52 loss:0.776286 accu:0.437500 gene_loss:[validity:0.763897]\n",
      "epoch:260 batch:52 loss:0.751137 accu:0.359375 gene_loss:[validity:0.794820]\n",
      "epoch:261 batch:52 loss:0.705080 accu:0.601562 gene_loss:[validity:0.672455]\n",
      "epoch:262 batch:52 loss:0.774474 accu:0.187500 gene_loss:[validity:0.760626]\n",
      "epoch:263 batch:52 loss:0.887740 accu:0.281250 gene_loss:[validity:0.733644]\n",
      "epoch:264 batch:52 loss:0.748725 accu:0.343750 gene_loss:[validity:0.825257]\n",
      "epoch:265 batch:52 loss:0.750256 accu:0.351562 gene_loss:[validity:0.871234]\n",
      "epoch:266 batch:52 loss:0.726840 accu:0.398438 gene_loss:[validity:0.688929]\n",
      "epoch:267 batch:52 loss:0.768715 accu:0.429688 gene_loss:[validity:0.856768]\n",
      "epoch:268 batch:52 loss:0.732266 accu:0.328125 gene_loss:[validity:0.764513]\n",
      "epoch:269 batch:52 loss:0.781285 accu:0.343750 gene_loss:[validity:0.942729]\n",
      "epoch:270 batch:52 loss:0.851002 accu:0.093750 gene_loss:[validity:0.903847]\n",
      "epoch:271 batch:52 loss:0.760960 accu:0.476562 gene_loss:[validity:0.937306]\n",
      "epoch:272 batch:52 loss:0.834427 accu:0.117188 gene_loss:[validity:0.866389]\n",
      "epoch:273 batch:52 loss:0.749906 accu:0.390625 gene_loss:[validity:0.815606]\n",
      "epoch:274 batch:52 loss:0.844186 accu:0.085938 gene_loss:[validity:0.766849]\n",
      "epoch:275 batch:52 loss:0.732621 accu:0.421875 gene_loss:[validity:0.734976]\n",
      "epoch:276 batch:52 loss:0.726040 accu:0.484375 gene_loss:[validity:0.720908]\n",
      "epoch:277 batch:52 loss:0.825064 accu:0.187500 gene_loss:[validity:0.797251]\n",
      "epoch:278 batch:52 loss:0.748247 accu:0.257812 gene_loss:[validity:0.794034]\n",
      "epoch:279 batch:52 loss:0.770139 accu:0.312500 gene_loss:[validity:0.761756]\n",
      "epoch:280 batch:52 loss:0.740155 accu:0.296875 gene_loss:[validity:0.864902]\n",
      "epoch:281 batch:52 loss:0.727968 accu:0.460938 gene_loss:[validity:0.833724]\n",
      "epoch:282 batch:52 loss:0.796708 accu:0.164062 gene_loss:[validity:0.804915]\n",
      "epoch:283 batch:52 loss:0.736350 accu:0.195312 gene_loss:[validity:0.835021]\n",
      "epoch:284 batch:52 loss:0.787261 accu:0.203125 gene_loss:[validity:0.783211]\n",
      "epoch:285 batch:52 loss:0.768912 accu:0.273438 gene_loss:[validity:0.770466]\n",
      "epoch:286 batch:52 loss:0.796272 accu:0.281250 gene_loss:[validity:0.911402]\n",
      "epoch:287 batch:52 loss:0.786732 accu:0.398438 gene_loss:[validity:0.820768]\n",
      "epoch:288 batch:52 loss:0.771581 accu:0.304688 gene_loss:[validity:0.756450]\n",
      "epoch:289 batch:52 loss:0.749660 accu:0.437500 gene_loss:[validity:0.813911]\n",
      "epoch:290 batch:52 loss:0.761592 accu:0.242188 gene_loss:[validity:0.668930]\n",
      "epoch:291 batch:52 loss:0.755619 accu:0.382812 gene_loss:[validity:0.794269]\n",
      "epoch:292 batch:52 loss:0.839335 accu:0.132812 gene_loss:[validity:0.733504]\n",
      "epoch:293 batch:52 loss:0.777924 accu:0.320312 gene_loss:[validity:0.776499]\n",
      "epoch:294 batch:52 loss:0.747251 accu:0.453125 gene_loss:[validity:0.834147]\n",
      "epoch:295 batch:52 loss:0.762650 accu:0.187500 gene_loss:[validity:0.811440]\n",
      "epoch:296 batch:52 loss:0.803980 accu:0.359375 gene_loss:[validity:0.839206]\n",
      "epoch:297 batch:52 loss:0.769370 accu:0.312500 gene_loss:[validity:0.817995]\n",
      "epoch:298 batch:52 loss:0.752165 accu:0.328125 gene_loss:[validity:0.704120]\n",
      "epoch:299 batch:52 loss:0.677229 accu:0.617188 gene_loss:[validity:0.719085]\n",
      "epoch:300 batch:52 loss:0.753636 accu:0.507812 gene_loss:[validity:0.622711]\n",
      "epoch:301 batch:52 loss:0.792129 accu:0.289062 gene_loss:[validity:0.688417]\n",
      "epoch:302 batch:52 loss:0.728455 accu:0.429688 gene_loss:[validity:0.729517]\n",
      "epoch:303 batch:52 loss:0.752464 accu:0.312500 gene_loss:[validity:0.764655]\n",
      "epoch:304 batch:52 loss:0.802219 accu:0.304688 gene_loss:[validity:0.790545]\n",
      "epoch:305 batch:52 loss:0.592557 accu:0.804688 gene_loss:[validity:0.904457]\n",
      "epoch:306 batch:52 loss:0.805380 accu:0.257812 gene_loss:[validity:0.687766]\n",
      "epoch:307 batch:52 loss:0.841718 accu:0.101562 gene_loss:[validity:0.800336]\n",
      "epoch:308 batch:52 loss:0.870781 accu:0.070312 gene_loss:[validity:0.833916]\n",
      "epoch:309 batch:52 loss:0.786294 accu:0.375000 gene_loss:[validity:0.831480]\n",
      "epoch:310 batch:52 loss:0.735564 accu:0.414062 gene_loss:[validity:0.885559]\n",
      "epoch:311 batch:52 loss:0.811342 accu:0.070312 gene_loss:[validity:0.834940]\n",
      "epoch:312 batch:52 loss:0.741320 accu:0.273438 gene_loss:[validity:0.809582]\n",
      "epoch:313 batch:52 loss:0.778688 accu:0.289062 gene_loss:[validity:0.772612]\n",
      "epoch:314 batch:52 loss:0.776737 accu:0.312500 gene_loss:[validity:0.781956]\n",
      "epoch:315 batch:52 loss:0.763894 accu:0.304688 gene_loss:[validity:0.732248]\n",
      "epoch:316 batch:52 loss:0.753290 accu:0.390625 gene_loss:[validity:0.791474]\n",
      "epoch:317 batch:52 loss:0.786651 accu:0.187500 gene_loss:[validity:0.710870]\n",
      "epoch:318 batch:52 loss:0.748448 accu:0.320312 gene_loss:[validity:0.820129]\n",
      "epoch:319 batch:52 loss:0.730673 accu:0.460938 gene_loss:[validity:0.822567]\n",
      "epoch:320 batch:52 loss:0.750968 accu:0.398438 gene_loss:[validity:0.767821]\n",
      "epoch:321 batch:52 loss:0.763689 accu:0.117188 gene_loss:[validity:0.736481]\n",
      "epoch:322 batch:52 loss:0.711205 accu:0.531250 gene_loss:[validity:0.795445]\n",
      "epoch:323 batch:52 loss:0.775157 accu:0.273438 gene_loss:[validity:0.804969]\n",
      "epoch:324 batch:52 loss:0.770635 accu:0.429688 gene_loss:[validity:0.732762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:325 batch:52 loss:0.785993 accu:0.257812 gene_loss:[validity:0.784843]\n",
      "epoch:326 batch:52 loss:0.742575 accu:0.445312 gene_loss:[validity:0.790473]\n",
      "epoch:327 batch:52 loss:0.727866 accu:0.398438 gene_loss:[validity:0.738341]\n",
      "epoch:328 batch:52 loss:0.735371 accu:0.156250 gene_loss:[validity:0.826841]\n",
      "epoch:329 batch:52 loss:0.768925 accu:0.382812 gene_loss:[validity:0.768144]\n",
      "epoch:330 batch:52 loss:0.749764 accu:0.406250 gene_loss:[validity:0.855385]\n",
      "epoch:331 batch:52 loss:0.730049 accu:0.398438 gene_loss:[validity:0.776720]\n",
      "epoch:332 batch:52 loss:0.771669 accu:0.242188 gene_loss:[validity:0.739900]\n",
      "epoch:333 batch:52 loss:0.739902 accu:0.492188 gene_loss:[validity:0.753379]\n",
      "epoch:334 batch:52 loss:0.725389 accu:0.296875 gene_loss:[validity:0.776556]\n",
      "epoch:335 batch:52 loss:0.811292 accu:0.140625 gene_loss:[validity:0.786877]\n",
      "epoch:336 batch:52 loss:0.745583 accu:0.195312 gene_loss:[validity:0.739817]\n",
      "epoch:337 batch:52 loss:0.700145 accu:0.539062 gene_loss:[validity:0.769773]\n",
      "epoch:338 batch:52 loss:0.768909 accu:0.265625 gene_loss:[validity:0.794778]\n",
      "epoch:339 batch:52 loss:0.709546 accu:0.578125 gene_loss:[validity:0.774257]\n",
      "epoch:340 batch:52 loss:0.744298 accu:0.414062 gene_loss:[validity:0.745510]\n",
      "epoch:341 batch:52 loss:0.748434 accu:0.234375 gene_loss:[validity:0.714001]\n",
      "epoch:342 batch:52 loss:0.718149 accu:0.500000 gene_loss:[validity:0.815048]\n",
      "epoch:343 batch:52 loss:0.768218 accu:0.320312 gene_loss:[validity:0.772440]\n",
      "epoch:344 batch:52 loss:0.742665 accu:0.398438 gene_loss:[validity:0.654294]\n",
      "epoch:345 batch:52 loss:0.759864 accu:0.312500 gene_loss:[validity:0.685258]\n",
      "epoch:346 batch:52 loss:0.730834 accu:0.328125 gene_loss:[validity:0.679041]\n",
      "epoch:347 batch:52 loss:0.755737 accu:0.273438 gene_loss:[validity:0.704069]\n",
      "epoch:348 batch:52 loss:0.735318 accu:0.437500 gene_loss:[validity:0.749730]\n",
      "epoch:349 batch:52 loss:0.811041 accu:0.242188 gene_loss:[validity:0.796473]\n",
      "epoch:350 batch:52 loss:0.712166 accu:0.523438 gene_loss:[validity:0.746601]\n",
      "epoch:351 batch:52 loss:0.625632 accu:0.734375 gene_loss:[validity:0.754801]\n",
      "epoch:352 batch:52 loss:0.789351 accu:0.359375 gene_loss:[validity:0.816218]\n",
      "epoch:353 batch:52 loss:0.669797 accu:0.585938 gene_loss:[validity:0.914372]\n",
      "epoch:354 batch:52 loss:0.761393 accu:0.273438 gene_loss:[validity:0.983115]\n",
      "epoch:355 batch:52 loss:0.765965 accu:0.164062 gene_loss:[validity:0.909868]\n",
      "epoch:356 batch:52 loss:0.752821 accu:0.210938 gene_loss:[validity:0.804889]\n",
      "epoch:357 batch:52 loss:0.660824 accu:0.679688 gene_loss:[validity:0.724785]\n",
      "epoch:358 batch:52 loss:0.755086 accu:0.453125 gene_loss:[validity:0.781215]\n",
      "epoch:359 batch:52 loss:0.782802 accu:0.398438 gene_loss:[validity:0.922615]\n",
      "epoch:360 batch:52 loss:0.826714 accu:0.125000 gene_loss:[validity:0.683780]\n",
      "epoch:361 batch:52 loss:0.654720 accu:0.632812 gene_loss:[validity:0.648081]\n",
      "epoch:362 batch:52 loss:0.748558 accu:0.218750 gene_loss:[validity:0.636717]\n",
      "epoch:363 batch:52 loss:0.671263 accu:0.687500 gene_loss:[validity:0.688760]\n",
      "epoch:364 batch:52 loss:0.585633 accu:0.804688 gene_loss:[validity:0.904874]\n",
      "epoch:365 batch:52 loss:0.788498 accu:0.390625 gene_loss:[validity:1.221726]\n",
      "epoch:366 batch:52 loss:0.903129 accu:0.000000 gene_loss:[validity:0.700955]\n",
      "epoch:367 batch:52 loss:0.624559 accu:0.742188 gene_loss:[validity:0.804248]\n",
      "epoch:368 batch:52 loss:0.794111 accu:0.171875 gene_loss:[validity:0.764009]\n",
      "epoch:369 batch:52 loss:0.845916 accu:0.140625 gene_loss:[validity:0.627190]\n",
      "epoch:370 batch:52 loss:0.698563 accu:0.484375 gene_loss:[validity:0.721519]\n",
      "epoch:371 batch:52 loss:0.730739 accu:0.492188 gene_loss:[validity:0.745456]\n",
      "epoch:372 batch:52 loss:0.767739 accu:0.203125 gene_loss:[validity:0.930589]\n",
      "epoch:373 batch:52 loss:0.637042 accu:0.703125 gene_loss:[validity:0.710558]\n",
      "epoch:374 batch:52 loss:0.805559 accu:0.179688 gene_loss:[validity:0.795822]\n",
      "epoch:375 batch:52 loss:0.727989 accu:0.421875 gene_loss:[validity:0.826272]\n",
      "epoch:376 batch:52 loss:0.811948 accu:0.257812 gene_loss:[validity:0.838498]\n",
      "epoch:377 batch:52 loss:0.896663 accu:0.132812 gene_loss:[validity:0.712627]\n",
      "epoch:378 batch:52 loss:0.739772 accu:0.484375 gene_loss:[validity:0.693968]\n",
      "epoch:379 batch:52 loss:0.662736 accu:0.765625 gene_loss:[validity:0.656980]\n",
      "epoch:380 batch:52 loss:0.734076 accu:0.437500 gene_loss:[validity:0.766764]\n",
      "epoch:381 batch:52 loss:0.678717 accu:0.812500 gene_loss:[validity:0.845012]\n",
      "epoch:382 batch:52 loss:0.869884 accu:0.070312 gene_loss:[validity:0.633492]\n",
      "epoch:383 batch:52 loss:0.849272 accu:0.226562 gene_loss:[validity:0.680398]\n",
      "epoch:384 batch:52 loss:0.769028 accu:0.367188 gene_loss:[validity:0.625932]\n",
      "epoch:385 batch:52 loss:0.751826 accu:0.375000 gene_loss:[validity:0.740599]\n",
      "epoch:386 batch:52 loss:0.837586 accu:0.000000 gene_loss:[validity:0.733554]\n",
      "epoch:387 batch:52 loss:0.728039 accu:0.445312 gene_loss:[validity:0.663817]\n",
      "epoch:388 batch:52 loss:0.736504 accu:0.359375 gene_loss:[validity:0.781394]\n",
      "epoch:389 batch:52 loss:0.782774 accu:0.234375 gene_loss:[validity:0.713900]\n",
      "epoch:390 batch:52 loss:0.714481 accu:0.437500 gene_loss:[validity:0.749007]\n",
      "epoch:391 batch:52 loss:0.845904 accu:0.148438 gene_loss:[validity:0.703760]\n",
      "epoch:392 batch:52 loss:0.773776 accu:0.492188 gene_loss:[validity:0.685998]\n",
      "epoch:393 batch:52 loss:0.753074 accu:0.328125 gene_loss:[validity:0.688081]\n",
      "epoch:394 batch:52 loss:0.695422 accu:0.546875 gene_loss:[validity:0.770952]\n",
      "epoch:395 batch:52 loss:0.715863 accu:0.375000 gene_loss:[validity:0.785791]\n",
      "epoch:396 batch:52 loss:0.761562 accu:0.351562 gene_loss:[validity:0.843786]\n",
      "epoch:397 batch:52 loss:0.748530 accu:0.351562 gene_loss:[validity:0.735110]\n",
      "epoch:398 batch:52 loss:0.768815 accu:0.250000 gene_loss:[validity:0.704816]\n",
      "epoch:399 batch:52 loss:0.735663 accu:0.265625 gene_loss:[validity:0.722932]\n",
      "epoch:400 batch:52 loss:0.744556 accu:0.109375 gene_loss:[validity:0.761757]\n"
     ]
    }
   ],
   "source": [
    "real_labels = np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(10001):\n",
    "    for j in range(int(IMAGES_COUNT/BATCH_SIZE)):\n",
    "    \n",
    "        #==========\n",
    "        #随机采样得到\n",
    "        noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "        corresponding_fake_label = np.random.randint(low=0 , high=CLASS_NUM , size=(BATCH_SIZE , 1)) #label的取值范围 可能会发生变化\n",
    "        #==========\n",
    "\n",
    "        real_image , corresponding_real_label = load_image()\n",
    "\n",
    "        #训练判别器\n",
    "        fake_image = generator_i.predict([noise , corresponding_fake_label])\n",
    "\n",
    "        real_loss = discriminator_i.train_on_batch([real_image , corresponding_real_label] , real_labels)\n",
    "        fake_loss = discriminator_i.train_on_batch([fake_image , corresponding_fake_label] , fake_labels) #应该是real还是fake\n",
    "\n",
    "        loss = np.add(real_loss , fake_loss)/2\n",
    "\n",
    "        #训练生成器\n",
    "        noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "        corresponding_fake_label2 = np.random.randint(low=0 , high=CLASS_NUM , size=(BATCH_SIZE , 1))\n",
    "\n",
    "            #下面的损失是一个list 有两个损失 一个是validity一个是与label的softmax\n",
    "        generator_loss = combined_model_i.train_on_batch([noise2 , corresponding_fake_label2] , real_labels)\n",
    "\n",
    "        print('epoch:%d batch:%d loss:%f accu:%f gene_loss:[validity:%f]' % (i , j , loss[0] , loss[1] , generator_loss))\n",
    "\n",
    "    #if i % 100 == 0:\n",
    "        write_image(i)\n",
    "    \n",
    "write_image(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
