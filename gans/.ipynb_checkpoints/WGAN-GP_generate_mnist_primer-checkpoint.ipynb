{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "#_Merge\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 28\n",
    "HEIGHT = 28\n",
    "CHANNEL = 1\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = 'faces/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n",
    "\n",
    "#为WGAN增加的\n",
    "N_CRITIC = 5 #训练G时使用\n",
    "CLIP_VALUE = 0.01 #更新G的权重参数时进行截断使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nload_index = 0\\n\\nimages_name = os.listdir(PATH)\\n\\nIMAGES_COUNT = len(images_name)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "(X_train , y_train),(X_test , y_test) = mnist.load_data()\n",
    "X_train = X_train/127.5-1\n",
    "X_train = np.expand_dims(X_train , 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist():\n",
    "    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\n",
    "    \n",
    "def write_image_mnist(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = generated_image*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('mnist_wgan-gp/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_image(batch_size = BATCH_SIZE):\\n    global load_index\\n    \\n    images = []\\n    \\n    for i in range(batch_size):\\n        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\\n    \\n    load_index += batch_size\\n    \\n    return np.array(images)/127.5-1\\n\\ndef write_image(epoch):\\n    \\n    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\\n    generated_image = generator_i.predict(noise)\\n    generated_image = (generated_image+1)*127.5\\n    \\n    fig , axes = plt.pyplot.subplots(ROW , COL)\\n    \\n    count=0\\n    \\n    for i in range(ROW):\\n        for j in range(COL):\\n            axes[i][j].imshow(generated_image[count])\\n            axes[i][j].axis('off')\\n            count += 1\\n            \\n    fig.savefig('generated_faces_wgan/No.%d.png' % epoch)\\n    plt.pyplot.close()\\n    \\n    #plt.image.imsave('images/'+str(epoch)+'.jpg')\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('generated_faces_wgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "\n",
    "    #mnist 图像使用 28*28*1\n",
    "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(UpSampling2D())\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    model.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    image = model(noise)\n",
    "    \n",
    "    return Model(noise , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='critic')\n",
    "    \n",
    "    model.add(Conv2D(filters=16 , kernel_size=(3,3) , strides=(2,2) , padding='same' , input_shape=(WIDTH , HEIGHT , CHANNEL) , name='conv1'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=32 , kernel_size=(3,3) , strides=(2,2) , padding='same' , name='conv2'))\n",
    "    model.add(ZeroPadding2D(padding=((0,1) , (0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=64 , kernel_size=(3,3) , strides=(2,2) , padding='same' , name='conv3'))\n",
    "    model.add(BatchNormalization(momentum=0.8))  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(filters=128 , kernel_size=(3,3) , strides=(1,1) , name='conv4'))\n",
    "    model.add(BatchNormalization(momentum=0.8))  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=1 , name='dense16')) #最后一层取消sigmoid 不使用非线性激活 WGAN\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    validity = model(image)\n",
    "    \n",
    "    return Model(image , validity , name='critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsprop = RMSprop(lr=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#需要继承_Merge类\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((BATCH_SIZE, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wgan_loss(y_true , y_pred):\n",
    "    return K.mean(y_true*y_pred)\n",
    "\n",
    "def GP_penelty_loss(y_true , y_pred , averaged_samples): #WGAN-GP在原有GAN损失中增加的损失\n",
    "    gradients = K.gradients(y_pred , averaged_samples)[0] #y_true在此位置代替的是averaged_samples \n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr , axis=np.arange(1 , len(gradients_sqr.shape)))\n",
    "    \n",
    "    gradients_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    \n",
    "    gradients_penalty = K.square(1-gradients_l2_norm)\n",
    "    \n",
    "    return K.mean(gradients_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "critic_i = critic()\n",
    "generator_i = generator()\n",
    "\n",
    "generator_i.trainable = False\n",
    "\n",
    "#critic_i.compile(optimizer=rmsprop , loss=wgan_loss , metrics=['accuracy'])\n",
    "\n",
    "real_image = Input(shape=(HEIGHT , WIDTH , CHANNEL) , name='real_image')\n",
    "\n",
    "\n",
    "z = Input(shape=(LATENT_DIM , ) , name = 'z')\n",
    "fake_image = generator_i(z)\n",
    "\n",
    "validity_fake = critic_i(fake_image)\n",
    "validity_real = critic_i(real_image)\n",
    "\n",
    "#根据WGAN-GP中的loss公式中的插值的样本\n",
    "interpolation_real_fake_image = RandomWeightedAverage()([real_image , fake_image])\n",
    "validity_interpolation = critic_i(interpolation_real_fake_image)\n",
    "\n",
    "#==========\n",
    "#partial为python函数 类似装饰器\n",
    "#下面就是在原有的GAN上的损失添加的惩罚项\n",
    "partial_gp_loss = partial(GP_penelty_loss , averaged_samples=interpolation_real_fake_image)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "#==========\n",
    "\n",
    "critic_model = Model(inputs=[real_image , z] , outputs=[validity_real , validity_fake , validity_interpolation] , name='critic_model')\n",
    "critic_model.compile(optimizer=rmsprop , loss=[wgan_loss , wgan_loss , partial_gp_loss] , loss_weights=[1,1,10])\n",
    "\n",
    "#==========\n",
    "critic_i.trainable = False\n",
    "generator_i.trainable = True\n",
    "\n",
    "\n",
    "z_ = Input(shape=(LATENT_DIM , ))\n",
    "image_ = generator_i(z_)\n",
    "valid_ = critic_i(image_)\n",
    "\n",
    "generator_model = Model(z_ , valid_)\n",
    "generator_model.compile(optimizer=rmsprop , loss=wgan_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss1:16.171274 loss2:0.070900 loss3:0.113360 gene_loss:-0.304509\n",
      "epoch:1 loss1:13.193399 loss2:0.105765 loss3:0.321583 gene_loss:-0.043860\n",
      "epoch:2 loss1:12.286305 loss2:-0.011313 loss3:0.212316 gene_loss:-0.291148\n",
      "epoch:3 loss1:7.768289 loss2:-0.059637 loss3:0.207096 gene_loss:-0.137341\n",
      "epoch:4 loss1:9.104754 loss2:-0.097519 loss3:0.139051 gene_loss:-0.376567\n",
      "epoch:5 loss1:9.025387 loss2:-0.259172 loss3:0.234452 gene_loss:-0.246675\n",
      "epoch:6 loss1:5.669413 loss2:-0.247442 loss3:0.248231 gene_loss:-0.271002\n",
      "epoch:7 loss1:5.961509 loss2:-0.076742 loss3:0.132393 gene_loss:-0.225727\n",
      "epoch:8 loss1:6.616660 loss2:-0.399122 loss3:0.146925 gene_loss:-0.191591\n",
      "epoch:9 loss1:3.940901 loss2:-0.400860 loss3:0.161166 gene_loss:-0.180558\n",
      "epoch:10 loss1:4.881891 loss2:-0.430288 loss3:0.212942 gene_loss:-0.237626\n",
      "epoch:11 loss1:2.980280 loss2:-0.533225 loss3:0.221612 gene_loss:-0.319159\n",
      "epoch:12 loss1:2.835474 loss2:-0.578522 loss3:0.174249 gene_loss:-0.206332\n",
      "epoch:13 loss1:2.295250 loss2:-0.477330 loss3:0.151918 gene_loss:-0.267049\n",
      "epoch:14 loss1:1.663740 loss2:-0.854841 loss3:0.166180 gene_loss:-0.164067\n",
      "epoch:15 loss1:1.454822 loss2:-0.845457 loss3:0.104341 gene_loss:-0.175211\n",
      "epoch:16 loss1:0.708401 loss2:-0.741763 loss3:0.191166 gene_loss:-0.204190\n",
      "epoch:17 loss1:0.909902 loss2:-0.949496 loss3:0.234993 gene_loss:-0.355901\n",
      "epoch:18 loss1:0.811754 loss2:-0.798119 loss3:0.240334 gene_loss:-0.399400\n",
      "epoch:19 loss1:0.129992 loss2:-1.105436 loss3:0.248785 gene_loss:-0.259557\n",
      "epoch:20 loss1:0.166164 loss2:-0.981265 loss3:0.427832 gene_loss:-0.408333\n",
      "epoch:21 loss1:0.312515 loss2:-1.087432 loss3:0.351778 gene_loss:-0.469492\n",
      "epoch:22 loss1:0.378383 loss2:-1.222003 loss3:0.593009 gene_loss:-0.403322\n",
      "epoch:23 loss1:-0.237643 loss2:-1.376091 loss3:0.456713 gene_loss:-0.481260\n",
      "epoch:24 loss1:-0.188352 loss2:-1.414230 loss3:0.518790 gene_loss:-0.595528\n",
      "epoch:25 loss1:-0.397166 loss2:-1.409671 loss3:0.582094 gene_loss:-0.669681\n",
      "epoch:26 loss1:-0.428117 loss2:-1.545334 loss3:0.600366 gene_loss:-0.705820\n",
      "epoch:27 loss1:-0.748250 loss2:-1.695395 loss3:0.563303 gene_loss:-0.709096\n",
      "epoch:28 loss1:-0.853620 loss2:-1.858023 loss3:0.674511 gene_loss:-0.803078\n",
      "epoch:29 loss1:-0.824849 loss2:-1.959599 loss3:0.820383 gene_loss:-0.697456\n",
      "epoch:30 loss1:-0.719381 loss2:-2.042133 loss3:1.037176 gene_loss:-0.956701\n",
      "epoch:31 loss1:-0.811123 loss2:-2.176125 loss3:1.077161 gene_loss:-1.068939\n",
      "epoch:32 loss1:-1.067306 loss2:-2.265160 loss3:0.927869 gene_loss:-1.061761\n",
      "epoch:33 loss1:-1.200385 loss2:-2.372573 loss3:1.063303 gene_loss:-1.082505\n",
      "epoch:34 loss1:-1.251263 loss2:-2.518659 loss3:1.084998 gene_loss:-1.062838\n",
      "epoch:35 loss1:-1.252086 loss2:-2.460589 loss3:1.074693 gene_loss:-1.140755\n",
      "epoch:36 loss1:-0.987527 loss2:-2.478299 loss3:1.316799 gene_loss:-1.367362\n",
      "epoch:37 loss1:-1.152462 loss2:-2.613462 loss3:1.314296 gene_loss:-1.360547\n",
      "epoch:38 loss1:-1.039755 loss2:-2.651385 loss3:1.400054 gene_loss:-1.380156\n",
      "epoch:39 loss1:-1.174303 loss2:-2.885452 loss3:1.571219 gene_loss:-1.411034\n",
      "epoch:40 loss1:-1.317074 loss2:-3.016859 loss3:1.578704 gene_loss:-1.449573\n",
      "epoch:41 loss1:-1.232716 loss2:-2.800084 loss3:1.479508 gene_loss:-1.503591\n",
      "epoch:42 loss1:-1.315993 loss2:-2.972309 loss3:1.461187 gene_loss:-1.335878\n",
      "epoch:43 loss1:-1.362195 loss2:-3.088927 loss3:1.635012 gene_loss:-1.517327\n",
      "epoch:44 loss1:-1.235668 loss2:-3.033201 loss3:1.645324 gene_loss:-1.587633\n",
      "epoch:45 loss1:-1.154650 loss2:-3.099390 loss3:1.786946 gene_loss:-1.703092\n",
      "epoch:46 loss1:-1.253187 loss2:-3.194477 loss3:1.805595 gene_loss:-1.747274\n",
      "epoch:47 loss1:-1.313472 loss2:-3.083046 loss3:1.636589 gene_loss:-1.639142\n",
      "epoch:48 loss1:-1.318802 loss2:-3.165242 loss3:1.629202 gene_loss:-1.802367\n",
      "epoch:49 loss1:-0.972880 loss2:-2.966749 loss3:1.807777 gene_loss:-1.623882\n",
      "epoch:50 loss1:-1.495707 loss2:-3.185894 loss3:1.529502 gene_loss:-1.914849\n",
      "epoch:51 loss1:-1.304465 loss2:-3.062109 loss3:1.545662 gene_loss:-1.734892\n",
      "epoch:52 loss1:-1.221205 loss2:-3.172793 loss3:1.824205 gene_loss:-1.492013\n",
      "epoch:53 loss1:-1.177231 loss2:-3.157829 loss3:1.824407 gene_loss:-1.825135\n",
      "epoch:54 loss1:-1.173797 loss2:-3.068528 loss3:1.679372 gene_loss:-1.506886\n",
      "epoch:55 loss1:-1.255806 loss2:-3.210304 loss3:1.857236 gene_loss:-1.810902\n",
      "epoch:56 loss1:-1.290981 loss2:-3.077484 loss3:1.622885 gene_loss:-1.776466\n",
      "epoch:57 loss1:-1.266970 loss2:-3.209594 loss3:1.798585 gene_loss:-1.953377\n",
      "epoch:58 loss1:-1.085021 loss2:-2.971321 loss3:1.730544 gene_loss:-1.733689\n",
      "epoch:59 loss1:-0.979721 loss2:-3.055079 loss3:1.875169 gene_loss:-1.660326\n",
      "epoch:60 loss1:-1.157566 loss2:-3.149060 loss3:1.887355 gene_loss:-1.777040\n",
      "epoch:61 loss1:-0.882125 loss2:-3.085781 loss3:2.053581 gene_loss:-1.792846\n",
      "epoch:62 loss1:-0.979356 loss2:-3.214756 loss3:2.145122 gene_loss:-2.014200\n",
      "epoch:63 loss1:-1.124653 loss2:-3.431297 loss3:2.142166 gene_loss:-2.138446\n",
      "epoch:64 loss1:-1.395094 loss2:-3.301551 loss3:1.791438 gene_loss:-1.867599\n",
      "epoch:65 loss1:-0.918842 loss2:-3.128798 loss3:2.049021 gene_loss:-1.930597\n",
      "epoch:66 loss1:-1.111267 loss2:-3.040925 loss3:1.840928 gene_loss:-1.701537\n",
      "epoch:67 loss1:-0.966676 loss2:-3.094064 loss3:2.040727 gene_loss:-2.061235\n",
      "epoch:68 loss1:-0.785397 loss2:-3.297633 loss3:2.372806 gene_loss:-1.994281\n",
      "epoch:69 loss1:-0.976794 loss2:-3.187842 loss3:2.108727 gene_loss:-1.952725\n",
      "epoch:70 loss1:-1.056278 loss2:-3.200846 loss3:2.045305 gene_loss:-2.044704\n",
      "epoch:71 loss1:-1.202058 loss2:-3.408121 loss3:2.113041 gene_loss:-2.186205\n",
      "epoch:72 loss1:-1.116143 loss2:-3.351971 loss3:2.163290 gene_loss:-2.093151\n",
      "epoch:73 loss1:-0.924084 loss2:-3.150275 loss3:2.125168 gene_loss:-2.089557\n",
      "epoch:74 loss1:-1.047891 loss2:-3.303284 loss3:2.133308 gene_loss:-1.909890\n",
      "epoch:75 loss1:-0.622182 loss2:-3.045717 loss3:2.290787 gene_loss:-2.114752\n",
      "epoch:76 loss1:-0.921067 loss2:-3.314323 loss3:2.300045 gene_loss:-2.207325\n",
      "epoch:77 loss1:-0.644563 loss2:-2.892994 loss3:2.094627 gene_loss:-2.421522\n",
      "epoch:78 loss1:-1.114317 loss2:-3.338984 loss3:2.109686 gene_loss:-1.894892\n",
      "epoch:79 loss1:-0.953895 loss2:-3.192467 loss3:2.106501 gene_loss:-2.388508\n",
      "epoch:80 loss1:-0.771499 loss2:-3.250968 loss3:2.364455 gene_loss:-2.276879\n",
      "epoch:81 loss1:-0.612697 loss2:-3.220062 loss3:2.495388 gene_loss:-2.092920\n",
      "epoch:82 loss1:-0.850625 loss2:-2.922955 loss3:1.972665 gene_loss:-2.376245\n",
      "epoch:83 loss1:-0.673303 loss2:-2.968738 loss3:2.166701 gene_loss:-1.892870\n",
      "epoch:84 loss1:-1.056311 loss2:-3.129255 loss3:1.969776 gene_loss:-2.158248\n",
      "epoch:85 loss1:-0.687758 loss2:-3.102788 loss3:2.306044 gene_loss:-2.099584\n",
      "epoch:86 loss1:-0.821715 loss2:-3.117399 loss3:2.174647 gene_loss:-2.344088\n",
      "epoch:87 loss1:-0.588498 loss2:-2.760144 loss3:2.088106 gene_loss:-1.982712\n",
      "epoch:88 loss1:-0.960684 loss2:-3.020225 loss3:1.948836 gene_loss:-2.085680\n",
      "epoch:89 loss1:-1.110703 loss2:-3.101955 loss3:1.879042 gene_loss:-1.953159\n",
      "epoch:90 loss1:-0.982363 loss2:-3.053524 loss3:1.946148 gene_loss:-1.894021\n",
      "epoch:91 loss1:-1.177507 loss2:-2.934511 loss3:1.655198 gene_loss:-1.910761\n",
      "epoch:92 loss1:-0.966495 loss2:-2.878602 loss3:1.802384 gene_loss:-2.054871\n",
      "epoch:93 loss1:-1.047750 loss2:-2.828238 loss3:1.663638 gene_loss:-1.737580\n",
      "epoch:94 loss1:-1.260295 loss2:-2.912271 loss3:1.503300 gene_loss:-1.822846\n",
      "epoch:95 loss1:-0.898202 loss2:-2.717139 loss3:1.702316 gene_loss:-1.662166\n",
      "epoch:96 loss1:-0.880242 loss2:-2.876710 loss3:1.884456 gene_loss:-1.607100\n",
      "epoch:97 loss1:-0.843472 loss2:-2.709650 loss3:1.742874 gene_loss:-1.552862\n",
      "epoch:98 loss1:-0.882029 loss2:-2.703801 loss3:1.686022 gene_loss:-1.794894\n",
      "epoch:99 loss1:-0.843043 loss2:-2.805138 loss3:1.825009 gene_loss:-1.493236\n",
      "epoch:100 loss1:-0.748664 loss2:-2.508636 loss3:1.547999 gene_loss:-1.700359\n",
      "epoch:101 loss1:-0.733350 loss2:-2.410874 loss3:1.581347 gene_loss:-1.616510\n",
      "epoch:102 loss1:-0.848916 loss2:-2.419486 loss3:1.444052 gene_loss:-1.559243\n",
      "epoch:103 loss1:-0.941492 loss2:-2.565220 loss3:1.536086 gene_loss:-1.445069\n",
      "epoch:104 loss1:-0.526298 loss2:-2.160398 loss3:1.491202 gene_loss:-1.317922\n",
      "epoch:105 loss1:-1.020743 loss2:-2.552135 loss3:1.406861 gene_loss:-1.473119\n",
      "epoch:106 loss1:-0.776816 loss2:-2.534917 loss3:1.659845 gene_loss:-1.489264\n",
      "epoch:107 loss1:-0.974328 loss2:-2.404732 loss3:1.356371 gene_loss:-1.496189\n",
      "epoch:108 loss1:-0.765314 loss2:-2.309337 loss3:1.440672 gene_loss:-1.364197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:109 loss1:-0.783123 loss2:-2.373210 loss3:1.481918 gene_loss:-1.526514\n",
      "epoch:110 loss1:-0.449762 loss2:-2.207461 loss3:1.663684 gene_loss:-1.490426\n",
      "epoch:111 loss1:-0.855918 loss2:-2.391241 loss3:1.416718 gene_loss:-1.196634\n",
      "epoch:112 loss1:-1.056695 loss2:-2.252119 loss3:1.109933 gene_loss:-1.070109\n",
      "epoch:113 loss1:-0.926339 loss2:-2.356496 loss3:1.301231 gene_loss:-1.557015\n",
      "epoch:114 loss1:-1.223467 loss2:-2.432592 loss3:1.097682 gene_loss:-1.394512\n",
      "epoch:115 loss1:-0.827910 loss2:-2.181158 loss3:1.244040 gene_loss:-1.110052\n",
      "epoch:116 loss1:-1.098044 loss2:-2.388787 loss3:1.207406 gene_loss:-1.254758\n",
      "epoch:117 loss1:-0.802721 loss2:-2.294774 loss3:1.354756 gene_loss:-1.348081\n",
      "epoch:118 loss1:-1.184222 loss2:-2.413788 loss3:1.128280 gene_loss:-1.143997\n",
      "epoch:119 loss1:-0.777533 loss2:-2.083502 loss3:1.170601 gene_loss:-1.251981\n",
      "epoch:120 loss1:-0.881178 loss2:-2.325777 loss3:1.214676 gene_loss:-1.313480\n",
      "epoch:121 loss1:-1.012053 loss2:-2.205118 loss3:1.064413 gene_loss:-1.171937\n",
      "epoch:122 loss1:-1.040330 loss2:-2.141970 loss3:0.988248 gene_loss:-1.035795\n",
      "epoch:123 loss1:-0.752376 loss2:-1.904042 loss3:1.000797 gene_loss:-0.998117\n",
      "epoch:124 loss1:-1.010628 loss2:-2.314853 loss3:1.187480 gene_loss:-1.090233\n",
      "epoch:125 loss1:-1.052082 loss2:-2.321412 loss3:1.164599 gene_loss:-1.348721\n",
      "epoch:126 loss1:-1.006412 loss2:-2.087192 loss3:0.998329 gene_loss:-1.227493\n",
      "epoch:127 loss1:-0.936787 loss2:-2.236221 loss3:1.079893 gene_loss:-1.326567\n",
      "epoch:128 loss1:-0.669322 loss2:-1.995255 loss3:1.167441 gene_loss:-1.143833\n",
      "epoch:129 loss1:-0.782442 loss2:-2.065970 loss3:1.165663 gene_loss:-1.224181\n",
      "epoch:130 loss1:-0.714029 loss2:-2.157384 loss3:1.327963 gene_loss:-1.269493\n",
      "epoch:131 loss1:-0.852340 loss2:-1.978165 loss3:1.016800 gene_loss:-1.107033\n",
      "epoch:132 loss1:-0.905298 loss2:-2.051407 loss3:0.995508 gene_loss:-0.982889\n",
      "epoch:133 loss1:-1.047594 loss2:-2.128547 loss3:0.966133 gene_loss:-1.054070\n",
      "epoch:134 loss1:-0.821308 loss2:-2.024518 loss3:1.044889 gene_loss:-0.792796\n",
      "epoch:135 loss1:-0.620948 loss2:-2.021334 loss3:1.253702 gene_loss:-1.137686\n",
      "epoch:136 loss1:-0.732398 loss2:-2.040284 loss3:1.192063 gene_loss:-1.219099\n",
      "epoch:137 loss1:-0.775935 loss2:-2.244817 loss3:1.361650 gene_loss:-1.376237\n",
      "epoch:138 loss1:-0.802819 loss2:-2.123230 loss3:1.192848 gene_loss:-1.237861\n",
      "epoch:139 loss1:-0.719530 loss2:-2.102441 loss3:1.248100 gene_loss:-1.238007\n",
      "epoch:140 loss1:-0.908984 loss2:-2.338286 loss3:1.326951 gene_loss:-1.207481\n",
      "epoch:141 loss1:-0.978055 loss2:-2.327954 loss3:1.225920 gene_loss:-1.307636\n",
      "epoch:142 loss1:-0.740512 loss2:-2.194634 loss3:1.368823 gene_loss:-1.208466\n",
      "epoch:143 loss1:-0.656868 loss2:-2.377140 loss3:1.553279 gene_loss:-1.300092\n",
      "epoch:144 loss1:-0.734740 loss2:-2.191686 loss3:1.335249 gene_loss:-1.527003\n",
      "epoch:145 loss1:-0.941149 loss2:-2.420949 loss3:1.382915 gene_loss:-1.675963\n",
      "epoch:146 loss1:-0.641761 loss2:-2.275483 loss3:1.535076 gene_loss:-1.792132\n",
      "epoch:147 loss1:-0.653046 loss2:-2.458604 loss3:1.677897 gene_loss:-1.445055\n",
      "epoch:148 loss1:-0.707452 loss2:-2.512685 loss3:1.725139 gene_loss:-1.648166\n",
      "epoch:149 loss1:-0.620485 loss2:-2.421449 loss3:1.698648 gene_loss:-1.588973\n",
      "epoch:150 loss1:-0.937874 loss2:-2.622887 loss3:1.599631 gene_loss:-1.723594\n",
      "epoch:151 loss1:-0.663863 loss2:-2.437050 loss3:1.668357 gene_loss:-1.494317\n",
      "epoch:152 loss1:-0.520849 loss2:-2.508218 loss3:1.866714 gene_loss:-1.750287\n",
      "epoch:153 loss1:-0.807291 loss2:-2.601065 loss3:1.683392 gene_loss:-1.715174\n",
      "epoch:154 loss1:-0.871938 loss2:-2.569147 loss3:1.580501 gene_loss:-1.766597\n",
      "epoch:155 loss1:-0.822512 loss2:-2.610456 loss3:1.731168 gene_loss:-1.532759\n",
      "epoch:156 loss1:-0.700152 loss2:-2.692432 loss3:1.875896 gene_loss:-1.787392\n",
      "epoch:157 loss1:-0.526078 loss2:-2.500091 loss3:1.872541 gene_loss:-1.822541\n",
      "epoch:158 loss1:-0.758991 loss2:-2.442647 loss3:1.568196 gene_loss:-1.509055\n",
      "epoch:159 loss1:-0.859857 loss2:-2.522942 loss3:1.523401 gene_loss:-1.645037\n",
      "epoch:160 loss1:-0.912260 loss2:-2.709973 loss3:1.701797 gene_loss:-1.692908\n",
      "epoch:161 loss1:-0.905274 loss2:-2.559788 loss3:1.545338 gene_loss:-1.773777\n",
      "epoch:162 loss1:-0.472565 loss2:-2.505734 loss3:1.915734 gene_loss:-1.665261\n",
      "epoch:163 loss1:-0.746983 loss2:-2.652463 loss3:1.789513 gene_loss:-1.708025\n",
      "epoch:164 loss1:-0.624109 loss2:-2.516198 loss3:1.778717 gene_loss:-1.663113\n",
      "epoch:165 loss1:-0.442401 loss2:-2.436934 loss3:1.848916 gene_loss:-1.773759\n",
      "epoch:166 loss1:-0.622814 loss2:-2.470876 loss3:1.743989 gene_loss:-1.792113\n",
      "epoch:167 loss1:-0.244447 loss2:-2.351736 loss3:1.978216 gene_loss:-1.849656\n",
      "epoch:168 loss1:-0.530528 loss2:-2.574337 loss3:1.946273 gene_loss:-1.747368\n",
      "epoch:169 loss1:-0.489031 loss2:-2.418048 loss3:1.845384 gene_loss:-1.675385\n",
      "epoch:170 loss1:-0.700391 loss2:-2.409739 loss3:1.597104 gene_loss:-1.510497\n",
      "epoch:171 loss1:-0.529773 loss2:-2.195862 loss3:1.552580 gene_loss:-1.786183\n",
      "epoch:172 loss1:-0.690502 loss2:-2.403417 loss3:1.601573 gene_loss:-1.595945\n",
      "epoch:173 loss1:-0.276641 loss2:-2.185321 loss3:1.834819 gene_loss:-1.815766\n",
      "epoch:174 loss1:-0.646348 loss2:-2.493816 loss3:1.761917 gene_loss:-1.921078\n",
      "epoch:175 loss1:-0.707044 loss2:-2.497087 loss3:1.666767 gene_loss:-1.516654\n",
      "epoch:176 loss1:-0.671722 loss2:-2.485608 loss3:1.695995 gene_loss:-1.584180\n",
      "epoch:177 loss1:-0.332979 loss2:-2.299556 loss3:1.866327 gene_loss:-1.544179\n",
      "epoch:178 loss1:-0.711723 loss2:-2.397783 loss3:1.612828 gene_loss:-1.435313\n",
      "epoch:179 loss1:-0.796952 loss2:-2.376000 loss3:1.493619 gene_loss:-1.510508\n",
      "epoch:180 loss1:-0.691143 loss2:-2.549979 loss3:1.764092 gene_loss:-1.355048\n",
      "epoch:181 loss1:-0.865033 loss2:-2.583624 loss3:1.621248 gene_loss:-1.490743\n",
      "epoch:182 loss1:-0.590664 loss2:-2.424581 loss3:1.750317 gene_loss:-1.509220\n",
      "epoch:183 loss1:-0.774407 loss2:-2.349526 loss3:1.499863 gene_loss:-1.557639\n",
      "epoch:184 loss1:-0.394924 loss2:-2.205424 loss3:1.663940 gene_loss:-1.533093\n",
      "epoch:185 loss1:-0.899631 loss2:-2.433175 loss3:1.454415 gene_loss:-1.686054\n",
      "epoch:186 loss1:-0.774640 loss2:-2.490272 loss3:1.624510 gene_loss:-1.463474\n",
      "epoch:187 loss1:-0.530313 loss2:-2.344770 loss3:1.747002 gene_loss:-1.424552\n",
      "epoch:188 loss1:-0.450508 loss2:-2.106971 loss3:1.527191 gene_loss:-1.746830\n",
      "epoch:189 loss1:-0.863986 loss2:-2.322618 loss3:1.320465 gene_loss:-1.595476\n",
      "epoch:190 loss1:-0.622564 loss2:-2.382930 loss3:1.665117 gene_loss:-1.503294\n",
      "epoch:191 loss1:-0.486535 loss2:-2.155522 loss3:1.579491 gene_loss:-1.483360\n",
      "epoch:192 loss1:-0.595681 loss2:-2.290715 loss3:1.597200 gene_loss:-1.433672\n",
      "epoch:193 loss1:-0.794130 loss2:-2.350873 loss3:1.465210 gene_loss:-1.436325\n",
      "epoch:194 loss1:-0.608309 loss2:-2.391594 loss3:1.688841 gene_loss:-1.439236\n",
      "epoch:195 loss1:-0.673614 loss2:-2.276706 loss3:1.512759 gene_loss:-1.680227\n",
      "epoch:196 loss1:-0.405570 loss2:-2.080774 loss3:1.569291 gene_loss:-1.603055\n",
      "epoch:197 loss1:-0.762953 loss2:-2.301186 loss3:1.430093 gene_loss:-1.685035\n",
      "epoch:198 loss1:-0.647491 loss2:-2.182420 loss3:1.436290 gene_loss:-1.505068\n",
      "epoch:199 loss1:-0.665649 loss2:-2.256819 loss3:1.516851 gene_loss:-1.515702\n",
      "epoch:200 loss1:-0.690352 loss2:-2.176287 loss3:1.390044 gene_loss:-1.693081\n",
      "epoch:201 loss1:-0.519873 loss2:-2.159691 loss3:1.507546 gene_loss:-1.594818\n",
      "epoch:202 loss1:-0.635640 loss2:-2.377380 loss3:1.641685 gene_loss:-1.389094\n",
      "epoch:203 loss1:-0.646116 loss2:-2.218232 loss3:1.487487 gene_loss:-1.595624\n",
      "epoch:204 loss1:-0.888795 loss2:-2.366064 loss3:1.412935 gene_loss:-1.575426\n",
      "epoch:205 loss1:-0.458300 loss2:-2.052334 loss3:1.499950 gene_loss:-1.666737\n",
      "epoch:206 loss1:-0.322108 loss2:-2.051867 loss3:1.632062 gene_loss:-1.648247\n",
      "epoch:207 loss1:-0.736035 loss2:-2.241400 loss3:1.421670 gene_loss:-1.478976\n",
      "epoch:208 loss1:-0.405141 loss2:-2.169130 loss3:1.692926 gene_loss:-1.501312\n",
      "epoch:209 loss1:-0.309994 loss2:-2.059499 loss3:1.658723 gene_loss:-1.507438\n",
      "epoch:210 loss1:-0.540920 loss2:-2.162928 loss3:1.530630 gene_loss:-1.618093\n",
      "epoch:211 loss1:-0.491885 loss2:-2.264561 loss3:1.693330 gene_loss:-1.445002\n",
      "epoch:212 loss1:-0.346641 loss2:-2.093032 loss3:1.622209 gene_loss:-1.523387\n",
      "epoch:213 loss1:-0.640144 loss2:-2.268662 loss3:1.535158 gene_loss:-1.764462\n",
      "epoch:214 loss1:-0.594609 loss2:-2.284142 loss3:1.597203 gene_loss:-1.823541\n",
      "epoch:215 loss1:-0.774746 loss2:-2.247243 loss3:1.424948 gene_loss:-1.685948\n",
      "epoch:216 loss1:-0.524154 loss2:-2.172055 loss3:1.554256 gene_loss:-1.603952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:217 loss1:-0.586190 loss2:-2.335407 loss3:1.670833 gene_loss:-1.827936\n",
      "epoch:218 loss1:-0.496880 loss2:-2.418803 loss3:1.845267 gene_loss:-1.424481\n",
      "epoch:219 loss1:-0.265224 loss2:-2.128615 loss3:1.719895 gene_loss:-1.750326\n",
      "epoch:220 loss1:-0.684143 loss2:-2.318016 loss3:1.569457 gene_loss:-1.518668\n",
      "epoch:221 loss1:-0.475271 loss2:-2.177556 loss3:1.610733 gene_loss:-1.527569\n",
      "epoch:222 loss1:-0.771944 loss2:-2.629191 loss3:1.745199 gene_loss:-1.765594\n",
      "epoch:223 loss1:-0.566519 loss2:-2.419311 loss3:1.746444 gene_loss:-1.854061\n",
      "epoch:224 loss1:-0.651513 loss2:-2.424029 loss3:1.678551 gene_loss:-1.643559\n",
      "epoch:225 loss1:-0.484539 loss2:-2.185075 loss3:1.627060 gene_loss:-1.810245\n",
      "epoch:226 loss1:-0.483288 loss2:-2.194090 loss3:1.642652 gene_loss:-1.611262\n",
      "epoch:227 loss1:-0.225734 loss2:-2.113591 loss3:1.797546 gene_loss:-1.741923\n",
      "epoch:228 loss1:-0.548897 loss2:-2.240930 loss3:1.606191 gene_loss:-1.740503\n",
      "epoch:229 loss1:-0.523214 loss2:-2.198243 loss3:1.583840 gene_loss:-1.347198\n",
      "epoch:230 loss1:-0.433250 loss2:-2.209028 loss3:1.686745 gene_loss:-1.729007\n",
      "epoch:231 loss1:-0.235125 loss2:-2.121266 loss3:1.787948 gene_loss:-1.466064\n",
      "epoch:232 loss1:-0.819757 loss2:-2.302276 loss3:1.410179 gene_loss:-1.611450\n",
      "epoch:233 loss1:-0.361072 loss2:-1.978869 loss3:1.541208 gene_loss:-1.537905\n",
      "epoch:234 loss1:-0.470360 loss2:-2.198754 loss3:1.639082 gene_loss:-1.645245\n",
      "epoch:235 loss1:-0.717470 loss2:-2.204617 loss3:1.390703 gene_loss:-1.557303\n",
      "epoch:236 loss1:-0.437968 loss2:-2.225728 loss3:1.694325 gene_loss:-1.787373\n",
      "epoch:237 loss1:-0.164130 loss2:-2.146321 loss3:1.870191 gene_loss:-1.612041\n",
      "epoch:238 loss1:-0.256500 loss2:-1.976109 loss3:1.646020 gene_loss:-1.892505\n",
      "epoch:239 loss1:-0.673820 loss2:-2.332638 loss3:1.574326 gene_loss:-1.589215\n",
      "epoch:240 loss1:-0.651194 loss2:-2.214511 loss3:1.509561 gene_loss:-1.756677\n",
      "epoch:241 loss1:-0.332540 loss2:-2.235340 loss3:1.825213 gene_loss:-1.641594\n",
      "epoch:242 loss1:-0.590972 loss2:-2.235217 loss3:1.553591 gene_loss:-1.840370\n",
      "epoch:243 loss1:-0.447900 loss2:-2.436157 loss3:1.899621 gene_loss:-1.597550\n",
      "epoch:244 loss1:-0.297832 loss2:-2.169706 loss3:1.791811 gene_loss:-1.786643\n",
      "epoch:245 loss1:-0.596476 loss2:-2.345580 loss3:1.666477 gene_loss:-1.541510\n",
      "epoch:246 loss1:0.022271 loss2:-1.936004 loss3:1.862808 gene_loss:-1.769614\n",
      "epoch:247 loss1:-0.223347 loss2:-2.095151 loss3:1.755262 gene_loss:-1.426432\n",
      "epoch:248 loss1:-0.460795 loss2:-2.095172 loss3:1.538240 gene_loss:-1.542375\n",
      "epoch:249 loss1:-0.060411 loss2:-2.008936 loss3:1.834411 gene_loss:-1.711711\n",
      "epoch:250 loss1:-0.276475 loss2:-2.048268 loss3:1.698006 gene_loss:-1.712577\n",
      "epoch:251 loss1:-0.280820 loss2:-2.125080 loss3:1.746854 gene_loss:-1.274226\n",
      "epoch:252 loss1:-0.620354 loss2:-2.141732 loss3:1.440043 gene_loss:-1.417983\n",
      "epoch:253 loss1:-0.513650 loss2:-2.099315 loss3:1.512849 gene_loss:-1.629693\n",
      "epoch:254 loss1:-0.428731 loss2:-1.943639 loss3:1.433606 gene_loss:-1.744379\n",
      "epoch:255 loss1:-0.326621 loss2:-1.982405 loss3:1.570670 gene_loss:-1.322772\n",
      "epoch:256 loss1:-0.561079 loss2:-2.141231 loss3:1.501410 gene_loss:-1.657206\n",
      "epoch:257 loss1:-0.536024 loss2:-2.003820 loss3:1.384431 gene_loss:-1.553178\n",
      "epoch:258 loss1:-0.310064 loss2:-1.902270 loss3:1.484802 gene_loss:-1.385252\n",
      "epoch:259 loss1:-0.145598 loss2:-1.753906 loss3:1.514529 gene_loss:-1.430952\n",
      "epoch:260 loss1:-0.234956 loss2:-1.939322 loss3:1.598465 gene_loss:-1.419536\n",
      "epoch:261 loss1:-0.534555 loss2:-2.047095 loss3:1.404511 gene_loss:-1.382032\n",
      "epoch:262 loss1:-0.672527 loss2:-1.924117 loss3:1.165910 gene_loss:-1.254023\n",
      "epoch:263 loss1:-0.361194 loss2:-1.963680 loss3:1.524478 gene_loss:-1.280613\n",
      "epoch:264 loss1:-0.515065 loss2:-1.869589 loss3:1.241114 gene_loss:-1.561289\n",
      "epoch:265 loss1:-0.262242 loss2:-1.897403 loss3:1.532192 gene_loss:-1.377526\n",
      "epoch:266 loss1:-1.127279 loss2:-2.412081 loss3:1.129171 gene_loss:-1.491630\n",
      "epoch:267 loss1:-0.583288 loss2:-2.020121 loss3:1.359641 gene_loss:-1.232836\n",
      "epoch:268 loss1:-0.403338 loss2:-2.055379 loss3:1.556681 gene_loss:-1.334435\n",
      "epoch:269 loss1:-0.436229 loss2:-2.009503 loss3:1.498899 gene_loss:-1.364721\n",
      "epoch:270 loss1:-0.724073 loss2:-2.113894 loss3:1.305764 gene_loss:-1.282450\n",
      "epoch:271 loss1:-0.408835 loss2:-1.641216 loss3:1.164804 gene_loss:-1.434810\n",
      "epoch:272 loss1:-0.671500 loss2:-2.054052 loss3:1.270681 gene_loss:-1.297315\n",
      "epoch:273 loss1:-0.643921 loss2:-2.063581 loss3:1.309565 gene_loss:-1.221236\n",
      "epoch:274 loss1:-0.311406 loss2:-1.819340 loss3:1.387823 gene_loss:-1.244814\n",
      "epoch:275 loss1:-0.501056 loss2:-1.914350 loss3:1.333289 gene_loss:-1.602861\n",
      "epoch:276 loss1:-0.360929 loss2:-1.982400 loss3:1.545067 gene_loss:-1.408488\n",
      "epoch:277 loss1:-0.393078 loss2:-1.876251 loss3:1.397590 gene_loss:-1.499155\n",
      "epoch:278 loss1:-0.404620 loss2:-1.841230 loss3:1.341581 gene_loss:-1.140315\n",
      "epoch:279 loss1:-0.613386 loss2:-1.915644 loss3:1.223819 gene_loss:-1.323707\n",
      "epoch:280 loss1:-0.152144 loss2:-1.656043 loss3:1.447932 gene_loss:-1.437844\n",
      "epoch:281 loss1:-0.516210 loss2:-1.814339 loss3:1.208385 gene_loss:-1.352465\n",
      "epoch:282 loss1:-0.680529 loss2:-2.096231 loss3:1.327803 gene_loss:-1.596900\n",
      "epoch:283 loss1:-0.135633 loss2:-1.730530 loss3:1.483741 gene_loss:-1.158419\n",
      "epoch:284 loss1:-0.496298 loss2:-1.785944 loss3:1.204887 gene_loss:-1.494908\n",
      "epoch:285 loss1:-0.289244 loss2:-1.745256 loss3:1.354446 gene_loss:-1.340011\n",
      "epoch:286 loss1:-0.286609 loss2:-1.811522 loss3:1.459052 gene_loss:-1.291894\n",
      "epoch:287 loss1:-0.569282 loss2:-1.893139 loss3:1.242415 gene_loss:-0.959155\n",
      "epoch:288 loss1:-0.649265 loss2:-1.861065 loss3:1.127911 gene_loss:-1.378268\n",
      "epoch:289 loss1:-0.385040 loss2:-1.894696 loss3:1.424442 gene_loss:-1.448314\n",
      "epoch:290 loss1:-0.574585 loss2:-1.902812 loss3:1.215333 gene_loss:-1.356876\n",
      "epoch:291 loss1:-0.445078 loss2:-1.860032 loss3:1.303296 gene_loss:-0.977889\n",
      "epoch:292 loss1:-0.462274 loss2:-1.773018 loss3:1.203494 gene_loss:-1.550101\n",
      "epoch:293 loss1:-0.534690 loss2:-1.922051 loss3:1.310843 gene_loss:-1.348470\n",
      "epoch:294 loss1:-0.439022 loss2:-1.682158 loss3:1.126261 gene_loss:-1.373393\n",
      "epoch:295 loss1:-0.355076 loss2:-1.792815 loss3:1.372501 gene_loss:-1.391515\n",
      "epoch:296 loss1:-0.297611 loss2:-1.654415 loss3:1.275697 gene_loss:-1.310305\n",
      "epoch:297 loss1:-0.403768 loss2:-1.749814 loss3:1.261125 gene_loss:-1.607383\n",
      "epoch:298 loss1:-0.282627 loss2:-1.885093 loss3:1.468229 gene_loss:-1.226346\n",
      "epoch:299 loss1:-0.737626 loss2:-2.133087 loss3:1.314933 gene_loss:-1.266092\n",
      "epoch:300 loss1:-0.788901 loss2:-1.747200 loss3:0.860049 gene_loss:-1.328162\n",
      "epoch:301 loss1:-0.468835 loss2:-1.672818 loss3:1.125934 gene_loss:-0.971098\n",
      "epoch:302 loss1:-0.268964 loss2:-1.455676 loss3:1.079806 gene_loss:-1.413027\n",
      "epoch:303 loss1:-0.076171 loss2:-1.334525 loss3:1.160132 gene_loss:-1.314842\n",
      "epoch:304 loss1:-0.155639 loss2:-1.346326 loss3:1.088803 gene_loss:-1.080941\n",
      "epoch:305 loss1:-0.342767 loss2:-1.546681 loss3:1.141230 gene_loss:-0.918543\n",
      "epoch:306 loss1:-0.624655 loss2:-1.716764 loss3:0.983958 gene_loss:-0.987014\n",
      "epoch:307 loss1:-0.568561 loss2:-1.606124 loss3:0.935115 gene_loss:-1.264630\n",
      "epoch:308 loss1:-0.433754 loss2:-1.606405 loss3:1.064290 gene_loss:-0.876785\n",
      "epoch:309 loss1:-0.255380 loss2:-1.348047 loss3:1.014054 gene_loss:-1.087409\n",
      "epoch:310 loss1:-0.306304 loss2:-1.360307 loss3:0.946675 gene_loss:-0.899324\n",
      "epoch:311 loss1:-0.397333 loss2:-1.395404 loss3:0.922439 gene_loss:-0.786563\n",
      "epoch:312 loss1:-0.646647 loss2:-1.278084 loss3:0.569438 gene_loss:-0.761008\n",
      "epoch:313 loss1:-0.531858 loss2:-1.454277 loss3:0.818974 gene_loss:-0.881693\n",
      "epoch:314 loss1:-0.531500 loss2:-1.420086 loss3:0.802311 gene_loss:-0.820679\n",
      "epoch:315 loss1:-0.690568 loss2:-1.450351 loss3:0.678218 gene_loss:-0.862810\n",
      "epoch:316 loss1:0.011250 loss2:-1.228197 loss3:1.132535 gene_loss:-0.691169\n",
      "epoch:317 loss1:-0.496016 loss2:-1.697693 loss3:1.116772 gene_loss:-1.019844\n",
      "epoch:318 loss1:-0.390096 loss2:-1.515129 loss3:1.071093 gene_loss:-1.065845\n",
      "epoch:319 loss1:-0.424084 loss2:-1.560375 loss3:1.042833 gene_loss:-1.006141\n",
      "epoch:320 loss1:-0.439605 loss2:-1.691464 loss3:1.182040 gene_loss:-0.551480\n",
      "epoch:321 loss1:-0.333473 loss2:-1.331506 loss3:0.930849 gene_loss:-0.889860\n",
      "epoch:322 loss1:-0.550250 loss2:-1.421563 loss3:0.798276 gene_loss:-1.031491\n",
      "epoch:323 loss1:-0.636628 loss2:-1.548268 loss3:0.808094 gene_loss:-0.775681\n",
      "epoch:324 loss1:-0.194401 loss2:-1.432140 loss3:1.155972 gene_loss:-0.662219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:325 loss1:-0.655019 loss2:-1.564733 loss3:0.818609 gene_loss:-0.952062\n",
      "epoch:326 loss1:-0.680330 loss2:-1.523149 loss3:0.762497 gene_loss:-0.900165\n",
      "epoch:327 loss1:-0.339719 loss2:-1.301048 loss3:0.906997 gene_loss:-0.865651\n",
      "epoch:328 loss1:-0.425830 loss2:-1.483921 loss3:0.956662 gene_loss:-0.741486\n",
      "epoch:329 loss1:-0.440413 loss2:-1.366150 loss3:0.865341 gene_loss:-0.663328\n",
      "epoch:330 loss1:-0.263042 loss2:-1.554723 loss3:1.215436 gene_loss:-1.060125\n",
      "epoch:331 loss1:-0.378749 loss2:-1.317854 loss3:0.855564 gene_loss:-0.794322\n",
      "epoch:332 loss1:-0.590336 loss2:-1.608405 loss3:0.937188 gene_loss:-0.885289\n",
      "epoch:333 loss1:-0.134496 loss2:-1.380948 loss3:1.169882 gene_loss:-0.873563\n",
      "epoch:334 loss1:-0.253325 loss2:-1.468646 loss3:1.159288 gene_loss:-1.004343\n",
      "epoch:335 loss1:-0.239514 loss2:-1.513420 loss3:1.168465 gene_loss:-0.847221\n",
      "epoch:336 loss1:-0.106673 loss2:-1.233619 loss3:1.061517 gene_loss:-0.919282\n",
      "epoch:337 loss1:-0.116162 loss2:-1.305399 loss3:1.117147 gene_loss:-0.875769\n",
      "epoch:338 loss1:-0.534879 loss2:-1.461881 loss3:0.836128 gene_loss:-0.996194\n",
      "epoch:339 loss1:-0.363500 loss2:-1.509428 loss3:1.069040 gene_loss:-1.124682\n",
      "epoch:340 loss1:-0.408934 loss2:-1.531024 loss3:1.039994 gene_loss:-0.901302\n",
      "epoch:341 loss1:-0.261665 loss2:-1.232069 loss3:0.886204 gene_loss:-1.054093\n",
      "epoch:342 loss1:-0.396500 loss2:-1.460476 loss3:0.985422 gene_loss:-0.983865\n",
      "epoch:343 loss1:-0.364068 loss2:-1.498286 loss3:1.053759 gene_loss:-0.977139\n",
      "epoch:344 loss1:-0.342617 loss2:-1.491103 loss3:1.091474 gene_loss:-1.170539\n",
      "epoch:345 loss1:-0.568479 loss2:-1.459886 loss3:0.807210 gene_loss:-0.937156\n",
      "epoch:346 loss1:-0.671827 loss2:-1.363714 loss3:0.622764 gene_loss:-0.723313\n",
      "epoch:347 loss1:-0.471452 loss2:-1.497183 loss3:0.949641 gene_loss:-0.871388\n",
      "epoch:348 loss1:-0.204654 loss2:-1.158370 loss3:0.852364 gene_loss:-1.251997\n",
      "epoch:349 loss1:-0.177680 loss2:-1.395541 loss3:1.133844 gene_loss:-0.945958\n",
      "epoch:350 loss1:-0.078620 loss2:-1.339643 loss3:1.154936 gene_loss:-1.223181\n",
      "epoch:351 loss1:-0.553382 loss2:-1.608337 loss3:0.971647 gene_loss:-1.002469\n",
      "epoch:352 loss1:-0.481611 loss2:-1.346413 loss3:0.794346 gene_loss:-0.899567\n",
      "epoch:353 loss1:-0.405577 loss2:-1.224077 loss3:0.729102 gene_loss:-0.929494\n",
      "epoch:354 loss1:-0.713135 loss2:-1.531753 loss3:0.714066 gene_loss:-1.048252\n",
      "epoch:355 loss1:-0.377110 loss2:-1.487542 loss3:1.015896 gene_loss:-0.888238\n",
      "epoch:356 loss1:-0.360239 loss2:-1.531153 loss3:1.090747 gene_loss:-1.127460\n",
      "epoch:357 loss1:-0.264545 loss2:-1.190909 loss3:0.855406 gene_loss:-0.826936\n",
      "epoch:358 loss1:-0.199099 loss2:-1.349438 loss3:1.071033 gene_loss:-0.910875\n",
      "epoch:359 loss1:-0.193726 loss2:-1.299096 loss3:1.025196 gene_loss:-0.943553\n",
      "epoch:360 loss1:-0.292565 loss2:-1.542903 loss3:1.161314 gene_loss:-0.950909\n",
      "epoch:361 loss1:-0.225086 loss2:-1.397193 loss3:1.064349 gene_loss:-0.975384\n",
      "epoch:362 loss1:-0.488493 loss2:-1.374980 loss3:0.813976 gene_loss:-0.940144\n",
      "epoch:363 loss1:-0.257672 loss2:-1.390212 loss3:1.060144 gene_loss:-1.028465\n",
      "epoch:364 loss1:-0.317514 loss2:-1.540988 loss3:1.156862 gene_loss:-0.999914\n",
      "epoch:365 loss1:-0.529499 loss2:-1.403444 loss3:0.787367 gene_loss:-0.933468\n",
      "epoch:366 loss1:-0.270956 loss2:-1.233220 loss3:0.880780 gene_loss:-0.855762\n",
      "epoch:367 loss1:-0.429089 loss2:-1.322106 loss3:0.808472 gene_loss:-0.721712\n",
      "epoch:368 loss1:-0.339010 loss2:-1.264096 loss3:0.827446 gene_loss:-0.782596\n",
      "epoch:369 loss1:-0.324000 loss2:-1.325046 loss3:0.911268 gene_loss:-0.894195\n",
      "epoch:370 loss1:-0.351620 loss2:-1.281543 loss3:0.868472 gene_loss:-0.727788\n",
      "epoch:371 loss1:0.311672 loss2:-0.882871 loss3:1.107882 gene_loss:-0.667736\n",
      "epoch:372 loss1:-0.460761 loss2:-1.191477 loss3:0.650601 gene_loss:-0.461021\n",
      "epoch:373 loss1:-0.058052 loss2:-1.038303 loss3:0.865392 gene_loss:-0.775426\n",
      "epoch:374 loss1:-0.352419 loss2:-1.215490 loss3:0.774450 gene_loss:-0.425266\n",
      "epoch:375 loss1:-0.482873 loss2:-1.041215 loss3:0.487917 gene_loss:-0.620686\n",
      "epoch:376 loss1:-0.184782 loss2:-1.177325 loss3:0.906844 gene_loss:-0.658670\n",
      "epoch:377 loss1:-0.154623 loss2:-0.932296 loss3:0.690819 gene_loss:-0.527830\n",
      "epoch:378 loss1:-0.464775 loss2:-0.928377 loss3:0.383072 gene_loss:-0.645778\n",
      "epoch:379 loss1:-0.407567 loss2:-1.261916 loss3:0.751303 gene_loss:-0.713242\n",
      "epoch:380 loss1:-0.366533 loss2:-0.951375 loss3:0.506419 gene_loss:-0.454231\n",
      "epoch:381 loss1:-0.512534 loss2:-1.010101 loss3:0.420827 gene_loss:-0.488529\n",
      "epoch:382 loss1:-0.360984 loss2:-0.933380 loss3:0.452179 gene_loss:-0.547968\n",
      "epoch:383 loss1:-0.323820 loss2:-0.861034 loss3:0.415797 gene_loss:-0.413416\n",
      "epoch:384 loss1:-0.431311 loss2:-0.998288 loss3:0.486974 gene_loss:-0.566941\n",
      "epoch:385 loss1:-0.181859 loss2:-0.873013 loss3:0.599140 gene_loss:-0.165851\n",
      "epoch:386 loss1:-0.445819 loss2:-0.983998 loss3:0.458417 gene_loss:-0.869827\n",
      "epoch:387 loss1:-0.221184 loss2:-1.047137 loss3:0.757162 gene_loss:-0.682225\n",
      "epoch:388 loss1:-0.259670 loss2:-0.965732 loss3:0.633037 gene_loss:-0.321527\n",
      "epoch:389 loss1:-0.466378 loss2:-0.952054 loss3:0.413008 gene_loss:-0.420990\n",
      "epoch:390 loss1:-0.100762 loss2:-1.027003 loss3:0.839344 gene_loss:-0.899740\n",
      "epoch:391 loss1:-0.503060 loss2:-1.432752 loss3:0.880690 gene_loss:-0.647184\n",
      "epoch:392 loss1:-0.308079 loss2:-1.135435 loss3:0.763671 gene_loss:-0.633492\n",
      "epoch:393 loss1:-0.439770 loss2:-1.020929 loss3:0.514263 gene_loss:-0.570185\n",
      "epoch:394 loss1:-0.106605 loss2:-0.925739 loss3:0.728150 gene_loss:-0.878603\n",
      "epoch:395 loss1:-0.495520 loss2:-1.197328 loss3:0.629788 gene_loss:-0.770251\n",
      "epoch:396 loss1:-0.308723 loss2:-1.113857 loss3:0.724126 gene_loss:-0.698832\n",
      "epoch:397 loss1:-0.446434 loss2:-1.203694 loss3:0.686413 gene_loss:-0.464709\n",
      "epoch:398 loss1:-0.449029 loss2:-1.253950 loss3:0.728964 gene_loss:-0.780197\n",
      "epoch:399 loss1:-0.261061 loss2:-0.978485 loss3:0.638838 gene_loss:-0.591029\n",
      "epoch:400 loss1:-0.405273 loss2:-1.123242 loss3:0.626656 gene_loss:-0.771289\n",
      "epoch:401 loss1:-0.342916 loss2:-1.167088 loss3:0.770365 gene_loss:-0.610938\n",
      "epoch:402 loss1:-0.568032 loss2:-1.012981 loss3:0.345913 gene_loss:-0.904804\n",
      "epoch:403 loss1:-0.610324 loss2:-1.132175 loss3:0.424771 gene_loss:-0.869650\n",
      "epoch:404 loss1:-0.216936 loss2:-1.146129 loss3:0.855043 gene_loss:-0.638364\n",
      "epoch:405 loss1:-0.425211 loss2:-1.133580 loss3:0.652858 gene_loss:-0.921377\n",
      "epoch:406 loss1:-0.666608 loss2:-1.495668 loss3:0.768028 gene_loss:-0.716607\n",
      "epoch:407 loss1:-0.585903 loss2:-1.299127 loss3:0.622499 gene_loss:-0.589110\n",
      "epoch:408 loss1:-0.807554 loss2:-1.291871 loss3:0.393350 gene_loss:-0.778338\n",
      "epoch:409 loss1:-0.134125 loss2:-1.188367 loss3:0.961600 gene_loss:-0.787967\n",
      "epoch:410 loss1:-0.555924 loss2:-1.389420 loss3:0.740758 gene_loss:-0.845896\n",
      "epoch:411 loss1:-0.524962 loss2:-1.359504 loss3:0.738429 gene_loss:-0.543974\n",
      "epoch:412 loss1:-0.312035 loss2:-1.506578 loss3:1.091056 gene_loss:-0.806970\n",
      "epoch:413 loss1:-0.723689 loss2:-1.533213 loss3:0.725722 gene_loss:-0.986132\n",
      "epoch:414 loss1:-0.556882 loss2:-1.748641 loss3:1.119194 gene_loss:-0.661224\n",
      "epoch:415 loss1:-0.501385 loss2:-1.466188 loss3:0.901805 gene_loss:-0.871846\n",
      "epoch:416 loss1:-0.267533 loss2:-1.214198 loss3:0.852671 gene_loss:-1.021846\n",
      "epoch:417 loss1:0.127166 loss2:-1.212366 loss3:1.259627 gene_loss:-0.941228\n",
      "epoch:418 loss1:-0.188409 loss2:-1.389885 loss3:1.119268 gene_loss:-1.155028\n",
      "epoch:419 loss1:-0.387032 loss2:-1.377254 loss3:0.898193 gene_loss:-0.714175\n",
      "epoch:420 loss1:0.350488 loss2:-1.081291 loss3:1.320969 gene_loss:-0.709284\n",
      "epoch:421 loss1:-0.311017 loss2:-1.431602 loss3:1.040953 gene_loss:-0.833472\n",
      "epoch:422 loss1:-0.447586 loss2:-1.437959 loss3:0.902123 gene_loss:-1.174135\n",
      "epoch:423 loss1:0.022161 loss2:-1.027457 loss3:0.973696 gene_loss:-0.793032\n",
      "epoch:424 loss1:0.066488 loss2:-1.202931 loss3:1.181589 gene_loss:-0.892037\n",
      "epoch:425 loss1:-0.494602 loss2:-1.219128 loss3:0.637236 gene_loss:-0.641797\n",
      "epoch:426 loss1:-0.380158 loss2:-1.167768 loss3:0.720436 gene_loss:-0.688494\n",
      "epoch:427 loss1:-0.511780 loss2:-1.401689 loss3:0.811632 gene_loss:-1.006356\n",
      "epoch:428 loss1:-0.304049 loss2:-1.386832 loss3:1.013281 gene_loss:-0.578142\n",
      "epoch:429 loss1:-0.058707 loss2:-0.930936 loss3:0.797659 gene_loss:-0.708878\n",
      "epoch:430 loss1:-0.293284 loss2:-1.292432 loss3:0.899798 gene_loss:-0.934965\n",
      "epoch:431 loss1:-0.850190 loss2:-1.453750 loss3:0.505561 gene_loss:-0.924445\n",
      "epoch:432 loss1:-0.640513 loss2:-1.664101 loss3:0.943687 gene_loss:-0.959935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:433 loss1:-0.051816 loss2:-1.284635 loss3:1.156796 gene_loss:-0.896121\n",
      "epoch:434 loss1:-0.018140 loss2:-1.082346 loss3:0.989344 gene_loss:-1.090502\n",
      "epoch:435 loss1:-0.493651 loss2:-1.316654 loss3:0.744625 gene_loss:-0.697435\n",
      "epoch:436 loss1:-0.151527 loss2:-1.390743 loss3:1.152669 gene_loss:-1.134798\n",
      "epoch:437 loss1:-0.252956 loss2:-1.482050 loss3:1.144675 gene_loss:-0.856793\n",
      "epoch:438 loss1:-0.698827 loss2:-1.434754 loss3:0.648531 gene_loss:-1.004266\n",
      "epoch:439 loss1:-0.598589 loss2:-1.394489 loss3:0.718521 gene_loss:-0.785310\n",
      "epoch:440 loss1:-0.644401 loss2:-1.471140 loss3:0.743678 gene_loss:-0.802384\n",
      "epoch:441 loss1:-0.397260 loss2:-1.285341 loss3:0.814028 gene_loss:-0.791824\n",
      "epoch:442 loss1:-0.300035 loss2:-1.254812 loss3:0.888877 gene_loss:-0.569678\n",
      "epoch:443 loss1:-0.427174 loss2:-1.424453 loss3:0.864547 gene_loss:-0.853418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c4f91d76817d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#训练判别器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreal_image\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreal_labels\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfake_labels\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#取消对权重参数的裁剪 clip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "real_labels = -np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.ones(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "dummy = np.zeros(shape=(BATCH_SIZE , 1)) #为WGAN-GP的惩罚项准备的label\n",
    "\n",
    "for i in range(10000):\n",
    "    #============================\n",
    "    #训练一次G就要训练N_CRITIC次D（Discriminator）\n",
    "    for _ in range(N_CRITIC):\n",
    "        \n",
    "        noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "\n",
    "        #real_image = load_image()\n",
    "        real_image = load_mnist()\n",
    "        \n",
    "        #训练判别器\n",
    "        loss = critic_model.train_on_batch([real_image , noise] , [real_labels , fake_labels , dummy])\n",
    "        \n",
    "        #取消对权重参数的裁剪 clip\n",
    "        #进行截断后 生成了怪异的图像\n",
    "        #for layer in critic_i.layers:\n",
    "        #    weights = layer.get_weights()\n",
    "        #    weights = [np.clip(w , - CLIP_VALUE , CLIP_VALUE) for w in weights]\n",
    "        #    layer.set_weights(weights)\n",
    "    #===#=========================\n",
    "    \n",
    "    #训练生成器\n",
    "    noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    generator_loss = generator_model.train_on_batch(noise2 , real_labels)\n",
    "\n",
    "    print('epoch:%d loss1:%f loss2:%f loss3:%f gene_loss:%f' % (i , loss[0] , loss[1] , loss[2] , generator_loss))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        #write_image(i)\n",
    "        write_image_mnist(i)\n",
    "    \n",
    "#write_image(999)\n",
    "write_image_mnist(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeli = Sequential()\n",
    "\n",
    "modeli.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "modeli.add(Reshape((7, 7, 128)))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(Activation(\"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modeli.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
