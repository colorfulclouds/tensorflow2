{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU , PReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "#addin cycleGAN 使用instance-norm\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "\n",
    "from keras.initializers import truncated_normal , constant , random_normal\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "#残差块使用\n",
    "from keras.layers import Add\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#导入存在的模型\n",
    "from keras.applications import VGG16 , VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "CHANNEL = 3\n",
    "\n",
    "SHAPE = (WIDTH , HEIGHT , CHANNEL)\n",
    "\n",
    "BATCH_SIZE = 4 #crazy!!! slow turtle\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = '../dataset/vangogh2photo/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 2 #几行决定显示几个测试样例 显示2个\n",
    "COL = 3 #3列是因为要显示 原图像 另一个特征空间的图像 还原后的图像\n",
    "\n",
    "TRAIN_APPLE_PATH = glob(PATH + 'trainA/*')\n",
    "TRAIN_ORANGE_PATH = glob(PATH + 'trainB/*')\n",
    "TEST_APPLE_PATH = glob(PATH + 'testA/*')\n",
    "TEST_ORANGE_PATH = glob(PATH + 'testB/*')\n",
    "\n",
    "#卷积使用 基卷积核大小\n",
    "G_filters = 32\n",
    "D_filters = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch = int(HEIGHT/(2**4)) #16\n",
    "disc_patch = (patch , patch , 1) #16*16*1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(batch_size = BATCH_SIZE , training = True):\n",
    "    #随机在图片库中挑选\n",
    "    if training:\n",
    "        APPLE_PATH = TRAIN_APPLE_PATH\n",
    "        ORANGE_PATH = TRAIN_ORANGE_PATH\n",
    "    else:\n",
    "        APPLE_PATH = TEST_APPLE_PATH\n",
    "        ORANGE_PATH = TEST_ORANGE_PATH\n",
    "        \n",
    "    images_apple = np.random.choice(APPLE_PATH , size=batch_size)\n",
    "    images_orange = np.random.choice(ORANGE_PATH , size=batch_size)\n",
    "    \n",
    "    apples = []\n",
    "    oranges = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        apple = scipy.misc.imread(images_apple[i] , mode='RGB').astype(np.float)\n",
    "        orange = scipy.misc.imread(images_orange[i] , mode='RGB').astype(np.float)\n",
    "        \n",
    "        #随机性地对训练样本进行 左右反转\n",
    "        if training and np.random.random()<0.5:\n",
    "            apple = np.fliplr(apple)\n",
    "            orange = np.fliplr(orange)\n",
    "        \n",
    "        apples.append(apple)\n",
    "        oranges.append(orange)\n",
    "        \n",
    "    apples = np.array(apples)/127.5 - 1\n",
    "    oranges = np.array(oranges)/127.5 - 1\n",
    "    \n",
    "    return apples , oranges\n",
    "\n",
    "\n",
    "def write_image(epoch):\n",
    "    #生成高分图像时 进行对比显示\n",
    "    apples , oranges = load_image(batch_size=1 , training=False) #1个batch就是两幅图像 一个苹果的 一个橘子的\n",
    "    \n",
    "    fake_apples = generator_apple2orange.predict(apples) #橘子风格的苹果\n",
    "    fake_oranges = generator_orange2apple.predict(oranges) #苹果风格的橘子\n",
    "    \n",
    "    apples_hat = generator_orange2apple.predict(fake_apples) #还原后的苹果\n",
    "    oranges_hat = generator_apple2orange.predict(fake_oranges) #还原后的橘子\n",
    "    \n",
    "    \n",
    "    apples = apples*0.5+0.5\n",
    "    oranges = oranges*0.5+0.5\n",
    "    \n",
    "    fake_apples = fake_apples*0.5+0.5\n",
    "    fake_oranges = fake_oranges*0.5+0.5\n",
    "    \n",
    "    apples_hat = apples_hat*0.5+0.5\n",
    "    oranges_hat = oranges_hat*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    count=0\n",
    "    \n",
    "    axes[0][0].imshow(apples[0])\n",
    "    axes[0][0].set_title('apple')\n",
    "    axes[0][0].axis('off')\n",
    "\n",
    "    axes[0][1].imshow(fake_apples[0])\n",
    "    axes[0][1].set_title('apple-orange')\n",
    "    axes[0][1].axis('off')\n",
    "    \n",
    "    axes[0][2].imshow(apples_hat[0])\n",
    "    axes[0][2].set_title('restruct apple')\n",
    "    axes[0][2].axis('off')\n",
    "\n",
    "    axes[1][0].imshow(oranges[0])\n",
    "    axes[1][0].set_title('orange')\n",
    "    axes[1][0].axis('off')\n",
    "\n",
    "    axes[1][1].imshow(fake_oranges[0])\n",
    "    axes[1][1].set_title('orange-apple')\n",
    "    axes[1][1].axis('off')\n",
    "    \n",
    "    axes[1][2].imshow(oranges_hat[0])\n",
    "    axes[1][2].set_title('restruct orange')\n",
    "    axes[1][2].axis('off')\n",
    "      \n",
    "    fig.savefig('apple2orange_cyclegan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input_data , output_size , filter_size=4 , instance_norm=True):\n",
    "    h = Conv2D(output_size , filter_size , strides=(2,2) , padding='same')(input_data)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    \n",
    "    if instance_norm:\n",
    "        h = InstanceNormalization()(h)\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "#实现U-Net使用 需要网络的跳连接\n",
    "def deconv2d(input_data , skip_input , output_size , filter_size=4 , dropout_rate=0.0):\n",
    "    h = UpSampling2D(size=2)(input_data)\n",
    "    h = Conv2D(output_size , filter_size , strides=(1,1) , padding='same')(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "    if dropout_rate:\n",
    "        h = Dropout(rate=dropout_rate)(h)\n",
    "    \n",
    "    h = InstanceNormalization()(h)\n",
    "    h =  Concatenate()([h , skip_input]) #跳连接具体实现\n",
    "\n",
    "    return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#G使用encoder-decoder结构 但是需要引入跳连接 即U-Net\n",
    "def generator(G_filters , name):\n",
    "    style = Input(shape=SHAPE) #输入一个风格的图像 生成另一个风格的图像\n",
    "    \n",
    "    #encoder\n",
    "    d1 = conv2d(style , G_filters)\n",
    "    d2 = conv2d(d1 , G_filters*2)\n",
    "    d3 = conv2d(d2 , G_filters*4)\n",
    "    d4 = conv2d(d3 , G_filters*8)\n",
    "\n",
    "    #decoder\n",
    "    u1 = deconv2d(d4 , d3 , G_filters*4)\n",
    "    u2 = deconv2d(u1 , d2 , G_filters*2)\n",
    "    u3 = deconv2d(u2 , d1 , G_filters)\n",
    "    \n",
    "    u4 = UpSampling2D(size=(2,2))(u3)\n",
    "    other_style = Conv2D(filters=CHANNEL , kernel_size=(4,4) , strides=(1,1) , padding='same' , activation='tanh')(u4) #还原后的图像\n",
    "    \n",
    "    return Model(style , other_style , name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(D_filters , name):\n",
    "    style = Input(shape=SHAPE) #风格1 的图像\n",
    "    #style2 = Input(shape=SHAPE) #风格2 的图像\n",
    "    \n",
    "    #style = Concatenate()([style1 , style2])\n",
    "    \n",
    "    h1 = conv2d(style , output_size=D_filters , instance_norm=False)\n",
    "    h2 = conv2d(h1 , output_size=D_filters*2)\n",
    "    h3 = conv2d(h2 , output_size=D_filters*4)\n",
    "    h4 = conv2d(h3 , output_size=D_filters*8)\n",
    "    \n",
    "    validity =  Conv2D(1 , kernel_size=(4,4) , strides=(1,1) , padding='same')(h4)\n",
    "    \n",
    "    return Model(style , validity , name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)\n",
    "\n",
    "discriminator_apple = discriminator(D_filters , name='discriminator_apple') #判别苹果风格\n",
    "discriminator_apple.compile(optimizer = adam , loss='mse' , metrics=['accuracy'])\n",
    "discriminator_orange = discriminator(D_filters , name='discriminator_orange') #判别橘子风格\n",
    "discriminator_orange.compile(optimizer = adam , loss='mse' , metrics=['accuracy'])\n",
    "\n",
    "\n",
    "generator_apple2orange = generator(G_filters , name='generator_apple2orange')\n",
    "generator_orange2apple = generator(G_filters , name='generator_orange2apple')\n",
    "\n",
    "\n",
    "apples = Input(shape=SHAPE)\n",
    "oranges = Input(shape=SHAPE)\n",
    "\n",
    "fake_apples = generator_apple2orange(apples) #使用G来 将苹果变成橘子风格的苹果\n",
    "fake_oranges = generator_orange2apple(oranges) #使用F来 将橘子变成苹果风格的橘子\n",
    "\n",
    "apples_hat = generator_orange2apple(fake_apples) #使用F将橘子风格的苹果还原为原苹果\n",
    "oranges_hat = generator_apple2orange(fake_oranges) #使用G将苹果风格的橘子还原为原橘子\n",
    "\n",
    "apples_id = generator_orange2apple(apples)\n",
    "oranges_id = generator_apple2orange(oranges)\n",
    "\n",
    "#freeze D\n",
    "discriminator_apple.trainable = False\n",
    "discriminator_orange.trainable = False\n",
    "\n",
    "validity_apple = discriminator_apple(fake_oranges) #真苹果 和 苹果风格的橘子 之间的潜在模式相似度\n",
    "validity_orange = discriminator_orange(fake_apples) #真橘子 和 橘子风格的苹果 之间的潜在模式相似度\n",
    "\n",
    "#一共6个输出 最后4个输出希望和原图像一致 这样图像同时具有两个风格\n",
    "combined = Model([apples , oranges] , [validity_apple , validity_orange , apples_hat , oranges_hat , apples_id , oranges_id])\n",
    "combined.compile(optimizer=adam , loss=['mse' , 'mse' , 'mae' , 'mae' , 'mae' , 'mae'] , loss_weights=[1 ,1,10, 10 , 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:11.553784 accu:0.214111 |mse1:77.071190 :mse2:33.420551 mae1:25.639145 mae2:0.877545 mae3:0.782217 mae4:0.701103\n",
      "epoch:1 loss:49.221931 accu:0.045166 |mse1:113.575821 :mse2:43.440929 mae1:55.090828 mae2:0.741155 mae3:0.641522 mae4:0.676303\n",
      "epoch:2 loss:45.915680 accu:0.069580 |mse1:21.033131 :mse2:4.208228 mae1:3.649053 mae2:0.669284 mae3:0.501797 mae4:0.760986\n",
      "epoch:3 loss:11.965063 accu:0.122070 |mse1:16.649717 :mse2:1.430929 mae1:1.866860 mae2:0.673152 mae3:0.504948 mae4:0.846047\n",
      "epoch:4 loss:1.906404 accu:0.242676 |mse1:14.612094 :mse2:1.134128 mae1:1.180288 mae2:0.638189 mae3:0.434565 mae4:0.825814\n",
      "epoch:5 loss:0.774842 accu:0.398682 |mse1:15.909773 :mse2:1.323307 mae1:1.301226 mae2:0.605599 mae3:0.564084 mae4:0.782066\n",
      "epoch:6 loss:0.342780 accu:0.623291 |mse1:15.112564 :mse2:0.802843 mae1:1.201771 mae2:0.559132 mae3:0.595427 mae4:0.787570\n",
      "epoch:7 loss:0.362108 accu:0.578369 |mse1:13.687601 :mse2:0.350701 mae1:0.931773 mae2:0.507696 mae3:0.571418 mae4:0.734756\n",
      "epoch:8 loss:0.254310 accu:0.679932 |mse1:13.178265 :mse2:0.370581 mae1:0.930317 mae2:0.491885 mae3:0.535129 mae4:0.775988\n",
      "epoch:9 loss:0.162216 accu:0.787598 |mse1:12.462142 :mse2:0.366722 mae1:0.942089 mae2:0.518553 mae3:0.438933 mae4:0.719521\n",
      "epoch:10 loss:0.154513 accu:0.814697 |mse1:12.933287 :mse2:0.409077 mae1:0.715401 mae2:0.528431 mae3:0.489914 mae4:0.769821\n",
      "epoch:11 loss:0.147985 accu:0.814697 |mse1:13.070522 :mse2:0.471444 mae1:0.716481 mae2:0.541816 mae3:0.486892 mae4:0.779773\n",
      "epoch:12 loss:0.171414 accu:0.789307 |mse1:12.171750 :mse2:0.338374 mae1:0.609424 mae2:0.421009 mae3:0.536361 mae4:0.779031\n",
      "epoch:13 loss:0.161872 accu:0.790527 |mse1:12.026316 :mse2:0.210321 mae1:0.485458 mae2:0.474528 mae3:0.509715 mae4:0.705486\n",
      "epoch:14 loss:0.145709 accu:0.813721 |mse1:11.999265 :mse2:0.208323 mae1:0.391733 mae2:0.465437 mae3:0.522829 mae4:0.714920\n",
      "epoch:15 loss:0.142984 accu:0.808350 |mse1:11.277291 :mse2:0.306460 mae1:0.392354 mae2:0.453116 mae3:0.445216 mae4:0.760359\n",
      "epoch:16 loss:0.142143 accu:0.812988 |mse1:10.601016 :mse2:0.225976 mae1:0.540509 mae2:0.436412 mae3:0.390675 mae4:0.792815\n",
      "epoch:17 loss:0.125359 accu:0.839844 |mse1:11.259242 :mse2:0.288665 mae1:0.587117 mae2:0.418306 mae3:0.476696 mae4:0.748106\n",
      "epoch:18 loss:0.148482 accu:0.804443 |mse1:10.921737 :mse2:0.245886 mae1:0.490056 mae2:0.415668 mae3:0.462626 mae4:0.657006\n",
      "epoch:19 loss:0.130336 accu:0.845703 |mse1:8.682899 :mse2:0.164819 mae1:0.478424 mae2:0.352264 mae3:0.307869 mae4:0.694846\n",
      "epoch:20 loss:0.156059 accu:0.784912 |mse1:10.451687 :mse2:0.475202 mae1:0.432850 mae2:0.383499 mae3:0.412091 mae4:0.841284\n",
      "epoch:21 loss:0.146603 accu:0.812744 |mse1:9.576358 :mse2:0.293461 mae1:0.497187 mae2:0.349839 mae3:0.372092 mae4:0.822645\n",
      "epoch:22 loss:0.189812 accu:0.723145 |mse1:11.635131 :mse2:0.229044 mae1:0.584084 mae2:0.433068 mae3:0.506783 mae4:0.726757\n",
      "epoch:23 loss:0.134691 accu:0.822021 |mse1:10.262731 :mse2:0.262374 mae1:0.282177 mae2:0.398970 mae3:0.423431 mae4:0.822095\n",
      "epoch:24 loss:0.103669 accu:0.879883 |mse1:8.359189 :mse2:0.389496 mae1:0.269970 mae2:0.348507 mae3:0.276440 mae4:0.734643\n",
      "epoch:25 loss:0.102916 accu:0.871826 |mse1:8.881463 :mse2:0.286291 mae1:0.524639 mae2:0.350379 mae3:0.310463 mae4:0.719055\n",
      "epoch:26 loss:0.112214 accu:0.878662 |mse1:10.051700 :mse2:0.329268 mae1:0.362748 mae2:0.396823 mae3:0.398095 mae4:0.700071\n",
      "epoch:27 loss:0.125425 accu:0.832764 |mse1:10.339255 :mse2:0.277823 mae1:0.394862 mae2:0.394624 mae3:0.433630 mae4:0.722547\n",
      "epoch:28 loss:0.235504 accu:0.657959 |mse1:10.354232 :mse2:0.166926 mae1:0.353027 mae2:0.364122 mae3:0.482999 mae4:0.666095\n",
      "epoch:29 loss:0.118003 accu:0.845703 |mse1:9.125554 :mse2:0.329517 mae1:0.268357 mae2:0.340816 mae3:0.360858 mae4:0.735057\n",
      "epoch:30 loss:0.186144 accu:0.742432 |mse1:10.194028 :mse2:0.209639 mae1:0.270420 mae2:0.349887 mae3:0.475523 mae4:0.731354\n",
      "epoch:31 loss:0.222391 accu:0.699707 |mse1:10.220800 :mse2:0.183166 mae1:0.303933 mae2:0.391776 mae3:0.438492 mae4:0.724953\n",
      "epoch:32 loss:0.185477 accu:0.749756 |mse1:9.990720 :mse2:0.168261 mae1:0.252978 mae2:0.442927 mae3:0.376042 mae4:0.705619\n",
      "epoch:33 loss:0.121825 accu:0.813477 |mse1:8.368346 :mse2:0.181912 mae1:0.239924 mae2:0.339993 mae3:0.312957 mae4:0.660314\n",
      "epoch:34 loss:0.077550 accu:0.910400 |mse1:9.235593 :mse2:0.207966 mae1:0.475502 mae2:0.341466 mae3:0.370162 mae4:0.685854\n",
      "epoch:35 loss:0.112811 accu:0.866943 |mse1:8.276451 :mse2:0.189081 mae1:0.434579 mae2:0.317754 mae3:0.309359 mae4:0.642660\n",
      "epoch:36 loss:0.200978 accu:0.736328 |mse1:9.560083 :mse2:0.154406 mae1:0.359804 mae2:0.321552 mae3:0.440906 mae4:0.731776\n",
      "epoch:37 loss:0.143203 accu:0.802490 |mse1:8.853440 :mse2:0.171498 mae1:0.545523 mae2:0.347869 mae3:0.322372 mae4:0.658791\n",
      "epoch:38 loss:0.103961 accu:0.894775 |mse1:9.014488 :mse2:0.267002 mae1:0.465486 mae2:0.366027 mae3:0.325364 mae4:0.600750\n",
      "epoch:39 loss:0.103213 accu:0.871338 |mse1:7.996380 :mse2:0.255738 mae1:0.354848 mae2:0.317845 mae3:0.271396 mae4:0.763646\n",
      "epoch:40 loss:0.168689 accu:0.748291 |mse1:8.426328 :mse2:0.208044 mae1:0.401146 mae2:0.326003 mae3:0.311153 mae4:0.675872\n",
      "epoch:41 loss:0.107470 accu:0.883789 |mse1:8.410333 :mse2:0.263437 mae1:0.424809 mae2:0.345329 mae3:0.279996 mae4:0.702631\n",
      "epoch:42 loss:0.107661 accu:0.872803 |mse1:8.108503 :mse2:0.207763 mae1:0.292279 mae2:0.349175 mae3:0.280200 mae4:0.632327\n",
      "epoch:43 loss:0.067549 accu:0.941162 |mse1:8.079764 :mse2:0.242034 mae1:0.349512 mae2:0.347814 mae3:0.257184 mae4:0.726092\n",
      "epoch:44 loss:0.128679 accu:0.831055 |mse1:7.756835 :mse2:0.379970 mae1:0.367768 mae2:0.292424 mae3:0.265284 mae4:0.750213\n",
      "epoch:45 loss:0.242797 accu:0.645264 |mse1:8.396956 :mse2:0.123204 mae1:0.803648 mae2:0.281186 mae3:0.318462 mae4:0.737994\n",
      "epoch:46 loss:0.282732 accu:0.623535 |mse1:8.972111 :mse2:0.583962 mae1:0.409642 mae2:0.315067 mae3:0.343006 mae4:0.743178\n",
      "epoch:47 loss:0.390267 accu:0.545898 |mse1:8.148482 :mse2:0.423619 mae1:0.420555 mae2:0.280609 mae3:0.292495 mae4:0.755685\n",
      "epoch:48 loss:0.532029 accu:0.538330 |mse1:7.588159 :mse2:0.457470 mae1:0.574417 mae2:0.278779 mae3:0.231899 mae4:0.753373\n",
      "epoch:49 loss:0.346072 accu:0.632812 |mse1:8.177996 :mse2:0.216241 mae1:0.998913 mae2:0.325407 mae3:0.227341 mae4:0.634144\n",
      "epoch:50 loss:0.254004 accu:0.587646 |mse1:8.275658 :mse2:0.694501 mae1:1.041193 mae2:0.276086 mae3:0.241360 mae4:0.664808\n",
      "epoch:51 loss:0.183652 accu:0.755615 |mse1:8.260006 :mse2:0.138971 mae1:0.650376 mae2:0.332947 mae3:0.277108 mae4:0.679900\n",
      "epoch:52 loss:0.177343 accu:0.752686 |mse1:7.838369 :mse2:0.346292 mae1:0.372633 mae2:0.313454 mae3:0.247069 mae4:0.713054\n",
      "epoch:53 loss:0.110116 accu:0.861572 |mse1:7.768183 :mse2:0.277778 mae1:0.315824 mae2:0.310452 mae3:0.261147 mae4:0.648375\n",
      "epoch:54 loss:0.109355 accu:0.870117 |mse1:7.665925 :mse2:0.225933 mae1:0.312897 mae2:0.333647 mae3:0.250569 mae4:0.551947\n",
      "epoch:55 loss:0.074027 accu:0.937256 |mse1:7.424009 :mse2:0.174452 mae1:0.227583 mae2:0.322985 mae3:0.236973 mae4:0.725868\n",
      "epoch:56 loss:0.072240 accu:0.938965 |mse1:7.639084 :mse2:0.173046 mae1:0.619616 mae2:0.338734 mae3:0.204684 mae4:0.649604\n",
      "epoch:57 loss:0.147044 accu:0.800781 |mse1:7.958724 :mse2:0.176938 mae1:0.173422 mae2:0.362505 mae3:0.256814 mae4:0.750114\n",
      "epoch:58 loss:0.060900 accu:0.959473 |mse1:6.954870 :mse2:0.160107 mae1:0.180711 mae2:0.292556 mae3:0.221504 mae4:0.746789\n",
      "epoch:59 loss:0.070894 accu:0.938477 |mse1:6.875480 :mse2:0.167591 mae1:0.112178 mae2:0.316256 mae3:0.198032 mae4:0.770523\n",
      "epoch:60 loss:0.091154 accu:0.908203 |mse1:7.290967 :mse2:0.140221 mae1:0.270174 mae2:0.310685 mae3:0.243256 mae4:0.606198\n",
      "epoch:61 loss:0.127367 accu:0.837402 |mse1:7.462111 :mse2:0.175616 mae1:0.287810 mae2:0.298815 mae3:0.248758 mae4:0.673364\n",
      "epoch:62 loss:0.173142 accu:0.768799 |mse1:7.128602 :mse2:0.222512 mae1:0.612748 mae2:0.276654 mae3:0.215302 mae4:0.692312\n",
      "epoch:63 loss:0.158206 accu:0.789551 |mse1:7.389200 :mse2:0.185708 mae1:0.699790 mae2:0.303401 mae3:0.212709 mae4:0.582303\n",
      "epoch:64 loss:0.177124 accu:0.769775 |mse1:7.888488 :mse2:0.128896 mae1:0.420659 mae2:0.314654 mae3:0.280591 mae4:0.648960\n",
      "epoch:65 loss:0.279411 accu:0.645508 |mse1:7.752077 :mse2:0.406189 mae1:0.181777 mae2:0.303331 mae3:0.269914 mae4:0.692402\n",
      "epoch:66 loss:0.243970 accu:0.674561 |mse1:7.362368 :mse2:0.162558 mae1:0.500067 mae2:0.254906 mae3:0.268175 mae4:0.614131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:67 loss:0.237849 accu:0.691895 |mse1:6.539742 :mse2:0.189083 mae1:0.176901 mae2:0.278064 mae3:0.202497 mae4:0.669647\n",
      "epoch:68 loss:0.335674 accu:0.618896 |mse1:7.359292 :mse2:0.148239 mae1:0.370633 mae2:0.278194 mae3:0.266630 mae4:0.621288\n",
      "epoch:69 loss:0.251676 accu:0.675049 |mse1:7.531012 :mse2:0.352548 mae1:0.421875 mae2:0.304857 mae3:0.244166 mae4:0.615229\n",
      "epoch:70 loss:0.266829 accu:0.721436 |mse1:6.844807 :mse2:0.274891 mae1:0.167250 mae2:0.274582 mae3:0.241722 mae4:0.654408\n",
      "epoch:71 loss:0.243897 accu:0.731934 |mse1:7.361660 :mse2:0.276413 mae1:0.170888 mae2:0.285685 mae3:0.266940 mae4:0.604747\n",
      "epoch:72 loss:0.276924 accu:0.662354 |mse1:7.509018 :mse2:0.233627 mae1:0.664754 mae2:0.258492 mae3:0.269582 mae4:0.603068\n",
      "epoch:73 loss:0.292874 accu:0.660889 |mse1:6.931196 :mse2:0.283773 mae1:0.211343 mae2:0.312321 mae3:0.201735 mae4:0.638508\n",
      "epoch:74 loss:0.412264 accu:0.495117 |mse1:7.421015 :mse2:0.345390 mae1:0.124306 mae2:0.323230 mae3:0.232698 mae4:0.678226\n",
      "epoch:75 loss:0.125561 accu:0.830566 |mse1:6.280893 :mse2:0.095623 mae1:0.204010 mae2:0.260287 mae3:0.210725 mae4:0.605795\n",
      "epoch:76 loss:0.085924 accu:0.927246 |mse1:6.536507 :mse2:0.199827 mae1:0.199057 mae2:0.292824 mae3:0.190567 mae4:0.576027\n",
      "epoch:77 loss:0.110546 accu:0.852295 |mse1:6.499554 :mse2:0.223437 mae1:0.138425 mae2:0.246815 mae3:0.236112 mae4:0.654156\n",
      "epoch:78 loss:0.164068 accu:0.761230 |mse1:7.706991 :mse2:0.303855 mae1:0.184158 mae2:0.270844 mae3:0.306056 mae4:0.657611\n",
      "epoch:79 loss:0.114206 accu:0.846924 |mse1:6.084916 :mse2:0.211047 mae1:0.103518 mae2:0.265265 mae3:0.186600 mae4:0.619060\n",
      "epoch:80 loss:0.120096 accu:0.843750 |mse1:6.410654 :mse2:0.163445 mae1:0.279682 mae2:0.252444 mae3:0.216291 mae4:0.576773\n",
      "epoch:81 loss:0.189459 accu:0.731689 |mse1:6.413060 :mse2:0.171167 mae1:0.211454 mae2:0.297591 mae3:0.190653 mae4:0.624649\n",
      "epoch:82 loss:0.161480 accu:0.768799 |mse1:7.141126 :mse2:0.105128 mae1:0.234092 mae2:0.296032 mae3:0.232173 mae4:0.624535\n",
      "epoch:83 loss:0.124476 accu:0.838135 |mse1:7.344670 :mse2:0.148986 mae1:0.439976 mae2:0.286846 mae3:0.243405 mae4:0.628260\n",
      "epoch:84 loss:0.083781 accu:0.930176 |mse1:6.736320 :mse2:0.144164 mae1:0.173878 mae2:0.266543 mae3:0.246107 mae4:0.596587\n",
      "epoch:85 loss:0.046337 accu:0.972168 |mse1:7.200485 :mse2:0.266515 mae1:0.256859 mae2:0.277174 mae3:0.254673 mae4:0.600867\n",
      "epoch:86 loss:0.044670 accu:0.983887 |mse1:6.189898 :mse2:0.109299 mae1:0.111438 mae2:0.278772 mae3:0.168836 mae4:0.757159\n",
      "epoch:87 loss:0.082007 accu:0.909668 |mse1:6.594305 :mse2:0.156344 mae1:0.228506 mae2:0.263284 mae3:0.212964 mae4:0.676179\n",
      "epoch:88 loss:0.127375 accu:0.855225 |mse1:7.324868 :mse2:0.250093 mae1:0.135695 mae2:0.305043 mae3:0.256423 mae4:0.594656\n",
      "epoch:89 loss:0.104620 accu:0.868896 |mse1:6.290440 :mse2:0.118231 mae1:0.101848 mae2:0.263454 mae3:0.207051 mae4:0.645529\n",
      "epoch:90 loss:0.160745 accu:0.761719 |mse1:6.893148 :mse2:0.198319 mae1:0.215295 mae2:0.299628 mae3:0.218007 mae4:0.630583\n",
      "epoch:91 loss:0.189982 accu:0.774414 |mse1:6.621452 :mse2:0.539898 mae1:0.163058 mae2:0.300890 mae3:0.169254 mae4:0.574747\n",
      "epoch:92 loss:0.281753 accu:0.606445 |mse1:6.975680 :mse2:0.085475 mae1:0.267613 mae2:0.316896 mae3:0.224076 mae4:0.622494\n",
      "epoch:93 loss:0.366639 accu:0.650879 |mse1:7.633344 :mse2:1.184934 mae1:0.148082 mae2:0.314001 mae3:0.199123 mae4:0.570738\n",
      "epoch:94 loss:0.212348 accu:0.790283 |mse1:7.003641 :mse2:1.042632 mae1:0.082671 mae2:0.227720 mae3:0.216773 mae4:0.692411\n",
      "epoch:95 loss:0.137516 accu:0.861816 |mse1:6.461353 :mse2:0.092900 mae1:0.375098 mae2:0.251921 mae3:0.202350 mae4:0.634093\n",
      "epoch:96 loss:0.115972 accu:0.878906 |mse1:7.022699 :mse2:0.127502 mae1:0.103480 mae2:0.296265 mae3:0.253850 mae4:0.606691\n",
      "epoch:97 loss:0.064847 accu:0.941162 |mse1:6.238897 :mse2:0.131179 mae1:0.220640 mae2:0.263790 mae3:0.195734 mae4:0.558577\n",
      "epoch:98 loss:0.101718 accu:0.870117 |mse1:6.644771 :mse2:0.110868 mae1:0.139214 mae2:0.292073 mae3:0.226351 mae4:0.557061\n",
      "epoch:99 loss:0.132252 accu:0.832520 |mse1:7.207633 :mse2:0.612693 mae1:0.532851 mae2:0.258027 mae3:0.226093 mae4:0.477453\n",
      "epoch:100 loss:0.233370 accu:0.670654 |mse1:7.125988 :mse2:0.782694 mae1:0.141805 mae2:0.260778 mae3:0.235563 mae4:0.578930\n",
      "epoch:101 loss:0.648430 accu:0.452393 |mse1:6.203434 :mse2:0.477844 mae1:0.128913 mae2:0.216828 mae3:0.231623 mae4:0.560800\n",
      "epoch:102 loss:0.262967 accu:0.665283 |mse1:6.248155 :mse2:0.203006 mae1:0.154379 mae2:0.263213 mae3:0.201852 mae4:0.557632\n",
      "epoch:103 loss:0.117527 accu:0.863525 |mse1:5.904413 :mse2:0.356728 mae1:0.135977 mae2:0.196161 mae3:0.218984 mae4:0.612605\n",
      "epoch:104 loss:0.185269 accu:0.830811 |mse1:6.715112 :mse2:0.104082 mae1:0.439533 mae2:0.255356 mae3:0.245306 mae4:0.576270\n",
      "epoch:105 loss:0.191970 accu:0.752930 |mse1:6.521056 :mse2:0.064759 mae1:0.169743 mae2:0.294094 mae3:0.207874 mae4:0.602715\n",
      "epoch:106 loss:0.278491 accu:0.663330 |mse1:5.941832 :mse2:0.074317 mae1:0.521307 mae2:0.228276 mae3:0.188643 mae4:0.575753\n",
      "epoch:107 loss:0.168076 accu:0.800781 |mse1:6.585520 :mse2:0.131355 mae1:0.161230 mae2:0.281251 mae3:0.244894 mae4:0.497311\n",
      "epoch:108 loss:0.350733 accu:0.553955 |mse1:6.359165 :mse2:0.226870 mae1:0.486004 mae2:0.260359 mae3:0.208036 mae4:0.506353\n",
      "epoch:109 loss:0.163477 accu:0.797119 |mse1:6.175511 :mse2:0.143199 mae1:0.113859 mae2:0.236018 mae3:0.243725 mae4:0.539186\n",
      "epoch:110 loss:0.136919 accu:0.831055 |mse1:6.294071 :mse2:0.118090 mae1:0.258126 mae2:0.272794 mae3:0.207799 mae4:0.562337\n",
      "epoch:111 loss:0.133854 accu:0.832031 |mse1:6.467465 :mse2:0.164195 mae1:0.204552 mae2:0.243800 mae3:0.245478 mae4:0.541309\n",
      "epoch:112 loss:0.132789 accu:0.812988 |mse1:6.663443 :mse2:0.152171 mae1:0.188871 mae2:0.290095 mae3:0.221992 mae4:0.515274\n",
      "epoch:113 loss:0.108632 accu:0.862793 |mse1:5.913380 :mse2:0.247391 mae1:0.125114 mae2:0.230143 mae3:0.211166 mae4:0.529261\n",
      "epoch:114 loss:0.109398 accu:0.878174 |mse1:6.348504 :mse2:0.068413 mae1:0.109414 mae2:0.262122 mae3:0.233857 mae4:0.503769\n",
      "epoch:115 loss:0.090853 accu:0.898926 |mse1:7.349448 :mse2:0.080699 mae1:0.154236 mae2:0.322266 mae3:0.273051 mae4:0.565485\n",
      "epoch:116 loss:0.169185 accu:0.751465 |mse1:5.738504 :mse2:0.080829 mae1:0.082467 mae2:0.261646 mae3:0.182045 mae4:0.548659\n",
      "epoch:117 loss:0.101957 accu:0.861816 |mse1:5.872921 :mse2:0.119052 mae1:0.108998 mae2:0.238104 mae3:0.208379 mae4:0.504053\n",
      "epoch:118 loss:0.154000 accu:0.803955 |mse1:6.241710 :mse2:0.129865 mae1:0.137515 mae2:0.249778 mae3:0.231464 mae4:0.526450\n",
      "epoch:119 loss:0.123973 accu:0.864258 |mse1:6.262004 :mse2:0.085575 mae1:0.517263 mae2:0.263981 mae3:0.200036 mae4:0.514938\n",
      "epoch:120 loss:0.109177 accu:0.877197 |mse1:5.933613 :mse2:0.160495 mae1:0.216027 mae2:0.224504 mae3:0.214321 mae4:0.568865\n",
      "epoch:121 loss:0.090953 accu:0.906982 |mse1:6.306900 :mse2:0.271924 mae1:0.202813 mae2:0.271960 mae3:0.191870 mae4:0.577565\n",
      "epoch:122 loss:0.175366 accu:0.779297 |mse1:5.480933 :mse2:0.145604 mae1:0.097925 mae2:0.240183 mae3:0.165448 mae4:0.573240\n",
      "epoch:123 loss:0.095247 accu:0.913330 |mse1:5.356244 :mse2:0.151379 mae1:0.101482 mae2:0.219625 mae3:0.177247 mae4:0.514813\n",
      "epoch:124 loss:0.069498 accu:0.951416 |mse1:5.731834 :mse2:0.095964 mae1:0.101739 mae2:0.264794 mae3:0.174589 mae4:0.538314\n",
      "epoch:125 loss:0.163851 accu:0.782227 |mse1:6.439038 :mse2:0.127097 mae1:0.223963 mae2:0.266284 mae3:0.231930 mae4:0.561435\n",
      "epoch:126 loss:0.234031 accu:0.649170 |mse1:5.768484 :mse2:0.162108 mae1:0.138421 mae2:0.254397 mae3:0.179839 mae4:0.532671\n",
      "epoch:127 loss:0.279135 accu:0.637451 |mse1:5.516750 :mse2:0.258783 mae1:0.100046 mae2:0.205450 mae3:0.211908 mae4:0.503074\n",
      "epoch:128 loss:0.267406 accu:0.656494 |mse1:5.375991 :mse2:0.107088 mae1:0.142358 mae2:0.237427 mae3:0.164106 mae4:0.499922\n",
      "epoch:129 loss:0.126113 accu:0.868164 |mse1:6.128462 :mse2:0.508902 mae1:0.227045 mae2:0.227829 mae3:0.201397 mae4:0.469537\n",
      "epoch:130 loss:0.143178 accu:0.805664 |mse1:5.415087 :mse2:0.241836 mae1:0.114436 mae2:0.228540 mae3:0.177815 mae4:0.467276\n",
      "epoch:131 loss:0.138705 accu:0.790771 |mse1:6.687663 :mse2:0.378776 mae1:0.386000 mae2:0.284463 mae3:0.215683 mae4:0.434510\n",
      "epoch:132 loss:0.454481 accu:0.573486 |mse1:6.620749 :mse2:0.204533 mae1:0.131527 mae2:0.351658 mae3:0.195769 mae4:0.484029\n",
      "epoch:133 loss:0.233409 accu:0.667480 |mse1:6.476675 :mse2:0.457798 mae1:0.584007 mae2:0.273487 mae3:0.187467 mae4:0.505116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:134 loss:0.196368 accu:0.734131 |mse1:6.208761 :mse2:0.213504 mae1:0.252441 mae2:0.267361 mae3:0.220499 mae4:0.425637\n",
      "epoch:135 loss:0.424306 accu:0.564941 |mse1:7.118859 :mse2:0.933553 mae1:0.715220 mae2:0.263731 mae3:0.207876 mae4:0.389450\n",
      "epoch:136 loss:0.456725 accu:0.499023 |mse1:6.137365 :mse2:0.172899 mae1:0.234004 mae2:0.262190 mae3:0.224902 mae4:0.449729\n",
      "epoch:137 loss:0.277249 accu:0.713135 |mse1:5.995755 :mse2:0.550426 mae1:0.309121 mae2:0.240020 mae3:0.193883 mae4:0.373585\n",
      "epoch:138 loss:0.243767 accu:0.664551 |mse1:6.071097 :mse2:0.213473 mae1:0.552719 mae2:0.224906 mae3:0.227073 mae4:0.375839\n",
      "epoch:139 loss:0.326402 accu:0.530762 |mse1:5.314434 :mse2:0.156407 mae1:0.121485 mae2:0.259213 mae3:0.175160 mae4:0.402177\n",
      "epoch:140 loss:0.258037 accu:0.671875 |mse1:6.319360 :mse2:0.373255 mae1:0.798568 mae2:0.238576 mae3:0.195275 mae4:0.382807\n",
      "epoch:141 loss:0.300581 accu:0.646240 |mse1:5.986039 :mse2:0.265205 mae1:0.139851 mae2:0.252764 mae3:0.226756 mae4:0.406645\n",
      "epoch:142 loss:0.187818 accu:0.735840 |mse1:6.143061 :mse2:0.239713 mae1:0.114126 mae2:0.262707 mae3:0.232663 mae4:0.412707\n",
      "epoch:143 loss:0.128082 accu:0.803711 |mse1:6.206979 :mse2:0.194136 mae1:0.671799 mae2:0.243059 mae3:0.210722 mae4:0.412779\n",
      "epoch:144 loss:0.313784 accu:0.595703 |mse1:6.408310 :mse2:0.305357 mae1:0.385575 mae2:0.271951 mae3:0.216438 mae4:0.445721\n",
      "epoch:145 loss:0.163134 accu:0.733154 |mse1:5.888509 :mse2:0.323544 mae1:0.231646 mae2:0.243471 mae3:0.211795 mae4:0.464822\n",
      "epoch:146 loss:0.186836 accu:0.782471 |mse1:6.955345 :mse2:0.291654 mae1:1.100850 mae2:0.224393 mae3:0.246325 mae4:0.406693\n",
      "epoch:147 loss:0.346093 accu:0.673584 |mse1:6.702864 :mse2:0.153641 mae1:1.038145 mae2:0.283356 mae3:0.186899 mae4:0.413374\n",
      "epoch:148 loss:0.200172 accu:0.733154 |mse1:6.582506 :mse2:0.112168 mae1:0.495085 mae2:0.278821 mae3:0.241957 mae4:0.419706\n",
      "epoch:149 loss:0.529407 accu:0.474121 |mse1:7.345798 :mse2:0.585442 mae1:0.340416 mae2:0.356685 mae3:0.197335 mae4:0.483458\n",
      "epoch:150 loss:0.292863 accu:0.594482 |mse1:6.710179 :mse2:0.639657 mae1:0.202824 mae2:0.239231 mae3:0.256876 mae4:0.401562\n",
      "epoch:151 loss:0.327981 accu:0.484619 |mse1:5.680930 :mse2:0.372158 mae1:0.159578 mae2:0.246914 mae3:0.186506 mae4:0.410442\n",
      "epoch:152 loss:0.150705 accu:0.777100 |mse1:5.564511 :mse2:0.389005 mae1:0.222834 mae2:0.225661 mae3:0.181931 mae4:0.503453\n",
      "epoch:153 loss:0.214082 accu:0.700928 |mse1:5.874440 :mse2:0.463906 mae1:0.092342 mae2:0.247888 mae3:0.199397 mae4:0.471089\n",
      "epoch:154 loss:0.186441 accu:0.752197 |mse1:6.044363 :mse2:0.366124 mae1:0.214769 mae2:0.254809 mae3:0.204622 mae4:0.435722\n",
      "epoch:155 loss:0.285468 accu:0.604980 |mse1:5.858905 :mse2:0.205716 mae1:0.227715 mae2:0.268102 mae3:0.194669 mae4:0.464403\n",
      "epoch:156 loss:0.233987 accu:0.732178 |mse1:6.048699 :mse2:0.753242 mae1:0.461217 mae2:0.207194 mae3:0.194066 mae4:0.421875\n",
      "epoch:157 loss:0.538021 accu:0.583252 |mse1:5.157987 :mse2:0.541692 mae1:0.225308 mae2:0.201603 mae3:0.163082 mae4:0.359042\n",
      "epoch:158 loss:0.273190 accu:0.583252 |mse1:5.240164 :mse2:0.152607 mae1:0.407715 mae2:0.219947 mae3:0.169243 mae4:0.387507\n",
      "epoch:159 loss:0.242552 accu:0.656250 |mse1:5.419544 :mse2:0.473545 mae1:0.132003 mae2:0.231094 mae3:0.176063 mae4:0.393542\n",
      "epoch:160 loss:0.145087 accu:0.802490 |mse1:5.705969 :mse2:0.224783 mae1:0.157779 mae2:0.264889 mae3:0.183516 mae4:0.418880\n",
      "epoch:161 loss:0.096251 accu:0.883789 |mse1:5.391832 :mse2:0.248840 mae1:0.197482 mae2:0.232637 mae3:0.178966 mae4:0.451854\n",
      "epoch:162 loss:0.197449 accu:0.706543 |mse1:4.694645 :mse2:0.259154 mae1:0.085355 mae2:0.197005 mae3:0.174671 mae4:0.322382\n",
      "epoch:163 loss:0.116725 accu:0.833496 |mse1:7.348522 :mse2:0.170194 mae1:1.388543 mae2:0.243092 mae3:0.253073 mae4:0.375872\n",
      "epoch:164 loss:0.257389 accu:0.654297 |mse1:6.173472 :mse2:0.171388 mae1:0.446370 mae2:0.259290 mae3:0.220732 mae4:0.396203\n",
      "epoch:165 loss:0.194706 accu:0.705566 |mse1:6.127863 :mse2:0.342616 mae1:0.117010 mae2:0.250511 mae3:0.233995 mae4:0.386847\n",
      "epoch:166 loss:0.273853 accu:0.596436 |mse1:5.653717 :mse2:0.411591 mae1:0.333494 mae2:0.214796 mae3:0.204417 mae4:0.323795\n",
      "epoch:167 loss:0.294285 accu:0.583008 |mse1:5.088886 :mse2:0.159224 mae1:0.366805 mae2:0.212989 mae3:0.165109 mae4:0.369597\n",
      "epoch:168 loss:0.200393 accu:0.675049 |mse1:5.975976 :mse2:0.340045 mae1:0.367109 mae2:0.236277 mae3:0.206561 mae4:0.387341\n",
      "epoch:169 loss:0.194924 accu:0.733398 |mse1:7.870550 :mse2:0.491457 mae1:0.417859 mae2:0.258244 mae3:0.341987 mae4:0.401411\n",
      "epoch:170 loss:0.235319 accu:0.691650 |mse1:6.212809 :mse2:0.267016 mae1:0.235718 mae2:0.278637 mae3:0.208100 mae4:0.447671\n",
      "epoch:171 loss:0.146391 accu:0.772461 |mse1:6.849174 :mse2:0.429552 mae1:0.166579 mae2:0.298387 mae3:0.241777 mae4:0.477791\n",
      "epoch:172 loss:0.163493 accu:0.729980 |mse1:6.391433 :mse2:0.226342 mae1:0.554462 mae2:0.275921 mae3:0.205631 mae4:0.447471\n",
      "epoch:173 loss:0.171084 accu:0.770752 |mse1:5.965746 :mse2:0.396026 mae1:0.180070 mae2:0.284375 mae3:0.167925 mae4:0.417072\n",
      "epoch:174 loss:0.198235 accu:0.757324 |mse1:5.083134 :mse2:0.166265 mae1:0.196405 mae2:0.226236 mae3:0.178239 mae4:0.386340\n",
      "epoch:175 loss:0.196967 accu:0.731689 |mse1:6.357013 :mse2:0.241881 mae1:0.171349 mae2:0.282998 mae3:0.234740 mae4:0.440441\n",
      "epoch:176 loss:0.170550 accu:0.730957 |mse1:5.927773 :mse2:0.342638 mae1:0.377479 mae2:0.232838 mae3:0.216703 mae4:0.379008\n",
      "epoch:177 loss:0.178486 accu:0.761230 |mse1:4.951953 :mse2:0.208307 mae1:0.097270 mae2:0.216343 mae3:0.178306 mae4:0.390888\n",
      "epoch:178 loss:0.244492 accu:0.691895 |mse1:6.546283 :mse2:0.449456 mae1:1.010836 mae2:0.219420 mae3:0.215167 mae4:0.328475\n",
      "epoch:179 loss:0.290267 accu:0.633789 |mse1:6.505645 :mse2:0.772616 mae1:0.666247 mae2:0.246667 mae3:0.174967 mae4:0.437250\n",
      "epoch:180 loss:0.260088 accu:0.629883 |mse1:5.231387 :mse2:0.181604 mae1:0.325739 mae2:0.207273 mae3:0.194720 mae4:0.370053\n",
      "epoch:181 loss:0.200543 accu:0.718994 |mse1:5.467555 :mse2:0.402555 mae1:0.625438 mae2:0.235038 mae3:0.147224 mae4:0.343893\n",
      "epoch:182 loss:0.275086 accu:0.673340 |mse1:5.949946 :mse2:0.555384 mae1:0.286157 mae2:0.216876 mae3:0.215334 mae4:0.342888\n",
      "epoch:183 loss:0.256325 accu:0.620117 |mse1:5.422544 :mse2:0.186352 mae1:0.143254 mae2:0.217641 mae3:0.216986 mae4:0.362307\n",
      "epoch:184 loss:0.188058 accu:0.758301 |mse1:5.396605 :mse2:0.233097 mae1:0.124040 mae2:0.251820 mae3:0.184528 mae4:0.371673\n",
      "epoch:185 loss:0.307663 accu:0.499512 |mse1:5.502878 :mse2:0.263261 mae1:0.112272 mae2:0.233571 mae3:0.213170 mae4:0.315086\n",
      "epoch:186 loss:0.225183 accu:0.677490 |mse1:5.027437 :mse2:0.344293 mae1:0.158335 mae2:0.207320 mae3:0.185342 mae4:0.344128\n",
      "epoch:187 loss:0.296066 accu:0.552002 |mse1:5.916692 :mse2:0.186623 mae1:0.107515 mae2:0.245588 mae3:0.246895 mae4:0.352077\n",
      "epoch:188 loss:0.256613 accu:0.618652 |mse1:6.440696 :mse2:0.257550 mae1:0.203556 mae2:0.281204 mae3:0.237034 mae4:0.402567\n",
      "epoch:189 loss:0.302945 accu:0.648682 |mse1:5.850951 :mse2:0.309952 mae1:0.180690 mae2:0.195040 mae3:0.261740 mae4:0.337492\n",
      "epoch:190 loss:0.203979 accu:0.705566 |mse1:5.814480 :mse2:0.120237 mae1:0.275721 mae2:0.249761 mae3:0.212763 mae4:0.403543\n",
      "epoch:191 loss:0.195487 accu:0.691406 |mse1:5.131704 :mse2:0.461810 mae1:0.120899 mae2:0.226076 mae3:0.163256 mae4:0.370073\n",
      "epoch:192 loss:0.223985 accu:0.677002 |mse1:6.336076 :mse2:0.444057 mae1:0.498748 mae2:0.279882 mae3:0.188336 mae4:0.402616\n",
      "epoch:193 loss:0.212669 accu:0.699707 |mse1:5.620383 :mse2:0.256459 mae1:0.191309 mae2:0.260253 mae3:0.185579 mae4:0.374684\n",
      "epoch:194 loss:0.172641 accu:0.764893 |mse1:4.705924 :mse2:0.203434 mae1:0.139516 mae2:0.231570 mae3:0.142085 mae4:0.340996\n",
      "epoch:195 loss:0.446233 accu:0.465088 |mse1:5.108211 :mse2:0.577101 mae1:0.127391 mae2:0.223099 mae3:0.156880 mae4:0.380923\n",
      "epoch:196 loss:0.322861 accu:0.578857 |mse1:5.646876 :mse2:0.223742 mae1:0.092476 mae2:0.254609 mae3:0.207586 mae4:0.400116\n",
      "epoch:197 loss:0.246993 accu:0.654297 |mse1:5.318431 :mse2:0.266884 mae1:0.112886 mae2:0.223047 mae3:0.203064 mae4:0.377658\n",
      "epoch:198 loss:0.219442 accu:0.720947 |mse1:5.591890 :mse2:0.440303 mae1:0.279854 mae2:0.224501 mae3:0.181066 mae4:0.400856\n",
      "epoch:199 loss:0.291270 accu:0.582520 |mse1:5.747761 :mse2:0.630939 mae1:0.374904 mae2:0.224940 mae3:0.178185 mae4:0.300012\n",
      "epoch:200 loss:0.287565 accu:0.585449 |mse1:6.086166 :mse2:0.211745 mae1:0.454920 mae2:0.216730 mae3:0.245010 mae4:0.421599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:201 loss:0.270349 accu:0.601562 |mse1:5.947822 :mse2:0.375101 mae1:0.169704 mae2:0.218040 mae3:0.238469 mae4:0.387277\n",
      "epoch:202 loss:0.340926 accu:0.546143 |mse1:5.927408 :mse2:0.747594 mae1:0.367255 mae2:0.223575 mae3:0.195694 mae4:0.332640\n",
      "epoch:203 loss:0.167337 accu:0.778564 |mse1:5.674900 :mse2:0.256491 mae1:0.204516 mae2:0.235665 mae3:0.202073 mae4:0.433436\n",
      "epoch:204 loss:0.479622 accu:0.528320 |mse1:5.680304 :mse2:0.587809 mae1:0.547195 mae2:0.207305 mae3:0.181382 mae4:0.317761\n",
      "epoch:205 loss:0.241996 accu:0.626465 |mse1:5.871343 :mse2:0.505170 mae1:0.125919 mae2:0.244828 mae3:0.190576 mae4:0.436231\n",
      "epoch:206 loss:0.206743 accu:0.726807 |mse1:5.468586 :mse2:0.187272 mae1:0.339731 mae2:0.214347 mae3:0.213598 mae4:0.365368\n",
      "epoch:207 loss:0.285093 accu:0.604492 |mse1:5.789184 :mse2:0.334768 mae1:0.124828 mae2:0.280831 mae3:0.189316 mae4:0.384867\n",
      "epoch:208 loss:0.301025 accu:0.663818 |mse1:6.196558 :mse2:0.723089 mae1:0.226337 mae2:0.251693 mae3:0.193620 mae4:0.388296\n",
      "epoch:209 loss:0.321972 accu:0.564453 |mse1:5.637190 :mse2:0.190584 mae1:0.150190 mae2:0.254869 mae3:0.202031 mae4:0.394711\n",
      "epoch:210 loss:0.172793 accu:0.729248 |mse1:5.267489 :mse2:0.171171 mae1:0.673477 mae2:0.225516 mae3:0.155206 mae4:0.320498\n",
      "epoch:211 loss:0.173727 accu:0.763672 |mse1:5.620260 :mse2:0.330689 mae1:0.206151 mae2:0.231531 mae3:0.205817 mae4:0.418457\n",
      "epoch:212 loss:0.189455 accu:0.708008 |mse1:5.728382 :mse2:0.342950 mae1:0.189447 mae2:0.230370 mae3:0.213488 mae4:0.424536\n",
      "epoch:213 loss:0.236079 accu:0.677002 |mse1:5.895721 :mse2:0.149509 mae1:0.139417 mae2:0.267373 mae3:0.220782 mae4:0.364122\n",
      "epoch:214 loss:0.182955 accu:0.720703 |mse1:5.375221 :mse2:0.456833 mae1:0.425188 mae2:0.202848 mae3:0.179097 mae4:0.302657\n",
      "epoch:215 loss:0.331701 accu:0.527100 |mse1:5.307847 :mse2:0.125716 mae1:0.353914 mae2:0.236416 mae3:0.172511 mae4:0.347846\n",
      "epoch:216 loss:0.167821 accu:0.731934 |mse1:5.718085 :mse2:0.301181 mae1:0.085809 mae2:0.282968 mae3:0.176772 mae4:0.445655\n",
      "epoch:217 loss:0.171715 accu:0.767822 |mse1:5.813126 :mse2:0.168325 mae1:0.131623 mae2:0.259884 mae3:0.214658 mae4:0.420178\n",
      "epoch:218 loss:0.270921 accu:0.660400 |mse1:4.694173 :mse2:0.247014 mae1:0.052535 mae2:0.187039 mae3:0.184283 mae4:0.325939\n",
      "epoch:219 loss:0.198400 accu:0.755615 |mse1:5.405724 :mse2:0.398317 mae1:0.378090 mae2:0.232813 mae3:0.154958 mae4:0.373598\n",
      "epoch:220 loss:0.317072 accu:0.548096 |mse1:4.976572 :mse2:0.385054 mae1:0.151502 mae2:0.222253 mae3:0.163555 mae4:0.300202\n",
      "epoch:221 loss:0.209581 accu:0.683838 |mse1:5.019117 :mse2:0.355451 mae1:0.214162 mae2:0.225823 mae3:0.158855 mae4:0.306430\n",
      "epoch:222 loss:0.162082 accu:0.774658 |mse1:5.297116 :mse2:0.221127 mae1:0.086998 mae2:0.245773 mae3:0.184033 mae4:0.363328\n",
      "epoch:223 loss:0.111872 accu:0.859619 |mse1:5.756686 :mse2:0.293165 mae1:0.098057 mae2:0.233823 mae3:0.223393 mae4:0.397778\n",
      "epoch:224 loss:0.201865 accu:0.693359 |mse1:5.304457 :mse2:0.097994 mae1:0.122121 mae2:0.193555 mae3:0.240723 mae4:0.309625\n",
      "epoch:225 loss:0.179778 accu:0.749512 |mse1:4.905827 :mse2:0.372502 mae1:0.116226 mae2:0.232129 mae3:0.148580 mae4:0.355658\n",
      "epoch:226 loss:0.202034 accu:0.720947 |mse1:4.905555 :mse2:0.132481 mae1:0.206927 mae2:0.213978 mae3:0.177130 mae4:0.337738\n",
      "epoch:227 loss:0.269791 accu:0.668457 |mse1:5.418712 :mse2:0.105010 mae1:0.328889 mae2:0.199533 mae3:0.223771 mae4:0.355936\n",
      "epoch:228 loss:0.170060 accu:0.764404 |mse1:5.069875 :mse2:0.224299 mae1:0.162138 mae2:0.202331 mae3:0.191341 mae4:0.414608\n",
      "epoch:229 loss:0.301901 accu:0.499512 |mse1:4.862366 :mse2:0.337260 mae1:0.121853 mae2:0.209519 mae3:0.169986 mae4:0.326668\n",
      "epoch:230 loss:0.200395 accu:0.742188 |mse1:4.728944 :mse2:0.153522 mae1:0.082768 mae2:0.197677 mae3:0.178974 mae4:0.381585\n",
      "epoch:231 loss:0.149926 accu:0.781982 |mse1:5.079171 :mse2:0.118697 mae1:0.306879 mae2:0.214599 mae3:0.182618 mae4:0.362017\n",
      "epoch:232 loss:0.180864 accu:0.748047 |mse1:5.371374 :mse2:0.279892 mae1:0.138144 mae2:0.225828 mae3:0.192686 mae4:0.411919\n",
      "epoch:233 loss:0.104500 accu:0.876465 |mse1:5.181572 :mse2:0.300082 mae1:0.170567 mae2:0.246702 mae3:0.143835 mae4:0.446651\n",
      "epoch:234 loss:0.085700 accu:0.901123 |mse1:5.855196 :mse2:0.398227 mae1:0.467445 mae2:0.240073 mae3:0.185225 mae4:0.414654\n",
      "epoch:235 loss:0.173124 accu:0.732422 |mse1:4.493138 :mse2:0.061798 mae1:0.085912 mae2:0.209348 mae3:0.157518 mae4:0.416224\n",
      "epoch:236 loss:0.109947 accu:0.859131 |mse1:5.298537 :mse2:0.384359 mae1:0.487019 mae2:0.205992 mae3:0.163452 mae4:0.398783\n",
      "epoch:237 loss:0.474715 accu:0.406250 |mse1:5.440487 :mse2:0.546650 mae1:0.113065 mae2:0.202208 mae3:0.204199 mae4:0.325032\n",
      "epoch:238 loss:0.159932 accu:0.775879 |mse1:4.796525 :mse2:0.085843 mae1:0.080503 mae2:0.216625 mae3:0.174102 mae4:0.397331\n",
      "epoch:239 loss:0.157537 accu:0.787598 |mse1:5.438466 :mse2:0.476058 mae1:0.238970 mae2:0.237390 mae3:0.166786 mae4:0.347997\n",
      "epoch:240 loss:0.147898 accu:0.795654 |mse1:5.777285 :mse2:0.238120 mae1:0.103349 mae2:0.253596 mae3:0.213067 mae4:0.331856\n",
      "epoch:241 loss:0.142386 accu:0.801270 |mse1:5.540886 :mse2:0.195820 mae1:0.164850 mae2:0.235382 mae3:0.207739 mae4:0.376435\n",
      "epoch:242 loss:0.150522 accu:0.840332 |mse1:5.045483 :mse2:0.321733 mae1:0.118887 mae2:0.200651 mae3:0.196367 mae4:0.305087\n",
      "epoch:243 loss:0.117636 accu:0.868164 |mse1:5.524302 :mse2:0.343728 mae1:0.062697 mae2:0.261391 mae3:0.177665 mae4:0.405806\n",
      "epoch:244 loss:0.285737 accu:0.593262 |mse1:4.164353 :mse2:0.240643 mae1:0.118591 mae2:0.178146 mae3:0.139043 mae4:0.347931\n",
      "epoch:245 loss:0.153132 accu:0.793457 |mse1:5.633028 :mse2:0.159465 mae1:0.229428 mae2:0.225728 mae3:0.217863 mae4:0.367878\n",
      "epoch:246 loss:0.240002 accu:0.618408 |mse1:4.767013 :mse2:0.159970 mae1:0.082826 mae2:0.203036 mae3:0.177778 mae4:0.307217\n",
      "epoch:247 loss:0.139170 accu:0.817383 |mse1:4.867243 :mse2:0.349948 mae1:0.199595 mae2:0.209398 mae3:0.156934 mae4:0.324427\n",
      "epoch:248 loss:0.112143 accu:0.861084 |mse1:4.606304 :mse2:0.169944 mae1:0.114381 mae2:0.201318 mae3:0.161208 mae4:0.381599\n",
      "epoch:249 loss:0.208803 accu:0.729492 |mse1:4.861636 :mse2:0.157369 mae1:0.091773 mae2:0.214956 mae3:0.181508 mae4:0.347874\n",
      "epoch:250 loss:0.085883 accu:0.916992 |mse1:5.722036 :mse2:0.300503 mae1:0.458277 mae2:0.228258 mae3:0.191242 mae4:0.377293\n",
      "epoch:251 loss:0.226017 accu:0.682861 |mse1:4.410041 :mse2:0.189697 mae1:0.075866 mae2:0.195257 mae3:0.158448 mae4:0.368682\n",
      "epoch:252 loss:0.200376 accu:0.714111 |mse1:5.086840 :mse2:0.584845 mae1:0.314557 mae2:0.191721 mae3:0.156519 mae4:0.294494\n",
      "epoch:253 loss:0.314727 accu:0.671143 |mse1:4.472161 :mse2:0.127690 mae1:0.123510 mae2:0.208053 mae3:0.130798 mae4:0.417666\n",
      "epoch:254 loss:0.191399 accu:0.795410 |mse1:5.042465 :mse2:0.187112 mae1:0.137996 mae2:0.224353 mae3:0.181729 mae4:0.358600\n",
      "epoch:255 loss:0.125360 accu:0.860107 |mse1:5.095963 :mse2:0.192980 mae1:0.408022 mae2:0.222040 mae3:0.159959 mae4:0.391443\n",
      "epoch:256 loss:0.325239 accu:0.493164 |mse1:4.973090 :mse2:0.087076 mae1:0.102630 mae2:0.192908 mae3:0.213257 mae4:0.313966\n",
      "epoch:257 loss:0.197110 accu:0.737061 |mse1:5.610243 :mse2:0.173768 mae1:0.447954 mae2:0.191567 mae3:0.238910 mae4:0.362374\n",
      "epoch:258 loss:0.297543 accu:0.648682 |mse1:5.791395 :mse2:0.526730 mae1:0.725599 mae2:0.208441 mae3:0.171345 mae4:0.403850\n",
      "epoch:259 loss:0.384558 accu:0.483154 |mse1:6.068252 :mse2:0.249867 mae1:1.118423 mae2:0.220010 mae3:0.176126 mae4:0.339966\n",
      "epoch:260 loss:0.397672 accu:0.527344 |mse1:4.852356 :mse2:0.220995 mae1:0.155476 mae2:0.236664 mae3:0.145043 mae4:0.409652\n",
      "epoch:261 loss:0.132442 accu:0.818848 |mse1:4.665280 :mse2:0.221930 mae1:0.641053 mae2:0.207831 mae3:0.110661 mae4:0.344634\n",
      "epoch:262 loss:0.248508 accu:0.630127 |mse1:5.125763 :mse2:0.117525 mae1:0.145486 mae2:0.204447 mae3:0.213325 mae4:0.322842\n",
      "epoch:263 loss:0.169537 accu:0.818359 |mse1:5.369402 :mse2:0.246005 mae1:0.273233 mae2:0.240512 mae3:0.168517 mae4:0.397007\n",
      "epoch:264 loss:0.074298 accu:0.931152 |mse1:4.723363 :mse2:0.226301 mae1:0.100096 mae2:0.201539 mae3:0.164867 mae4:0.408990\n",
      "epoch:265 loss:0.170174 accu:0.761230 |mse1:5.630425 :mse2:0.160657 mae1:0.076281 mae2:0.224573 mae3:0.229281 mae4:0.351983\n",
      "epoch:266 loss:0.063745 accu:0.966309 |mse1:4.303581 :mse2:0.195604 mae1:0.313213 mae2:0.204658 mae3:0.113504 mae4:0.373658\n",
      "epoch:267 loss:0.284056 accu:0.702393 |mse1:5.238760 :mse2:0.730246 mae1:0.196783 mae2:0.193063 mae3:0.175378 mae4:0.304039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:268 loss:0.164533 accu:0.799805 |mse1:4.249792 :mse2:0.197857 mae1:0.083080 mae2:0.190145 mae3:0.144406 mae4:0.332699\n",
      "epoch:269 loss:0.126231 accu:0.831787 |mse1:5.014046 :mse2:0.142114 mae1:0.125072 mae2:0.236800 mae3:0.160255 mae4:0.367309\n",
      "epoch:270 loss:0.090514 accu:0.908203 |mse1:5.247957 :mse2:0.306515 mae1:0.091391 mae2:0.244072 mae3:0.174754 mae4:0.353092\n",
      "epoch:271 loss:0.081030 accu:0.913330 |mse1:4.152543 :mse2:0.147868 mae1:0.045064 mae2:0.186204 mae3:0.142732 mae4:0.366485\n",
      "epoch:272 loss:0.175056 accu:0.759277 |mse1:5.005109 :mse2:0.146009 mae1:0.103868 mae2:0.197908 mae3:0.207919 mae4:0.326856\n",
      "epoch:273 loss:0.231875 accu:0.648193 |mse1:4.427078 :mse2:0.231457 mae1:0.107713 mae2:0.185806 mae3:0.155548 mae4:0.337473\n",
      "epoch:274 loss:0.338398 accu:0.543213 |mse1:4.930279 :mse2:0.310727 mae1:0.055031 mae2:0.234344 mae3:0.160367 mae4:0.342835\n",
      "epoch:275 loss:0.223128 accu:0.758301 |mse1:4.005049 :mse2:0.104603 mae1:0.057066 mae2:0.155821 mae3:0.170862 mae4:0.312947\n",
      "epoch:276 loss:0.218104 accu:0.718994 |mse1:5.063858 :mse2:0.489874 mae1:0.055810 mae2:0.202251 mae3:0.181237 mae4:0.358443\n",
      "epoch:277 loss:0.129424 accu:0.822021 |mse1:4.707036 :mse2:0.148712 mae1:0.262262 mae2:0.203857 mae3:0.158826 mae4:0.344618\n",
      "epoch:278 loss:0.146899 accu:0.809326 |mse1:6.752689 :mse2:0.571761 mae1:1.011054 mae2:0.217819 mae3:0.234685 mae4:0.334101\n",
      "epoch:279 loss:0.433587 accu:0.330078 |mse1:4.622953 :mse2:0.118909 mae1:0.181355 mae2:0.205934 mae3:0.165453 mae4:0.345232\n",
      "epoch:280 loss:0.120332 accu:0.867676 |mse1:4.602912 :mse2:0.064945 mae1:0.044336 mae2:0.202084 mae3:0.187778 mae4:0.330471\n",
      "epoch:281 loss:0.140003 accu:0.825439 |mse1:4.987125 :mse2:0.106581 mae1:0.157191 mae2:0.217676 mae3:0.192328 mae4:0.339245\n",
      "epoch:282 loss:0.175393 accu:0.731445 |mse1:5.380664 :mse2:0.162055 mae1:0.089278 mae2:0.257960 mae3:0.182240 mae4:0.376825\n",
      "epoch:283 loss:0.117168 accu:0.848877 |mse1:4.789586 :mse2:0.391310 mae1:0.162742 mae2:0.201571 mae3:0.154570 mae4:0.327786\n",
      "epoch:284 loss:0.177024 accu:0.768799 |mse1:5.378729 :mse2:0.100695 mae1:0.092675 mae2:0.234106 mae3:0.216169 mae4:0.397135\n",
      "epoch:285 loss:0.101837 accu:0.877930 |mse1:4.784374 :mse2:0.163388 mae1:0.466016 mae2:0.197369 mae3:0.145438 mae4:0.330788\n",
      "epoch:286 loss:0.120925 accu:0.868408 |mse1:4.605175 :mse2:0.338268 mae1:0.346062 mae2:0.180191 mae3:0.155049 mae4:0.295943\n",
      "epoch:287 loss:0.167395 accu:0.804932 |mse1:4.711341 :mse2:0.154139 mae1:0.181406 mae2:0.209031 mae3:0.159207 mae4:0.429278\n",
      "epoch:288 loss:0.144490 accu:0.807861 |mse1:4.496250 :mse2:0.144835 mae1:0.151105 mae2:0.203807 mae3:0.153598 mae4:0.307408\n",
      "epoch:289 loss:0.168081 accu:0.797607 |mse1:4.686371 :mse2:0.060176 mae1:0.203427 mae2:0.201888 mae3:0.171288 mae4:0.314589\n",
      "epoch:290 loss:0.184262 accu:0.756836 |mse1:4.913509 :mse2:0.065032 mae1:0.070113 mae2:0.228639 mae3:0.182620 mae4:0.373606\n",
      "epoch:291 loss:0.330396 accu:0.588379 |mse1:4.745149 :mse2:0.091416 mae1:0.112017 mae2:0.219885 mae3:0.154876 mae4:0.354681\n",
      "epoch:292 loss:0.136474 accu:0.811523 |mse1:4.934654 :mse2:0.233537 mae1:0.210489 mae2:0.211195 mae3:0.176401 mae4:0.335434\n",
      "epoch:293 loss:0.203633 accu:0.765869 |mse1:4.556547 :mse2:0.107982 mae1:0.244921 mae2:0.230054 mae3:0.127594 mae4:0.359428\n",
      "epoch:294 loss:0.179059 accu:0.814453 |mse1:4.758587 :mse2:0.082159 mae1:0.130376 mae2:0.209910 mae3:0.174694 mae4:0.327474\n",
      "epoch:295 loss:0.100584 accu:0.886963 |mse1:4.746121 :mse2:0.089689 mae1:0.190233 mae2:0.200228 mae3:0.182851 mae4:0.344087\n",
      "epoch:296 loss:0.242443 accu:0.659912 |mse1:4.360085 :mse2:0.109705 mae1:0.142248 mae2:0.202304 mae3:0.149995 mae4:0.330048\n",
      "epoch:297 loss:0.121827 accu:0.859131 |mse1:4.696615 :mse2:0.179892 mae1:0.171623 mae2:0.217583 mae3:0.154858 mae4:0.318619\n",
      "epoch:298 loss:0.095499 accu:0.882080 |mse1:4.492777 :mse2:0.289966 mae1:0.287489 mae2:0.177428 mae3:0.149196 mae4:0.358240\n",
      "epoch:299 loss:0.228018 accu:0.671143 |mse1:4.403745 :mse2:0.099465 mae1:0.088371 mae2:0.176997 mae3:0.180009 mae4:0.312630\n",
      "epoch:300 loss:0.254913 accu:0.693359 |mse1:4.653690 :mse2:0.302068 mae1:0.075254 mae2:0.196530 mae3:0.164598 mae4:0.308199\n",
      "epoch:301 loss:0.164277 accu:0.774658 |mse1:4.716516 :mse2:0.093853 mae1:0.185819 mae2:0.183274 mae3:0.186645 mae4:0.331062\n",
      "epoch:302 loss:0.194316 accu:0.724609 |mse1:4.774406 :mse2:0.186019 mae1:0.146015 mae2:0.218090 mae3:0.156808 mae4:0.376155\n",
      "epoch:303 loss:0.154438 accu:0.782715 |mse1:4.847541 :mse2:0.336045 mae1:0.379409 mae2:0.197914 mae3:0.156460 mae4:0.314805\n",
      "epoch:304 loss:0.280751 accu:0.651855 |mse1:5.157667 :mse2:0.205476 mae1:0.081583 mae2:0.231798 mae3:0.181285 mae4:0.425942\n",
      "epoch:305 loss:0.175495 accu:0.748291 |mse1:5.092175 :mse2:0.340065 mae1:0.050699 mae2:0.243749 mae3:0.157335 mae4:0.362963\n",
      "epoch:306 loss:0.051059 accu:0.975586 |mse1:3.806186 :mse2:0.171708 mae1:0.057143 mae2:0.178889 mae3:0.115315 mae4:0.372081\n",
      "epoch:307 loss:0.169495 accu:0.770996 |mse1:4.321528 :mse2:0.133658 mae1:0.047807 mae2:0.210916 mae3:0.153036 mae4:0.283303\n",
      "epoch:308 loss:0.099512 accu:0.896729 |mse1:4.716312 :mse2:0.174051 mae1:0.049560 mae2:0.189887 mae3:0.186256 mae4:0.268378\n",
      "epoch:309 loss:0.123914 accu:0.830566 |mse1:5.127593 :mse2:0.163705 mae1:0.247672 mae2:0.212284 mae3:0.191300 mae4:0.336045\n",
      "epoch:310 loss:0.424585 accu:0.479736 |mse1:4.566486 :mse2:0.285186 mae1:0.035083 mae2:0.190511 mae3:0.170246 mae4:0.309738\n",
      "epoch:311 loss:0.171991 accu:0.761719 |mse1:4.996875 :mse2:0.225667 mae1:0.229545 mae2:0.225176 mae3:0.173015 mae4:0.299677\n",
      "epoch:312 loss:0.282623 accu:0.611084 |mse1:5.546643 :mse2:0.161858 mae1:0.455162 mae2:0.217059 mae3:0.202399 mae4:0.387329\n",
      "epoch:313 loss:0.195269 accu:0.764648 |mse1:5.277940 :mse2:0.202619 mae1:0.320399 mae2:0.197837 mae3:0.211961 mae4:0.297933\n",
      "epoch:314 loss:0.293628 accu:0.679688 |mse1:4.885087 :mse2:0.257696 mae1:0.081937 mae2:0.202818 mae3:0.184458 mae4:0.375470\n",
      "epoch:315 loss:0.158897 accu:0.792969 |mse1:5.089570 :mse2:0.347212 mae1:0.153958 mae2:0.232042 mae3:0.167996 mae4:0.349716\n",
      "epoch:316 loss:0.551772 accu:0.467041 |mse1:5.385664 :mse2:0.909514 mae1:0.301161 mae2:0.211386 mae3:0.149383 mae4:0.314821\n",
      "epoch:317 loss:0.309784 accu:0.684814 |mse1:4.769347 :mse2:0.466571 mae1:0.134806 mae2:0.221074 mae3:0.137665 mae4:0.330822\n",
      "epoch:318 loss:0.123102 accu:0.841309 |mse1:5.315473 :mse2:0.222626 mae1:0.046876 mae2:0.235261 mae3:0.195953 mae4:0.375274\n",
      "epoch:319 loss:0.137114 accu:0.855957 |mse1:5.196163 :mse2:0.170017 mae1:0.105635 mae2:0.210183 mae3:0.202755 mae4:0.398814\n",
      "epoch:320 loss:0.097776 accu:0.875244 |mse1:4.557281 :mse2:0.185555 mae1:0.074568 mae2:0.190893 mae3:0.172452 mae4:0.362833\n",
      "epoch:321 loss:0.107772 accu:0.854492 |mse1:4.674745 :mse2:0.202505 mae1:0.109479 mae2:0.221849 mae3:0.154144 mae4:0.339890\n",
      "epoch:322 loss:0.184485 accu:0.734619 |mse1:4.619889 :mse2:0.127445 mae1:0.098917 mae2:0.197856 mae3:0.176872 mae4:0.326829\n",
      "epoch:323 loss:0.172294 accu:0.767578 |mse1:4.150787 :mse2:0.152618 mae1:0.035209 mae2:0.173197 mae3:0.154239 mae4:0.374579\n",
      "epoch:324 loss:0.119236 accu:0.838623 |mse1:5.398928 :mse2:0.197522 mae1:0.156475 mae2:0.229585 mae3:0.203900 mae4:0.377005\n",
      "epoch:325 loss:0.156114 accu:0.765137 |mse1:4.489987 :mse2:0.111411 mae1:0.035084 mae2:0.202510 mae3:0.172760 mae4:0.297690\n",
      "epoch:326 loss:0.273480 accu:0.675293 |mse1:4.409559 :mse2:0.208216 mae1:0.205800 mae2:0.179522 mae3:0.154300 mae4:0.299563\n",
      "epoch:327 loss:0.152399 accu:0.794922 |mse1:4.266665 :mse2:0.182512 mae1:0.087268 mae2:0.185955 mae3:0.158029 mae4:0.313590\n",
      "epoch:328 loss:0.211637 accu:0.714844 |mse1:4.874826 :mse2:0.153113 mae1:0.173106 mae2:0.211010 mae3:0.168764 mae4:0.451870\n",
      "epoch:329 loss:0.067060 accu:0.956787 |mse1:3.601434 :mse2:0.142836 mae1:0.080267 mae2:0.158823 mae3:0.125956 mae4:0.290212\n",
      "epoch:330 loss:0.125550 accu:0.828857 |mse1:4.901786 :mse2:0.209142 mae1:0.033156 mae2:0.183945 mae3:0.202657 mae4:0.363476\n",
      "epoch:331 loss:0.076756 accu:0.936768 |mse1:3.973459 :mse2:0.134905 mae1:0.148306 mae2:0.182883 mae3:0.122891 mae4:0.354016\n",
      "epoch:332 loss:0.119488 accu:0.873779 |mse1:4.230954 :mse2:0.089339 mae1:0.085372 mae2:0.182928 mae3:0.151024 mae4:0.314507\n",
      "epoch:333 loss:0.104474 accu:0.881592 |mse1:4.366498 :mse2:0.178030 mae1:0.050659 mae2:0.179849 mae3:0.166015 mae4:0.347195\n",
      "epoch:334 loss:0.113772 accu:0.850586 |mse1:4.262047 :mse2:0.183901 mae1:0.040007 mae2:0.165561 mae3:0.174228 mae4:0.322998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:335 loss:0.078039 accu:0.919434 |mse1:5.206775 :mse2:0.340966 mae1:0.288020 mae2:0.185693 mae3:0.211122 mae4:0.305002\n",
      "epoch:336 loss:0.180029 accu:0.785889 |mse1:4.398787 :mse2:0.075556 mae1:0.297053 mae2:0.206176 mae3:0.136877 mae4:0.392843\n",
      "epoch:337 loss:0.106831 accu:0.873779 |mse1:4.808628 :mse2:0.145799 mae1:0.461561 mae2:0.219302 mae3:0.132967 mae4:0.363135\n",
      "epoch:338 loss:0.258622 accu:0.619873 |mse1:3.926843 :mse2:0.171676 mae1:0.116157 mae2:0.174991 mae3:0.122789 mae4:0.369955\n",
      "epoch:339 loss:0.139710 accu:0.807617 |mse1:5.213583 :mse2:0.226121 mae1:0.218685 mae2:0.232199 mae3:0.179294 mae4:0.365461\n",
      "epoch:340 loss:0.159385 accu:0.790283 |mse1:4.980995 :mse2:0.105759 mae1:0.258961 mae2:0.207224 mae3:0.179759 mae4:0.364147\n",
      "epoch:341 loss:0.177075 accu:0.805908 |mse1:4.681004 :mse2:0.094167 mae1:0.237210 mae2:0.183047 mae3:0.179821 mae4:0.349920\n",
      "epoch:342 loss:0.127980 accu:0.856934 |mse1:4.265828 :mse2:0.072668 mae1:0.069489 mae2:0.207978 mae3:0.134697 mae4:0.323950\n",
      "epoch:343 loss:0.121004 accu:0.855225 |mse1:4.467325 :mse2:0.132685 mae1:0.062304 mae2:0.195907 mae3:0.157279 mae4:0.398444\n",
      "epoch:344 loss:0.166432 accu:0.811279 |mse1:4.348153 :mse2:0.159343 mae1:0.041186 mae2:0.189752 mae3:0.153304 mae4:0.342865\n",
      "epoch:345 loss:0.084831 accu:0.918701 |mse1:3.878185 :mse2:0.120541 mae1:0.081239 mae2:0.186226 mae3:0.127789 mae4:0.286990\n",
      "epoch:346 loss:0.105322 accu:0.887207 |mse1:4.210026 :mse2:0.240903 mae1:0.037979 mae2:0.194669 mae3:0.131715 mae4:0.403569\n",
      "epoch:347 loss:0.222879 accu:0.715088 |mse1:4.937035 :mse2:0.537477 mae1:0.560304 mae2:0.168666 mae3:0.156152 mae4:0.320195\n",
      "epoch:348 loss:0.276372 accu:0.588135 |mse1:4.740720 :mse2:0.075263 mae1:0.180371 mae2:0.189166 mae3:0.177915 mae4:0.375924\n",
      "epoch:349 loss:0.139481 accu:0.804688 |mse1:4.544590 :mse2:0.109043 mae1:0.070843 mae2:0.211223 mae3:0.142988 mae4:0.362322\n",
      "epoch:350 loss:0.100314 accu:0.882080 |mse1:4.501655 :mse2:0.132861 mae1:0.035987 mae2:0.178178 mae3:0.187607 mae4:0.367653\n",
      "epoch:351 loss:0.120318 accu:0.855469 |mse1:4.323612 :mse2:0.489201 mae1:0.113204 mae2:0.186847 mae3:0.121444 mae4:0.353467\n",
      "epoch:352 loss:0.218465 accu:0.686035 |mse1:4.287478 :mse2:0.082176 mae1:0.070190 mae2:0.200870 mae3:0.143702 mae4:0.443386\n",
      "epoch:353 loss:0.187572 accu:0.788086 |mse1:4.442067 :mse2:0.076807 mae1:0.260490 mae2:0.192472 mae3:0.143987 mae4:0.423196\n",
      "epoch:354 loss:0.112625 accu:0.877686 |mse1:4.381382 :mse2:0.080265 mae1:0.052840 mae2:0.204572 mae3:0.152544 mae4:0.331699\n",
      "epoch:355 loss:0.138591 accu:0.804199 |mse1:4.599954 :mse2:0.120401 mae1:0.064872 mae2:0.177809 mae3:0.197818 mae4:0.325146\n",
      "epoch:356 loss:0.122002 accu:0.854980 |mse1:4.112496 :mse2:0.139127 mae1:0.066390 mae2:0.182238 mae3:0.145879 mae4:0.285167\n",
      "epoch:357 loss:0.092048 accu:0.902588 |mse1:4.027917 :mse2:0.425807 mae1:0.086646 mae2:0.168033 mae3:0.118425 mae4:0.393800\n",
      "epoch:358 loss:0.218908 accu:0.709961 |mse1:4.808148 :mse2:0.267191 mae1:0.202464 mae2:0.194594 mae3:0.163991 mae4:0.383130\n",
      "epoch:359 loss:0.190756 accu:0.742920 |mse1:4.548912 :mse2:0.204169 mae1:0.156871 mae2:0.202467 mae3:0.142377 mae4:0.354655\n",
      "epoch:360 loss:0.168251 accu:0.727539 |mse1:4.601649 :mse2:0.253957 mae1:0.056446 mae2:0.209990 mae3:0.150164 mae4:0.408383\n",
      "epoch:361 loss:0.132479 accu:0.794922 |mse1:5.081792 :mse2:0.226585 mae1:0.136395 mae2:0.198482 mae3:0.205604 mae4:0.369822\n",
      "epoch:362 loss:0.115674 accu:0.859619 |mse1:4.801857 :mse2:0.162612 mae1:0.241330 mae2:0.184386 mae3:0.175301 mae4:0.337431\n",
      "epoch:363 loss:0.236040 accu:0.679932 |mse1:4.877011 :mse2:0.148473 mae1:0.046197 mae2:0.176584 mae3:0.218257 mae4:0.377017\n",
      "epoch:364 loss:0.104610 accu:0.897705 |mse1:3.986378 :mse2:0.068084 mae1:0.050330 mae2:0.194359 mae3:0.128375 mae4:0.322645\n",
      "epoch:365 loss:0.161207 accu:0.762207 |mse1:3.883470 :mse2:0.077997 mae1:0.036026 mae2:0.165076 mae3:0.149767 mae4:0.348908\n",
      "epoch:366 loss:0.090927 accu:0.889160 |mse1:4.286332 :mse2:0.246858 mae1:0.170774 mae2:0.184596 mae3:0.141185 mae4:0.345216\n",
      "epoch:367 loss:0.266676 accu:0.589600 |mse1:4.521730 :mse2:0.191346 mae1:0.066424 mae2:0.206434 mae3:0.144177 mae4:0.341838\n",
      "epoch:368 loss:0.059294 accu:0.940918 |mse1:3.787947 :mse2:0.140711 mae1:0.047388 mae2:0.171286 mae3:0.124672 mae4:0.342213\n",
      "epoch:369 loss:0.066032 accu:0.942627 |mse1:4.237316 :mse2:0.108524 mae1:0.107478 mae2:0.181373 mae3:0.150920 mae4:0.343231\n",
      "epoch:370 loss:0.076009 accu:0.943848 |mse1:4.607202 :mse2:0.471088 mae1:0.226110 mae2:0.190981 mae3:0.139923 mae4:0.312857\n",
      "epoch:371 loss:0.240098 accu:0.681885 |mse1:4.536935 :mse2:0.287392 mae1:0.081981 mae2:0.171633 mae3:0.161355 mae4:0.369129\n",
      "epoch:372 loss:0.225408 accu:0.700439 |mse1:4.523062 :mse2:0.240466 mae1:0.099223 mae2:0.177694 mae3:0.165650 mae4:0.379892\n",
      "epoch:373 loss:0.085194 accu:0.918945 |mse1:4.670484 :mse2:0.045409 mae1:0.250819 mae2:0.157148 mae3:0.207921 mae4:0.324402\n",
      "epoch:374 loss:0.136601 accu:0.852295 |mse1:3.747266 :mse2:0.065906 mae1:0.048621 mae2:0.177547 mae3:0.121689 mae4:0.328395\n",
      "epoch:375 loss:0.117436 accu:0.899414 |mse1:4.066740 :mse2:0.088970 mae1:0.134243 mae2:0.194776 mae3:0.126965 mae4:0.357101\n",
      "epoch:376 loss:0.512155 accu:0.348145 |mse1:5.025867 :mse2:0.189956 mae1:0.035989 mae2:0.197555 mae3:0.202899 mae4:0.349618\n",
      "epoch:377 loss:0.134873 accu:0.807861 |mse1:4.343000 :mse2:0.099974 mae1:0.034606 mae2:0.182445 mae3:0.172042 mae4:0.335299\n",
      "epoch:378 loss:0.072649 accu:0.946533 |mse1:4.201957 :mse2:0.042940 mae1:0.281093 mae2:0.197089 mae3:0.131724 mae4:0.344432\n",
      "epoch:379 loss:0.137158 accu:0.806152 |mse1:4.835447 :mse2:0.177958 mae1:0.061452 mae2:0.200737 mae3:0.192123 mae4:0.389111\n",
      "epoch:380 loss:0.114025 accu:0.874268 |mse1:3.881465 :mse2:0.195062 mae1:0.035443 mae2:0.161559 mae3:0.139479 mae4:0.353408\n",
      "epoch:381 loss:0.131806 accu:0.810791 |mse1:4.659431 :mse2:0.160723 mae1:0.141229 mae2:0.206918 mae3:0.166937 mae4:0.318468\n",
      "epoch:382 loss:0.185901 accu:0.760010 |mse1:4.267621 :mse2:0.049797 mae1:0.134755 mae2:0.184278 mae3:0.161683 mae4:0.352125\n",
      "epoch:383 loss:0.138256 accu:0.835938 |mse1:4.023348 :mse2:0.043003 mae1:0.020167 mae2:0.193699 mae3:0.129919 mae4:0.380283\n",
      "epoch:384 loss:0.156261 accu:0.790039 |mse1:4.188294 :mse2:0.242850 mae1:0.134420 mae2:0.175759 mae3:0.133189 mae4:0.314587\n",
      "epoch:385 loss:0.150358 accu:0.788574 |mse1:4.176267 :mse2:0.179387 mae1:0.025420 mae2:0.159289 mae3:0.175427 mae4:0.280675\n",
      "epoch:386 loss:0.126886 accu:0.863281 |mse1:4.514798 :mse2:0.177149 mae1:0.056475 mae2:0.234008 mae3:0.134688 mae4:0.354915\n",
      "epoch:387 loss:0.098604 accu:0.894531 |mse1:4.116629 :mse2:0.076218 mae1:0.069492 mae2:0.190696 mae3:0.145355 mae4:0.342705\n",
      "epoch:388 loss:0.084759 accu:0.910400 |mse1:4.137844 :mse2:0.055625 mae1:0.117613 mae2:0.184398 mae3:0.142190 mae4:0.356355\n",
      "epoch:389 loss:0.091734 accu:0.921387 |mse1:3.896775 :mse2:0.069922 mae1:0.035803 mae2:0.169121 mae3:0.143065 mae4:0.321609\n",
      "epoch:390 loss:0.108298 accu:0.875732 |mse1:4.100634 :mse2:0.195858 mae1:0.136790 mae2:0.192757 mae3:0.120476 mae4:0.340474\n",
      "epoch:391 loss:0.141060 accu:0.805664 |mse1:4.127220 :mse2:0.083229 mae1:0.104602 mae2:0.186719 mae3:0.139855 mae4:0.419160\n",
      "epoch:392 loss:0.095979 accu:0.907471 |mse1:3.736835 :mse2:0.114209 mae1:0.114957 mae2:0.181284 mae3:0.114854 mae4:0.302890\n",
      "epoch:393 loss:0.153137 accu:0.814697 |mse1:4.152515 :mse2:0.213519 mae1:0.021347 mae2:0.172377 mae3:0.150413 mae4:0.380064\n",
      "epoch:394 loss:0.157468 accu:0.757568 |mse1:4.574532 :mse2:0.218455 mae1:0.041367 mae2:0.184228 mae3:0.183018 mae4:0.341823\n",
      "epoch:395 loss:0.252676 accu:0.663574 |mse1:4.443447 :mse2:0.405077 mae1:0.143304 mae2:0.207009 mae3:0.123360 mae4:0.395437\n",
      "epoch:396 loss:0.173083 accu:0.812744 |mse1:4.594999 :mse2:0.253628 mae1:0.144606 mae2:0.184350 mae3:0.167035 mae4:0.363347\n",
      "epoch:397 loss:0.146566 accu:0.842041 |mse1:3.808921 :mse2:0.404019 mae1:0.078712 mae2:0.163407 mae3:0.102090 mae4:0.372517\n",
      "epoch:398 loss:0.110785 accu:0.864014 |mse1:4.949262 :mse2:0.154314 mae1:0.081206 mae2:0.210147 mae3:0.178966 mae4:0.388976\n",
      "epoch:399 loss:0.107731 accu:0.844482 |mse1:4.437399 :mse2:0.084735 mae1:0.029422 mae2:0.183851 mae3:0.182425 mae4:0.334786\n",
      "epoch:400 loss:0.151922 accu:0.789307 |mse1:5.578771 :mse2:0.360170 mae1:0.050178 mae2:0.205139 mae3:0.241953 mae4:0.363419\n",
      "epoch:401 loss:0.119074 accu:0.856201 |mse1:3.931867 :mse2:0.088868 mae1:0.112470 mae2:0.179197 mae3:0.135669 mae4:0.282676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:402 loss:0.240223 accu:0.712891 |mse1:4.516990 :mse2:0.086623 mae1:0.076694 mae2:0.196409 mae3:0.168327 mae4:0.356813\n",
      "epoch:403 loss:0.088381 accu:0.905273 |mse1:4.340602 :mse2:0.030328 mae1:0.056343 mae2:0.191264 mae3:0.162268 mae4:0.334810\n",
      "epoch:404 loss:0.134387 accu:0.827637 |mse1:4.240595 :mse2:0.075878 mae1:0.223017 mae2:0.169021 mae3:0.150024 mae4:0.381186\n",
      "epoch:405 loss:0.204746 accu:0.703613 |mse1:4.087601 :mse2:0.038726 mae1:0.052267 mae2:0.170851 mae3:0.169647 mae4:0.307627\n",
      "epoch:406 loss:0.094201 accu:0.903320 |mse1:3.860641 :mse2:0.333349 mae1:0.044540 mae2:0.155983 mae3:0.128935 mae4:0.358571\n",
      "epoch:407 loss:0.137231 accu:0.821777 |mse1:4.928768 :mse2:0.117877 mae1:0.075174 mae2:0.214002 mae3:0.191581 mae4:0.401265\n",
      "epoch:408 loss:0.077379 accu:0.923096 |mse1:4.219263 :mse2:0.164768 mae1:0.038613 mae2:0.179752 mae3:0.146210 mae4:0.402045\n",
      "epoch:409 loss:0.112248 accu:0.895752 |mse1:4.614598 :mse2:0.274076 mae1:0.060593 mae2:0.208025 mae3:0.154575 mae4:0.387652\n",
      "epoch:410 loss:0.150793 accu:0.797607 |mse1:3.861725 :mse2:0.149312 mae1:0.081511 mae2:0.176668 mae3:0.119942 mae4:0.372875\n",
      "epoch:411 loss:0.138181 accu:0.799072 |mse1:4.145545 :mse2:0.257934 mae1:0.089942 mae2:0.167447 mae3:0.140354 mae4:0.312424\n",
      "epoch:412 loss:0.094305 accu:0.911133 |mse1:3.911360 :mse2:0.111575 mae1:0.092619 mae2:0.147831 mae3:0.157398 mae4:0.334523\n",
      "epoch:413 loss:0.234031 accu:0.692627 |mse1:4.515808 :mse2:0.059407 mae1:0.094785 mae2:0.194935 mae3:0.168016 mae4:0.420118\n",
      "epoch:414 loss:0.091696 accu:0.901611 |mse1:4.962045 :mse2:0.164901 mae1:0.062210 mae2:0.170838 mae3:0.220960 mae4:0.355053\n",
      "epoch:415 loss:0.147650 accu:0.780518 |mse1:4.538384 :mse2:0.423300 mae1:0.189476 mae2:0.170973 mae3:0.158028 mae4:0.361676\n",
      "epoch:416 loss:0.197490 accu:0.720947 |mse1:4.371133 :mse2:0.072098 mae1:0.093352 mae2:0.184563 mae3:0.162120 mae4:0.343001\n",
      "epoch:417 loss:0.122031 accu:0.841064 |mse1:4.180707 :mse2:0.054020 mae1:0.218686 mae2:0.166881 mae3:0.153745 mae4:0.304556\n",
      "epoch:418 loss:0.238774 accu:0.599609 |mse1:4.341528 :mse2:0.082487 mae1:0.042477 mae2:0.211518 mae3:0.142077 mae4:0.372287\n",
      "epoch:419 loss:0.159797 accu:0.765869 |mse1:4.071108 :mse2:0.120018 mae1:0.102343 mae2:0.169020 mae3:0.153673 mae4:0.301616\n",
      "epoch:420 loss:0.091143 accu:0.900879 |mse1:4.209496 :mse2:0.068844 mae1:0.078737 mae2:0.163213 mae3:0.178708 mae4:0.342099\n",
      "epoch:421 loss:0.108203 accu:0.914062 |mse1:3.960280 :mse2:0.082756 mae1:0.080546 mae2:0.183200 mae3:0.130400 mae4:0.346084\n",
      "epoch:422 loss:0.197085 accu:0.791260 |mse1:4.050841 :mse2:0.219836 mae1:0.150663 mae2:0.168453 mae3:0.133514 mae4:0.315584\n",
      "epoch:423 loss:0.098606 accu:0.877441 |mse1:3.710629 :mse2:0.110176 mae1:0.023998 mae2:0.179294 mae3:0.111913 mae4:0.355832\n",
      "epoch:424 loss:0.035864 accu:0.985596 |mse1:3.969107 :mse2:0.050788 mae1:0.017135 mae2:0.164114 mae3:0.163284 mae4:0.384389\n",
      "epoch:425 loss:0.062003 accu:0.967041 |mse1:4.155193 :mse2:0.052723 mae1:0.082483 mae2:0.185601 mae3:0.152220 mae4:0.323252\n",
      "epoch:426 loss:0.122423 accu:0.893799 |mse1:4.026958 :mse2:0.059662 mae1:0.091818 mae2:0.188066 mae3:0.138887 mae4:0.323873\n",
      "epoch:427 loss:0.099971 accu:0.880371 |mse1:4.160508 :mse2:0.071300 mae1:0.086427 mae2:0.165479 mae3:0.174200 mae4:0.345479\n",
      "epoch:428 loss:0.298209 accu:0.542725 |mse1:4.600084 :mse2:0.051234 mae1:0.259036 mae2:0.163498 mae3:0.189219 mae4:0.333216\n",
      "epoch:429 loss:0.284511 accu:0.636230 |mse1:4.221532 :mse2:0.039562 mae1:0.061414 mae2:0.176183 mae3:0.174814 mae4:0.301213\n",
      "epoch:430 loss:0.147918 accu:0.814209 |mse1:3.755061 :mse2:0.173022 mae1:0.292547 mae2:0.155162 mae3:0.117487 mae4:0.281198\n",
      "epoch:431 loss:0.062737 accu:0.951416 |mse1:3.649926 :mse2:0.040844 mae1:0.024191 mae2:0.162585 mae3:0.133355 mae4:0.332716\n",
      "epoch:432 loss:0.067193 accu:0.919922 |mse1:4.020597 :mse2:0.027585 mae1:0.075845 mae2:0.170639 mae3:0.150974 mae4:0.340731\n",
      "epoch:433 loss:0.219221 accu:0.711182 |mse1:4.071789 :mse2:0.102809 mae1:0.193254 mae2:0.158690 mae3:0.152591 mae4:0.346539\n",
      "epoch:434 loss:0.081549 accu:0.932861 |mse1:3.920605 :mse2:0.052530 mae1:0.049463 mae2:0.184899 mae3:0.133073 mae4:0.308204\n",
      "epoch:435 loss:0.127826 accu:0.836914 |mse1:4.358340 :mse2:0.126265 mae1:0.061453 mae2:0.194864 mae3:0.158021 mae4:0.400424\n",
      "epoch:436 loss:0.094955 accu:0.890869 |mse1:4.759877 :mse2:0.606349 mae1:0.117765 mae2:0.188366 mae3:0.150303 mae4:0.332325\n",
      "epoch:437 loss:0.220210 accu:0.681396 |mse1:5.450635 :mse2:0.105070 mae1:0.556613 mae2:0.207599 mae3:0.202670 mae4:0.333458\n",
      "epoch:438 loss:0.213335 accu:0.710693 |mse1:4.411026 :mse2:0.043909 mae1:0.076131 mae2:0.208457 mae3:0.153447 mae4:0.359464\n",
      "epoch:439 loss:0.108513 accu:0.875244 |mse1:4.318804 :mse2:0.200680 mae1:0.041466 mae2:0.186037 mae3:0.155494 mae4:0.363078\n",
      "epoch:440 loss:0.066499 accu:0.966064 |mse1:3.674153 :mse2:0.148809 mae1:0.061352 mae2:0.131443 mae3:0.156083 mae4:0.338383\n",
      "epoch:441 loss:0.126422 accu:0.850342 |mse1:4.156024 :mse2:0.354272 mae1:0.212163 mae2:0.161204 mae3:0.130763 mae4:0.394516\n",
      "epoch:442 loss:0.159729 accu:0.793457 |mse1:4.297441 :mse2:0.127536 mae1:0.195394 mae2:0.155752 mae3:0.175240 mae4:0.374685\n",
      "epoch:443 loss:0.162225 accu:0.771484 |mse1:3.820528 :mse2:0.313824 mae1:0.224344 mae2:0.144355 mae3:0.130256 mae4:0.288731\n",
      "epoch:444 loss:0.151567 accu:0.797363 |mse1:3.492965 :mse2:0.249444 mae1:0.067703 mae2:0.163836 mae3:0.106370 mae4:0.284905\n",
      "epoch:445 loss:0.127658 accu:0.834229 |mse1:3.930325 :mse2:0.085643 mae1:0.053189 mae2:0.178496 mae3:0.143224 mae4:0.335342\n",
      "epoch:446 loss:0.103455 accu:0.871338 |mse1:3.628724 :mse2:0.180695 mae1:0.073675 mae2:0.168930 mae3:0.111327 mae4:0.304187\n",
      "epoch:447 loss:0.305553 accu:0.535889 |mse1:4.342247 :mse2:0.088244 mae1:0.109029 mae2:0.170063 mae3:0.177964 mae4:0.345029\n",
      "epoch:448 loss:0.137155 accu:0.847168 |mse1:4.072050 :mse2:0.037923 mae1:0.117328 mae2:0.166294 mae3:0.164389 mae4:0.333738\n",
      "epoch:449 loss:0.116192 accu:0.875977 |mse1:4.022183 :mse2:0.230717 mae1:0.359044 mae2:0.174719 mae3:0.110454 mae4:0.336818\n",
      "epoch:450 loss:0.219126 accu:0.694580 |mse1:3.899821 :mse2:0.284975 mae1:0.085554 mae2:0.167016 mae3:0.118039 mae4:0.377181\n",
      "epoch:451 loss:0.084741 accu:0.866211 |mse1:4.010446 :mse2:0.127620 mae1:0.046949 mae2:0.154556 mae3:0.156921 mae4:0.377887\n",
      "epoch:452 loss:0.139523 accu:0.845703 |mse1:4.032193 :mse2:0.192229 mae1:0.070547 mae2:0.171851 mae3:0.142136 mae4:0.392823\n",
      "epoch:453 loss:0.171111 accu:0.819092 |mse1:3.731451 :mse2:0.113267 mae1:0.041870 mae2:0.153107 mae3:0.143367 mae4:0.326410\n",
      "epoch:454 loss:0.106290 accu:0.851074 |mse1:4.140837 :mse2:0.037068 mae1:0.198747 mae2:0.173873 mae3:0.154488 mae4:0.338308\n",
      "epoch:455 loss:0.115488 accu:0.862305 |mse1:3.852502 :mse2:0.013379 mae1:0.194919 mae2:0.163858 mae3:0.138367 mae4:0.361699\n",
      "epoch:456 loss:0.122412 accu:0.856201 |mse1:3.948034 :mse2:0.023584 mae1:0.165305 mae2:0.167186 mae3:0.148965 mae4:0.331721\n",
      "epoch:457 loss:0.198301 accu:0.765381 |mse1:4.263037 :mse2:0.036852 mae1:0.463643 mae2:0.175954 mae3:0.138746 mae4:0.380881\n",
      "epoch:458 loss:0.166754 accu:0.816162 |mse1:4.040079 :mse2:0.037167 mae1:0.097559 mae2:0.189027 mae3:0.140998 mae4:0.340009\n",
      "epoch:459 loss:0.058196 accu:0.939697 |mse1:4.724608 :mse2:0.011711 mae1:0.027806 mae2:0.216614 mae3:0.182377 mae4:0.440219\n",
      "epoch:460 loss:0.122641 accu:0.858887 |mse1:5.315369 :mse2:0.431186 mae1:0.174314 mae2:0.206319 mae3:0.197239 mae4:0.330432\n",
      "epoch:461 loss:0.218850 accu:0.746582 |mse1:4.289808 :mse2:0.205402 mae1:0.077869 mae2:0.194201 mae3:0.144292 mae4:0.392773\n",
      "epoch:462 loss:0.167751 accu:0.833252 |mse1:4.731211 :mse2:0.218072 mae1:0.084720 mae2:0.155802 mae3:0.214688 mae4:0.381532\n",
      "epoch:463 loss:0.193518 accu:0.719971 |mse1:4.307650 :mse2:0.393593 mae1:0.064138 mae2:0.164052 mae3:0.149793 mae4:0.338200\n",
      "epoch:464 loss:0.203901 accu:0.692871 |mse1:3.689788 :mse2:0.147658 mae1:0.146199 mae2:0.145569 mae3:0.138746 mae4:0.312481\n",
      "epoch:465 loss:0.196672 accu:0.797363 |mse1:4.907051 :mse2:0.186046 mae1:0.457611 mae2:0.192084 mae3:0.164912 mae4:0.433443\n",
      "epoch:466 loss:0.208529 accu:0.699463 |mse1:4.153705 :mse2:0.025772 mae1:0.045621 mae2:0.190476 mae3:0.159135 mae4:0.330644\n",
      "epoch:467 loss:0.111167 accu:0.865479 |mse1:4.162731 :mse2:0.130549 mae1:0.135474 mae2:0.164590 mae3:0.152166 mae4:0.378098\n",
      "epoch:468 loss:0.098594 accu:0.909180 |mse1:4.273175 :mse2:0.219543 mae1:0.185494 mae2:0.178745 mae3:0.139701 mae4:0.351845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:469 loss:0.101679 accu:0.886230 |mse1:3.444670 :mse2:0.035601 mae1:0.023807 mae2:0.158107 mae3:0.108852 mae4:0.402276\n",
      "epoch:470 loss:0.069918 accu:0.928955 |mse1:4.155539 :mse2:0.109302 mae1:0.115334 mae2:0.187869 mae3:0.132200 mae4:0.379715\n",
      "epoch:471 loss:0.063461 accu:0.951660 |mse1:3.445394 :mse2:0.036643 mae1:0.081210 mae2:0.163773 mae3:0.113272 mae4:0.338312\n",
      "epoch:472 loss:0.067201 accu:0.973633 |mse1:3.533886 :mse2:0.087246 mae1:0.090772 mae2:0.144541 mae3:0.124651 mae4:0.355955\n",
      "epoch:473 loss:0.083144 accu:0.927490 |mse1:4.140047 :mse2:0.180140 mae1:0.303930 mae2:0.158329 mae3:0.131335 mae4:0.374767\n",
      "epoch:474 loss:0.182284 accu:0.715820 |mse1:4.128108 :mse2:0.018370 mae1:0.123468 mae2:0.180616 mae3:0.155024 mae4:0.318852\n",
      "epoch:475 loss:0.173770 accu:0.764893 |mse1:4.904886 :mse2:0.589378 mae1:0.481618 mae2:0.187578 mae3:0.129387 mae4:0.316367\n",
      "epoch:476 loss:0.181815 accu:0.751709 |mse1:5.236001 :mse2:0.437786 mae1:0.494226 mae2:0.169268 mae3:0.198438 mae4:0.350332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ae31dd5eac31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#训练生成器\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mgenerator_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mapples_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0moranges_\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreal_labels\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mreal_labels\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mapples_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0moranges_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mapples_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0moranges_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch:%d loss:%f accu:%f |mse1:%f :mse2:%f mae1:%f mae2:%f mae3:%f mae4:%f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mgenerator_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tuple类型相加 相当于cat连接\n",
    "real_labels = np.ones(shape=(BATCH_SIZE , )+disc_patch) \n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , )+disc_patch)\n",
    "\n",
    "\n",
    "for i in range(1000001):\n",
    "    apples_ , oranges_ = load_image()\n",
    "    \n",
    "    fake_apples_ = generator_apple2orange.predict(apples_) #使用G将苹果变成 橘子风格的苹果\n",
    "    fake_oranges_ = generator_orange2apple.predict(oranges_) #使用F将橘子变成 苹果风格的橘子\n",
    "    #训练判别器\n",
    "    apple_loss = discriminator_apple.train_on_batch(apples_ , real_labels)\n",
    "    fake_apple_loss = discriminator_apple.train_on_batch(fake_apples_ , fake_labels)\n",
    "    loss_apple = np.add(apple_loss , fake_apple_loss)/2\n",
    "\n",
    "    orange_loss = discriminator_orange.train_on_batch(oranges_ , real_labels)\n",
    "    fake_orange_loss = discriminator_orange.train_on_batch(fake_oranges_ , fake_labels)\n",
    "    loss_orange = np.add(orange_loss , fake_orange_loss)/2\n",
    "\n",
    "    loss = np.add(loss_apple , loss_orange)/2\n",
    "\n",
    "    #训练生成器\n",
    "    generator_loss = combined.train_on_batch([apples_ , oranges_] , [real_labels , real_labels , apples_ , oranges_ , apples_ , oranges_])\n",
    "    \n",
    "    print('epoch:%d loss:%f accu:%f |mse1:%f :mse2:%f mae1:%f mae2:%f mae3:%f mae4:%f' % (i , loss[0] , loss[1] , generator_loss[0] , generator_loss[1] , generator_loss[2] , generator_loss[3] , generator_loss[4] , generator_loss[5]))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        write_image(i)\n",
    "\n",
    "write_image(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
