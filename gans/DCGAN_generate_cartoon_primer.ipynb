{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 96\n",
    "HEIGHT = 96\n",
    "CHANNEL = 3\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = 'faces/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(X_train , y_train),(X_test , y_test) = mnist.load_data()\n",
    "X_train = X_train/127.5-1\n",
    "X_train = np.expand_dims(X_train , 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def load_mnist():\n",
    "    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\n",
    "    \n",
    "def write_image_mnist(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = generated_image*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('mnist_dcgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('generated_faces_dcgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "    \n",
    "    #cartoon 图像使用 96*96*3\n",
    "    model.add(Dense(128 * 6 * 6, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "    model.add(Reshape((6, 6, 128)))\n",
    "    model.add(UpSampling2D()) #12\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D()) #24\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D()) #48\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D()) #96\n",
    "    \n",
    "    model.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #\n",
    "    model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    image = model(noise)\n",
    "    \n",
    "    return Model(noise , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='discriminator')\n",
    "    \n",
    "    model.add(Conv2D(filters=32 , kernel_size=(3,3) , strides=(2,2) , padding='same' , input_shape=(WIDTH , HEIGHT , CHANNEL) , name='conv1'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    #model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(filters=64 , kernel_size=(3,3) , strides=(2,2) , padding='same' , name='conv2'))\n",
    "    model.add(ZeroPadding2D(padding=((0,1) , (0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters=128 , kernel_size=(3,3) , strides=(2,2) , padding='same' , name='conv3'))\n",
    "    model.add(BatchNormalization(momentum=0.8))  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    \n",
    "    model.add(Conv2D(filters=256 , kernel_size=(3,3) , strides=(1,1) , name='conv4'))\n",
    "    model.add(BatchNormalization(momentum=0.8))  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=1 , activation='sigmoid' , name='dense16'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    validity = model(image)\n",
    "    \n",
    "    return Model(image , validity , name='discriminator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_model(generator_i , discriminator_i):\n",
    "    #生成器和判别器组合成整体\n",
    "    z = Input(shape=(LATENT_DIM , ) , name='z')\n",
    "    \n",
    "    image = generator_i(z)\n",
    "    \n",
    "    #print(image.shape)\n",
    "    \n",
    "    discriminator_i.trainable = False\n",
    "    validity = discriminator_i(image)\n",
    "    \n",
    "    return Model(z , validity , name='combined_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25, 25, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 11, 11, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 11, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30976)             0         \n",
      "_________________________________________________________________\n",
      "dense16 (Dense)              (None, 1)                 30977     \n",
      "=================================================================\n",
      "Total params: 421,185\n",
      "Trainable params: 420,289\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4608)              465408    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 96, 96, 3)         867       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 96, 96, 3)         0         \n",
      "=================================================================\n",
      "Total params: 707,011\n",
      "Trainable params: 706,563\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i = discriminator()\n",
    "discriminator_i.compile(optimizer=adam , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator()\n",
    "\n",
    "combined_model_i = combined_model(generator_i , discriminator_i)\n",
    "\n",
    "\n",
    "combined_model_i.compile(optimizer=adam , loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:1.446787 accu:0.320312 gene_loss:0.455436\n",
      "epoch:1 loss:0.731037 accu:0.585938 gene_loss:1.398017\n",
      "epoch:2 loss:0.594655 accu:0.718750 gene_loss:2.022584\n",
      "epoch:3 loss:0.446518 accu:0.820312 gene_loss:1.710653\n",
      "epoch:4 loss:0.424779 accu:0.812500 gene_loss:2.188331\n",
      "epoch:5 loss:0.405232 accu:0.835938 gene_loss:2.421367\n",
      "epoch:6 loss:0.213117 accu:0.937500 gene_loss:3.141307\n",
      "epoch:7 loss:0.314316 accu:0.882812 gene_loss:2.807391\n",
      "epoch:8 loss:0.371971 accu:0.820312 gene_loss:2.587402\n",
      "epoch:9 loss:0.268522 accu:0.882812 gene_loss:3.041912\n",
      "epoch:10 loss:0.220139 accu:0.945312 gene_loss:3.102090\n",
      "epoch:11 loss:0.349809 accu:0.867188 gene_loss:2.574595\n",
      "epoch:12 loss:0.375242 accu:0.773438 gene_loss:3.276208\n",
      "epoch:13 loss:0.251004 accu:0.906250 gene_loss:3.350988\n",
      "epoch:14 loss:0.400142 accu:0.812500 gene_loss:3.600238\n",
      "epoch:15 loss:0.332151 accu:0.859375 gene_loss:2.285674\n",
      "epoch:16 loss:0.269793 accu:0.882812 gene_loss:1.184132\n",
      "epoch:17 loss:0.155001 accu:0.945312 gene_loss:0.612140\n",
      "epoch:18 loss:0.062736 accu:0.992188 gene_loss:0.236561\n",
      "epoch:19 loss:0.077735 accu:0.976562 gene_loss:0.283356\n",
      "epoch:20 loss:0.057761 accu:1.000000 gene_loss:0.393756\n",
      "epoch:21 loss:0.098102 accu:0.976562 gene_loss:0.544851\n",
      "epoch:22 loss:0.241673 accu:0.937500 gene_loss:1.019450\n",
      "epoch:23 loss:0.152024 accu:0.953125 gene_loss:1.369567\n",
      "epoch:24 loss:0.349303 accu:0.828125 gene_loss:2.816024\n",
      "epoch:25 loss:0.888070 accu:0.531250 gene_loss:4.304397\n",
      "epoch:26 loss:0.847216 accu:0.609375 gene_loss:3.059243\n",
      "epoch:27 loss:0.386325 accu:0.867188 gene_loss:1.871550\n",
      "epoch:28 loss:0.209264 accu:0.914062 gene_loss:0.451428\n",
      "epoch:29 loss:0.128351 accu:0.968750 gene_loss:0.200137\n",
      "epoch:30 loss:0.077695 accu:0.984375 gene_loss:0.101838\n",
      "epoch:31 loss:0.175966 accu:0.945312 gene_loss:0.286728\n",
      "epoch:32 loss:0.148657 accu:0.953125 gene_loss:0.329528\n",
      "epoch:33 loss:0.321551 accu:0.875000 gene_loss:1.042557\n",
      "epoch:34 loss:0.397656 accu:0.812500 gene_loss:2.032790\n",
      "epoch:35 loss:1.758985 accu:0.390625 gene_loss:5.792984\n",
      "epoch:36 loss:1.272573 accu:0.515625 gene_loss:2.055250\n",
      "epoch:37 loss:0.498299 accu:0.765625 gene_loss:0.188287\n",
      "epoch:38 loss:0.107855 accu:0.968750 gene_loss:0.056799\n",
      "epoch:39 loss:0.047683 accu:0.984375 gene_loss:0.026168\n",
      "epoch:40 loss:0.062026 accu:0.976562 gene_loss:0.070110\n",
      "epoch:41 loss:0.061964 accu:0.976562 gene_loss:0.080185\n",
      "epoch:42 loss:0.083466 accu:0.992188 gene_loss:0.109353\n",
      "epoch:43 loss:0.085059 accu:0.992188 gene_loss:0.445180\n",
      "epoch:44 loss:0.216155 accu:0.937500 gene_loss:1.031561\n",
      "epoch:45 loss:0.437629 accu:0.796875 gene_loss:1.891311\n",
      "epoch:46 loss:0.482395 accu:0.742188 gene_loss:2.301626\n",
      "epoch:47 loss:0.631729 accu:0.640625 gene_loss:3.657080\n",
      "epoch:48 loss:0.455976 accu:0.804688 gene_loss:2.035986\n",
      "epoch:49 loss:0.653678 accu:0.679688 gene_loss:3.707108\n",
      "epoch:50 loss:0.385967 accu:0.804688 gene_loss:4.408876\n",
      "epoch:51 loss:1.765013 accu:0.335938 gene_loss:3.497328\n",
      "epoch:52 loss:0.972134 accu:0.570312 gene_loss:1.051342\n",
      "epoch:53 loss:0.787793 accu:0.703125 gene_loss:0.562756\n",
      "epoch:54 loss:0.208114 accu:0.914062 gene_loss:0.850365\n",
      "epoch:55 loss:0.247789 accu:0.898438 gene_loss:2.092245\n",
      "epoch:56 loss:0.169716 accu:0.945312 gene_loss:3.124675\n",
      "epoch:57 loss:0.570839 accu:0.695312 gene_loss:4.792726\n",
      "epoch:58 loss:0.299259 accu:0.867188 gene_loss:4.022982\n",
      "epoch:59 loss:1.108066 accu:0.492188 gene_loss:5.138880\n",
      "epoch:60 loss:0.961499 accu:0.593750 gene_loss:2.202915\n",
      "epoch:61 loss:1.139978 accu:0.468750 gene_loss:1.043286\n",
      "epoch:62 loss:0.621795 accu:0.734375 gene_loss:1.522700\n",
      "epoch:63 loss:0.453391 accu:0.781250 gene_loss:1.638885\n",
      "epoch:64 loss:0.292007 accu:0.890625 gene_loss:1.797184\n",
      "epoch:65 loss:0.389138 accu:0.781250 gene_loss:3.062570\n",
      "epoch:66 loss:0.398320 accu:0.804688 gene_loss:3.778272\n",
      "epoch:67 loss:0.873819 accu:0.515625 gene_loss:6.541223\n",
      "epoch:68 loss:0.799047 accu:0.664062 gene_loss:3.305101\n",
      "epoch:69 loss:0.524875 accu:0.773438 gene_loss:3.725283\n",
      "epoch:70 loss:0.370490 accu:0.843750 gene_loss:2.080696\n",
      "epoch:71 loss:0.337132 accu:0.851562 gene_loss:3.078199\n",
      "epoch:72 loss:0.362932 accu:0.843750 gene_loss:2.960031\n",
      "epoch:73 loss:0.891789 accu:0.617188 gene_loss:3.335458\n",
      "epoch:74 loss:1.351201 accu:0.398438 gene_loss:4.510293\n",
      "epoch:75 loss:0.839493 accu:0.625000 gene_loss:4.269552\n",
      "epoch:76 loss:0.729085 accu:0.617188 gene_loss:5.401917\n",
      "epoch:77 loss:0.767027 accu:0.664062 gene_loss:4.077268\n",
      "epoch:78 loss:0.852926 accu:0.617188 gene_loss:2.325636\n",
      "epoch:79 loss:0.272114 accu:0.906250 gene_loss:1.232817\n",
      "epoch:80 loss:0.236141 accu:0.898438 gene_loss:0.961657\n",
      "epoch:81 loss:0.111298 accu:0.960938 gene_loss:0.792635\n",
      "epoch:82 loss:0.094667 accu:0.976562 gene_loss:0.488275\n",
      "epoch:83 loss:0.116252 accu:0.960938 gene_loss:0.811295\n",
      "epoch:84 loss:0.119384 accu:0.953125 gene_loss:1.960100\n",
      "epoch:85 loss:0.343708 accu:0.828125 gene_loss:3.859289\n",
      "epoch:86 loss:1.288917 accu:0.500000 gene_loss:7.674748\n",
      "epoch:87 loss:1.113364 accu:0.562500 gene_loss:4.946444\n",
      "epoch:88 loss:0.435029 accu:0.781250 gene_loss:2.638354\n",
      "epoch:89 loss:0.133042 accu:0.945312 gene_loss:0.923538\n",
      "epoch:90 loss:0.203717 accu:0.914062 gene_loss:0.298112\n",
      "epoch:91 loss:0.144524 accu:0.945312 gene_loss:0.348896\n",
      "epoch:92 loss:0.129580 accu:0.976562 gene_loss:0.271687\n",
      "epoch:93 loss:0.182571 accu:0.914062 gene_loss:0.902498\n",
      "epoch:94 loss:0.100762 accu:0.960938 gene_loss:1.419295\n",
      "epoch:95 loss:0.181527 accu:0.937500 gene_loss:1.244623\n",
      "epoch:96 loss:0.311533 accu:0.890625 gene_loss:1.992816\n",
      "epoch:97 loss:0.176873 accu:0.953125 gene_loss:2.192870\n",
      "epoch:98 loss:0.267423 accu:0.890625 gene_loss:2.142122\n",
      "epoch:99 loss:0.558341 accu:0.734375 gene_loss:3.992790\n",
      "epoch:100 loss:0.763595 accu:0.601562 gene_loss:4.156218\n",
      "epoch:101 loss:1.086522 accu:0.523438 gene_loss:4.476314\n",
      "epoch:102 loss:0.906790 accu:0.554688 gene_loss:2.367074\n",
      "epoch:103 loss:0.340671 accu:0.859375 gene_loss:1.284036\n",
      "epoch:104 loss:0.291085 accu:0.914062 gene_loss:0.655421\n",
      "epoch:105 loss:0.124907 accu:0.953125 gene_loss:1.019212\n",
      "epoch:106 loss:0.145176 accu:0.945312 gene_loss:1.259571\n",
      "epoch:107 loss:0.184305 accu:0.945312 gene_loss:1.585125\n",
      "epoch:108 loss:0.595835 accu:0.703125 gene_loss:2.819516\n",
      "epoch:109 loss:0.566916 accu:0.742188 gene_loss:3.446848\n",
      "epoch:110 loss:1.176697 accu:0.453125 gene_loss:3.924840\n",
      "epoch:111 loss:0.714713 accu:0.703125 gene_loss:2.880635\n",
      "epoch:112 loss:0.177554 accu:0.945312 gene_loss:1.274808\n",
      "epoch:113 loss:0.311486 accu:0.882812 gene_loss:0.644426\n",
      "epoch:114 loss:0.152991 accu:0.960938 gene_loss:0.248609\n",
      "epoch:115 loss:0.115669 accu:0.968750 gene_loss:0.472755\n",
      "epoch:116 loss:0.248528 accu:0.914062 gene_loss:0.396544\n",
      "epoch:117 loss:0.106473 accu:0.960938 gene_loss:0.618173\n",
      "epoch:118 loss:0.241854 accu:0.882812 gene_loss:1.585828\n",
      "epoch:119 loss:0.402830 accu:0.835938 gene_loss:2.743086\n",
      "epoch:120 loss:2.396611 accu:0.210938 gene_loss:4.930136\n",
      "epoch:121 loss:1.312129 accu:0.476562 gene_loss:3.445263\n",
      "epoch:122 loss:1.111352 accu:0.484375 gene_loss:3.221030\n",
      "epoch:123 loss:0.563275 accu:0.796875 gene_loss:1.600469\n",
      "epoch:124 loss:0.269092 accu:0.875000 gene_loss:2.455561\n",
      "epoch:125 loss:0.316285 accu:0.875000 gene_loss:3.092566\n",
      "epoch:126 loss:0.306991 accu:0.898438 gene_loss:2.996861\n",
      "epoch:127 loss:0.680660 accu:0.687500 gene_loss:3.396264\n",
      "epoch:128 loss:0.560443 accu:0.796875 gene_loss:2.032196\n",
      "epoch:129 loss:0.414385 accu:0.820312 gene_loss:1.174865\n",
      "epoch:130 loss:0.310262 accu:0.875000 gene_loss:0.476221\n",
      "epoch:131 loss:0.279141 accu:0.851562 gene_loss:0.878692\n",
      "epoch:132 loss:0.118605 accu:0.968750 gene_loss:1.317979\n",
      "epoch:133 loss:0.281134 accu:0.898438 gene_loss:1.212774\n",
      "epoch:134 loss:0.583414 accu:0.726562 gene_loss:1.237779\n",
      "epoch:135 loss:0.646312 accu:0.710938 gene_loss:1.826916\n",
      "epoch:136 loss:0.906135 accu:0.539062 gene_loss:1.946953\n",
      "epoch:137 loss:0.466771 accu:0.773438 gene_loss:1.554790\n",
      "epoch:138 loss:0.636165 accu:0.710938 gene_loss:1.461144\n",
      "epoch:139 loss:0.317349 accu:0.882812 gene_loss:1.372814\n",
      "epoch:140 loss:0.174709 accu:0.937500 gene_loss:1.278736\n",
      "epoch:141 loss:0.697697 accu:0.617188 gene_loss:2.150733\n",
      "epoch:142 loss:0.355626 accu:0.843750 gene_loss:1.976325\n",
      "epoch:143 loss:0.992658 accu:0.546875 gene_loss:3.482660\n",
      "epoch:144 loss:0.166161 accu:0.953125 gene_loss:3.555258\n",
      "epoch:145 loss:1.107579 accu:0.515625 gene_loss:4.243501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:146 loss:0.488054 accu:0.789062 gene_loss:2.171204\n",
      "epoch:147 loss:0.219134 accu:0.914062 gene_loss:1.223020\n",
      "epoch:148 loss:0.190821 accu:0.953125 gene_loss:1.427864\n",
      "epoch:149 loss:0.150101 accu:0.945312 gene_loss:1.784628\n",
      "epoch:150 loss:0.158958 accu:0.937500 gene_loss:0.559735\n",
      "epoch:151 loss:0.166552 accu:0.929688 gene_loss:0.200103\n",
      "epoch:152 loss:0.134319 accu:0.953125 gene_loss:0.651035\n",
      "epoch:153 loss:0.073959 accu:0.992188 gene_loss:1.053143\n",
      "epoch:154 loss:0.344693 accu:0.820312 gene_loss:1.316391\n",
      "epoch:155 loss:0.544638 accu:0.765625 gene_loss:2.347795\n",
      "epoch:156 loss:1.121507 accu:0.406250 gene_loss:3.766684\n",
      "epoch:157 loss:0.625963 accu:0.726562 gene_loss:3.298679\n",
      "epoch:158 loss:0.450189 accu:0.757812 gene_loss:3.820457\n",
      "epoch:159 loss:0.800742 accu:0.687500 gene_loss:0.846168\n",
      "epoch:160 loss:0.170257 accu:0.937500 gene_loss:1.763065\n",
      "epoch:161 loss:0.069584 accu:0.984375 gene_loss:2.030299\n",
      "epoch:162 loss:0.139376 accu:0.960938 gene_loss:1.245799\n",
      "epoch:163 loss:0.621095 accu:0.710938 gene_loss:4.078862\n",
      "epoch:164 loss:0.505248 accu:0.750000 gene_loss:2.306919\n",
      "epoch:165 loss:0.485315 accu:0.796875 gene_loss:3.323573\n",
      "epoch:166 loss:0.355132 accu:0.882812 gene_loss:2.699442\n",
      "epoch:167 loss:0.179159 accu:0.945312 gene_loss:1.952693\n",
      "epoch:168 loss:0.715854 accu:0.617188 gene_loss:2.898974\n",
      "epoch:169 loss:0.281203 accu:0.882812 gene_loss:2.335749\n",
      "epoch:170 loss:0.547561 accu:0.773438 gene_loss:1.303316\n",
      "epoch:171 loss:0.383239 accu:0.835938 gene_loss:1.391491\n",
      "epoch:172 loss:0.218786 accu:0.929688 gene_loss:1.068007\n",
      "epoch:173 loss:0.323388 accu:0.898438 gene_loss:0.423444\n",
      "epoch:174 loss:0.117106 accu:0.976562 gene_loss:0.672247\n",
      "epoch:175 loss:0.206373 accu:0.929688 gene_loss:0.785647\n",
      "epoch:176 loss:0.153912 accu:0.937500 gene_loss:0.853497\n",
      "epoch:177 loss:0.501846 accu:0.742188 gene_loss:1.177495\n",
      "epoch:178 loss:0.229106 accu:0.890625 gene_loss:1.473270\n",
      "epoch:179 loss:0.666699 accu:0.648438 gene_loss:1.354244\n",
      "epoch:180 loss:0.539206 accu:0.726562 gene_loss:1.905112\n",
      "epoch:181 loss:0.200611 accu:0.945312 gene_loss:2.238540\n",
      "epoch:182 loss:0.553977 accu:0.750000 gene_loss:1.933464\n",
      "epoch:183 loss:0.798703 accu:0.664062 gene_loss:1.216103\n",
      "epoch:184 loss:0.272059 accu:0.882812 gene_loss:1.622407\n",
      "epoch:185 loss:0.492460 accu:0.828125 gene_loss:0.805612\n",
      "epoch:186 loss:0.202724 accu:0.921875 gene_loss:1.651653\n",
      "epoch:187 loss:0.631554 accu:0.687500 gene_loss:1.093982\n",
      "epoch:188 loss:0.466216 accu:0.757812 gene_loss:2.461403\n",
      "epoch:189 loss:1.251420 accu:0.445312 gene_loss:2.998336\n",
      "epoch:190 loss:0.416324 accu:0.773438 gene_loss:3.156799\n",
      "epoch:191 loss:1.065780 accu:0.531250 gene_loss:3.394695\n",
      "epoch:192 loss:1.022781 accu:0.523438 gene_loss:3.574812\n",
      "epoch:193 loss:0.390511 accu:0.835938 gene_loss:2.827697\n",
      "epoch:194 loss:0.212723 accu:0.929688 gene_loss:2.964470\n",
      "epoch:195 loss:0.550075 accu:0.742188 gene_loss:1.307319\n",
      "epoch:196 loss:0.172093 accu:0.929688 gene_loss:1.071224\n",
      "epoch:197 loss:0.258962 accu:0.898438 gene_loss:0.744312\n",
      "epoch:198 loss:0.281891 accu:0.914062 gene_loss:0.454222\n",
      "epoch:199 loss:0.220141 accu:0.937500 gene_loss:0.896080\n",
      "epoch:200 loss:0.506304 accu:0.765625 gene_loss:0.578260\n",
      "epoch:201 loss:0.268100 accu:0.906250 gene_loss:0.836295\n",
      "epoch:202 loss:1.681643 accu:0.234375 gene_loss:0.995281\n",
      "epoch:203 loss:0.302467 accu:0.890625 gene_loss:1.996690\n",
      "epoch:204 loss:2.107624 accu:0.210938 gene_loss:2.505796\n",
      "epoch:205 loss:0.345837 accu:0.843750 gene_loss:2.647886\n",
      "epoch:206 loss:0.568995 accu:0.726562 gene_loss:2.031022\n",
      "epoch:207 loss:0.225551 accu:0.914062 gene_loss:1.497514\n",
      "epoch:208 loss:0.208640 accu:0.914062 gene_loss:0.546051\n",
      "epoch:209 loss:0.152159 accu:0.945312 gene_loss:0.394405\n",
      "epoch:210 loss:0.129917 accu:0.960938 gene_loss:0.566565\n",
      "epoch:211 loss:0.222949 accu:0.890625 gene_loss:0.818191\n",
      "epoch:212 loss:0.316901 accu:0.867188 gene_loss:1.104364\n",
      "epoch:213 loss:0.476807 accu:0.726562 gene_loss:2.657074\n",
      "epoch:214 loss:1.691194 accu:0.265625 gene_loss:4.573623\n",
      "epoch:215 loss:1.002603 accu:0.617188 gene_loss:2.933889\n",
      "epoch:216 loss:0.835898 accu:0.585938 gene_loss:3.924513\n",
      "epoch:217 loss:1.036766 accu:0.578125 gene_loss:2.251951\n",
      "epoch:218 loss:0.678778 accu:0.679688 gene_loss:2.389912\n",
      "epoch:219 loss:0.453820 accu:0.773438 gene_loss:1.225856\n",
      "epoch:220 loss:0.508227 accu:0.765625 gene_loss:1.458774\n",
      "epoch:221 loss:0.362923 accu:0.812500 gene_loss:1.332452\n",
      "epoch:222 loss:1.362615 accu:0.351562 gene_loss:3.089314\n",
      "epoch:223 loss:0.440513 accu:0.789062 gene_loss:3.315372\n",
      "epoch:224 loss:1.970660 accu:0.210938 gene_loss:2.555332\n",
      "epoch:225 loss:0.838412 accu:0.679688 gene_loss:3.092525\n",
      "epoch:226 loss:0.858049 accu:0.562500 gene_loss:1.942401\n",
      "epoch:227 loss:0.456647 accu:0.796875 gene_loss:1.697361\n",
      "epoch:228 loss:0.159512 accu:0.945312 gene_loss:1.022499\n",
      "epoch:229 loss:0.273151 accu:0.882812 gene_loss:1.323796\n",
      "epoch:230 loss:0.207822 accu:0.945312 gene_loss:1.616314\n",
      "epoch:231 loss:0.411706 accu:0.820312 gene_loss:1.618752\n",
      "epoch:232 loss:0.236069 accu:0.867188 gene_loss:1.375476\n",
      "epoch:233 loss:0.178612 accu:0.929688 gene_loss:0.563752\n",
      "epoch:234 loss:0.327128 accu:0.835938 gene_loss:0.831420\n",
      "epoch:235 loss:0.157978 accu:0.937500 gene_loss:1.693514\n",
      "epoch:236 loss:0.233924 accu:0.906250 gene_loss:0.733589\n",
      "epoch:237 loss:0.373667 accu:0.828125 gene_loss:2.038122\n",
      "epoch:238 loss:0.301483 accu:0.867188 gene_loss:2.211745\n",
      "epoch:239 loss:1.147061 accu:0.421875 gene_loss:3.747324\n",
      "epoch:240 loss:0.878105 accu:0.617188 gene_loss:2.879247\n",
      "epoch:241 loss:0.323075 accu:0.851562 gene_loss:1.812705\n",
      "epoch:242 loss:0.597635 accu:0.687500 gene_loss:2.286855\n",
      "epoch:243 loss:0.300142 accu:0.898438 gene_loss:1.131803\n",
      "epoch:244 loss:0.414999 accu:0.796875 gene_loss:0.705735\n",
      "epoch:245 loss:0.222810 accu:0.914062 gene_loss:1.677050\n",
      "epoch:246 loss:0.812323 accu:0.609375 gene_loss:1.336760\n",
      "epoch:247 loss:0.450632 accu:0.773438 gene_loss:1.578334\n",
      "epoch:248 loss:0.722694 accu:0.593750 gene_loss:2.595490\n",
      "epoch:249 loss:0.545557 accu:0.765625 gene_loss:2.424262\n",
      "epoch:250 loss:0.497778 accu:0.796875 gene_loss:2.636340\n",
      "epoch:251 loss:0.561886 accu:0.710938 gene_loss:3.154965\n",
      "epoch:252 loss:0.946917 accu:0.554688 gene_loss:2.010338\n",
      "epoch:253 loss:0.429879 accu:0.765625 gene_loss:2.981076\n",
      "epoch:254 loss:0.388552 accu:0.812500 gene_loss:2.684143\n",
      "epoch:255 loss:0.399456 accu:0.804688 gene_loss:3.305057\n",
      "epoch:256 loss:0.931357 accu:0.601562 gene_loss:1.887226\n",
      "epoch:257 loss:0.765998 accu:0.609375 gene_loss:1.891733\n",
      "epoch:258 loss:0.665408 accu:0.679688 gene_loss:1.557155\n",
      "epoch:259 loss:0.815386 accu:0.601562 gene_loss:1.108086\n",
      "epoch:260 loss:0.458498 accu:0.765625 gene_loss:0.952447\n",
      "epoch:261 loss:0.676691 accu:0.726562 gene_loss:1.718154\n",
      "epoch:262 loss:0.536446 accu:0.773438 gene_loss:1.572296\n",
      "epoch:263 loss:0.561339 accu:0.750000 gene_loss:0.312986\n",
      "epoch:264 loss:0.390423 accu:0.789062 gene_loss:1.096317\n",
      "epoch:265 loss:0.464016 accu:0.742188 gene_loss:0.192240\n",
      "epoch:266 loss:0.282084 accu:0.859375 gene_loss:0.834363\n",
      "epoch:267 loss:0.185375 accu:0.945312 gene_loss:0.734204\n",
      "epoch:268 loss:0.464023 accu:0.773438 gene_loss:1.538509\n",
      "epoch:269 loss:1.011985 accu:0.570312 gene_loss:1.694412\n",
      "epoch:270 loss:0.620144 accu:0.703125 gene_loss:2.666301\n",
      "epoch:271 loss:0.448107 accu:0.781250 gene_loss:1.874638\n",
      "epoch:272 loss:0.331071 accu:0.843750 gene_loss:1.458532\n",
      "epoch:273 loss:0.490026 accu:0.789062 gene_loss:1.962317\n",
      "epoch:274 loss:0.477278 accu:0.773438 gene_loss:1.673126\n",
      "epoch:275 loss:0.352449 accu:0.898438 gene_loss:2.161720\n",
      "epoch:276 loss:0.348851 accu:0.835938 gene_loss:2.877485\n",
      "epoch:277 loss:0.792570 accu:0.632812 gene_loss:1.607145\n",
      "epoch:278 loss:0.380176 accu:0.867188 gene_loss:0.721858\n",
      "epoch:279 loss:0.145984 accu:0.921875 gene_loss:0.344122\n",
      "epoch:280 loss:0.176562 accu:0.921875 gene_loss:0.301615\n",
      "epoch:281 loss:0.158368 accu:0.968750 gene_loss:0.808533\n",
      "epoch:282 loss:0.269104 accu:0.882812 gene_loss:0.745259\n",
      "epoch:283 loss:1.514048 accu:0.375000 gene_loss:5.602310\n",
      "epoch:284 loss:0.900004 accu:0.648438 gene_loss:3.869098\n",
      "epoch:285 loss:0.831740 accu:0.609375 gene_loss:0.979760\n",
      "epoch:286 loss:0.046308 accu:0.984375 gene_loss:1.037359\n",
      "epoch:287 loss:0.122805 accu:0.945312 gene_loss:0.155915\n",
      "epoch:288 loss:0.060578 accu:0.976562 gene_loss:0.018185\n",
      "epoch:289 loss:0.053605 accu:0.992188 gene_loss:0.016731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:290 loss:0.127368 accu:0.960938 gene_loss:0.134852\n",
      "epoch:291 loss:0.021329 accu:1.000000 gene_loss:0.186122\n",
      "epoch:292 loss:0.096693 accu:0.976562 gene_loss:0.412163\n",
      "epoch:293 loss:0.057874 accu:0.992188 gene_loss:0.409344\n",
      "epoch:294 loss:0.149925 accu:0.953125 gene_loss:0.797484\n",
      "epoch:295 loss:0.284250 accu:0.867188 gene_loss:1.390096\n",
      "epoch:296 loss:0.260966 accu:0.898438 gene_loss:2.346503\n",
      "epoch:297 loss:0.755359 accu:0.640625 gene_loss:3.133653\n",
      "epoch:298 loss:0.813296 accu:0.648438 gene_loss:1.331667\n",
      "epoch:299 loss:0.226096 accu:0.929688 gene_loss:0.828487\n",
      "epoch:300 loss:0.246491 accu:0.921875 gene_loss:0.931587\n",
      "epoch:301 loss:0.376088 accu:0.812500 gene_loss:0.735767\n",
      "epoch:302 loss:0.101868 accu:0.968750 gene_loss:0.641072\n",
      "epoch:303 loss:0.179165 accu:0.953125 gene_loss:0.464328\n",
      "epoch:304 loss:0.273243 accu:0.867188 gene_loss:1.023208\n",
      "epoch:305 loss:0.340312 accu:0.867188 gene_loss:0.779819\n",
      "epoch:306 loss:0.601379 accu:0.734375 gene_loss:1.014649\n",
      "epoch:307 loss:0.574082 accu:0.695312 gene_loss:0.975642\n",
      "epoch:308 loss:1.248379 accu:0.359375 gene_loss:1.313825\n",
      "epoch:309 loss:0.963889 accu:0.492188 gene_loss:5.270897\n",
      "epoch:310 loss:0.699980 accu:0.695312 gene_loss:2.923369\n",
      "epoch:311 loss:0.469795 accu:0.757812 gene_loss:2.149080\n",
      "epoch:312 loss:0.239112 accu:0.906250 gene_loss:1.083013\n",
      "epoch:313 loss:0.183078 accu:0.945312 gene_loss:0.972171\n",
      "epoch:314 loss:0.119594 accu:0.968750 gene_loss:0.907585\n",
      "epoch:315 loss:0.542661 accu:0.718750 gene_loss:2.046548\n",
      "epoch:316 loss:0.533987 accu:0.750000 gene_loss:1.691414\n",
      "epoch:317 loss:0.862017 accu:0.554688 gene_loss:3.557798\n",
      "epoch:318 loss:0.754369 accu:0.625000 gene_loss:1.143676\n",
      "epoch:319 loss:0.224254 accu:0.929688 gene_loss:0.483459\n",
      "epoch:320 loss:0.104147 accu:0.976562 gene_loss:0.251658\n",
      "epoch:321 loss:0.465727 accu:0.773438 gene_loss:0.614739\n",
      "epoch:322 loss:0.164644 accu:0.945312 gene_loss:0.456408\n",
      "epoch:323 loss:0.720212 accu:0.617188 gene_loss:0.175362\n",
      "epoch:324 loss:0.192080 accu:0.953125 gene_loss:0.591776\n",
      "epoch:325 loss:0.427569 accu:0.789062 gene_loss:1.562100\n",
      "epoch:326 loss:1.136264 accu:0.421875 gene_loss:1.312317\n",
      "epoch:327 loss:0.528185 accu:0.742188 gene_loss:3.951821\n",
      "epoch:328 loss:1.747620 accu:0.320312 gene_loss:2.270316\n",
      "epoch:329 loss:0.270462 accu:0.906250 gene_loss:1.687234\n",
      "epoch:330 loss:0.310841 accu:0.875000 gene_loss:0.231427\n",
      "epoch:331 loss:0.176371 accu:0.968750 gene_loss:0.214736\n",
      "epoch:332 loss:0.156201 accu:0.945312 gene_loss:0.439214\n",
      "epoch:333 loss:0.065731 accu:0.984375 gene_loss:0.648967\n",
      "epoch:334 loss:0.115201 accu:0.960938 gene_loss:0.692790\n",
      "epoch:335 loss:0.264358 accu:0.906250 gene_loss:0.895384\n",
      "epoch:336 loss:0.219956 accu:0.914062 gene_loss:1.672204\n",
      "epoch:337 loss:0.414343 accu:0.765625 gene_loss:3.801411\n",
      "epoch:338 loss:0.501388 accu:0.742188 gene_loss:2.708740\n",
      "epoch:339 loss:0.944309 accu:0.585938 gene_loss:1.200039\n",
      "epoch:340 loss:0.191135 accu:0.921875 gene_loss:1.124257\n",
      "epoch:341 loss:0.436326 accu:0.812500 gene_loss:0.252016\n",
      "epoch:342 loss:0.090382 accu:0.984375 gene_loss:0.419101\n",
      "epoch:343 loss:0.168260 accu:0.945312 gene_loss:0.314834\n",
      "epoch:344 loss:0.104865 accu:0.960938 gene_loss:0.405816\n",
      "epoch:345 loss:0.190445 accu:0.937500 gene_loss:0.313296\n",
      "epoch:346 loss:0.121845 accu:0.976562 gene_loss:0.611332\n",
      "epoch:347 loss:0.121025 accu:0.976562 gene_loss:0.653434\n",
      "epoch:348 loss:1.164722 accu:0.476562 gene_loss:3.076777\n",
      "epoch:349 loss:0.599610 accu:0.742188 gene_loss:2.612646\n",
      "epoch:350 loss:1.464976 accu:0.335938 gene_loss:2.721163\n",
      "epoch:351 loss:0.533968 accu:0.773438 gene_loss:1.112257\n",
      "epoch:352 loss:0.921423 accu:0.546875 gene_loss:2.891618\n",
      "epoch:353 loss:0.526331 accu:0.757812 gene_loss:0.932641\n",
      "epoch:354 loss:0.961036 accu:0.562500 gene_loss:1.962531\n",
      "epoch:355 loss:0.188355 accu:0.937500 gene_loss:4.227106\n",
      "epoch:356 loss:0.520204 accu:0.765625 gene_loss:1.683908\n",
      "epoch:357 loss:0.505889 accu:0.781250 gene_loss:3.962791\n",
      "epoch:358 loss:0.843613 accu:0.562500 gene_loss:2.536825\n",
      "epoch:359 loss:0.467632 accu:0.789062 gene_loss:1.162764\n",
      "epoch:360 loss:0.617085 accu:0.664062 gene_loss:0.387805\n",
      "epoch:361 loss:0.366572 accu:0.843750 gene_loss:0.640828\n"
     ]
    }
   ],
   "source": [
    "real_labels = np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(10000):\n",
    "    noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "\n",
    "    real_image = load_image()\n",
    "    #real_image = load_mnist()\n",
    "    #训练判别器\n",
    "    fake_image = generator_i.predict(noise)\n",
    "\n",
    "    real_loss = discriminator_i.train_on_batch(real_image , real_labels)\n",
    "    fake_loss = discriminator_i.train_on_batch(fake_image , fake_labels)\n",
    "\n",
    "    loss = np.add(real_loss , fake_loss)/2\n",
    "\n",
    "    #训练生成器\n",
    "    noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    generator_loss = combined_model_i.train_on_batch(noise2 , real_labels)\n",
    "    \n",
    "    print('epoch:%d loss:%f accu:%f gene_loss:%f' % (i , loss[0] , loss[1] , generator_loss))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        write_image(i)\n",
    "        #write_image_mnist(i)\n",
    "    \n",
    "write_image(999)\n",
    "#write_image_mnist(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeli = Sequential()\n",
    "\n",
    "modeli.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "modeli.add(Reshape((7, 7, 128)))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(Activation(\"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modeli.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
