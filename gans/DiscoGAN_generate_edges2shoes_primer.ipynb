{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU , PReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "#addin cycleGAN 使用instance-norm\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "\n",
    "from keras.initializers import truncated_normal , constant , random_normal\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "#残差块使用\n",
    "from keras.layers import Add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import scipy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "CHANNEL = 3\n",
    "\n",
    "SHAPE = (WIDTH , HEIGHT , CHANNEL)\n",
    "\n",
    "BATCH_SIZE = 4 #crazy!!! slow turtle\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = '../dataset/edges2shoes/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 2 #几行决定显示几个测试样例 显示2个\n",
    "COL = 3 #3列是因为要显示 原图像 另一个特征空间的图像 还原后的图像\n",
    "\n",
    "TRAIN_PATH = glob(PATH + 'train/*')\n",
    "\n",
    "#卷积使用 基卷积核大小\n",
    "G_filters = 64\n",
    "D_filters = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch = int(HEIGHT/(2**4)) #16\n",
    "disc_patch = (patch , patch , 1) #16*16*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(batch_size = BATCH_SIZE , training = True):\n",
    "    #随机在图片库中挑选\n",
    "        \n",
    "    images = np.random.choice(TRAIN_PATH , size=batch_size)\n",
    "    \n",
    "    edges = []\n",
    "    shoes = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        image = scipy.misc.imread(images[i] , mode='RGB').astype(np.float)\n",
    "        \n",
    "        edge = image[: , :WIDTH , : ]\n",
    "        shoe = image[: , WIDTH: , : ]\n",
    "        \n",
    "        #随机性地对训练样本进行 左右反转\n",
    "        if training and np.random.random()<0.5:\n",
    "            edge = np.fliplr(edge)\n",
    "            shoe = np.fliplr(shoe)\n",
    "        \n",
    "        edges.append(edge)\n",
    "        shoes.append(shoe)\n",
    "        \n",
    "    edges = np.array(edges)/127.5 - 1\n",
    "    shoes = np.array(shoes)/127.5 - 1\n",
    "    \n",
    "    return edges , shoes\n",
    "\n",
    "def write_image(epoch):\n",
    "    #生成高分图像时 进行对比显示\n",
    "    edges , shoes = load_image(batch_size=1 , training=False) #1个batch就是两幅图像 一个edge 一个shoe\n",
    "    \n",
    "    fake_shoes = generator_apple2orange.predict(edges)\n",
    "    fake_edges = generator_orange2apple.predict(shoes)\n",
    "    \n",
    "    edges_hat = generator_orange2apple.predict(fake_edges)\n",
    "    shoes_hat = generator_apple2orange.predict(fake_shoes)\n",
    "    \n",
    "    edges = edges*0.5+0.5\n",
    "    shoes = shoes*0.5+0.5\n",
    "    \n",
    "    fake_edges = fake_edges*0.5+0.5\n",
    "    fake_shoes = fake_shoes*0.5+0.5\n",
    "    \n",
    "    edges_hat = edges_hat*0.5+0.5\n",
    "    shoes_hat = shoes_hat*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    count=0\n",
    "    \n",
    "    axes[0][0].imshow(edges[0])\n",
    "    axes[0][0].set_title('edges')\n",
    "    axes[0][0].axis('off')\n",
    "\n",
    "    axes[0][1].imshow(fake_shoes[0])\n",
    "    axes[0][1].set_title('fake_shoes')\n",
    "    axes[0][1].axis('off')\n",
    "    \n",
    "    axes[0][2].imshow(edges_hat[0])\n",
    "    axes[0][2].set_title('restruct_edges')\n",
    "    axes[0][2].axis('off')\n",
    "\n",
    "    axes[1][0].imshow(shoes[0])\n",
    "    axes[1][0].set_title('shoes')\n",
    "    axes[1][0].axis('off')\n",
    "\n",
    "    axes[1][1].imshow(fake_edges[0])\n",
    "    axes[1][1].set_title('fake_edges')\n",
    "    axes[1][1].axis('off')\n",
    "    \n",
    "    axes[1][2].imshow(shoes_hat[0])\n",
    "    axes[1][2].set_title('restruct_shoes')\n",
    "    axes[1][2].axis('off')\n",
    "      \n",
    "    fig.savefig('edges2shoes_discogan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input_data , output_size , filter_size=4 , instance_norm=True):\n",
    "    h = Conv2D(output_size , filter_size , strides=(2,2) , padding='same')(input_data)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    \n",
    "    if instance_norm:\n",
    "        h = InstanceNormalization()(h)\n",
    "    \n",
    "    return h\n",
    "\n",
    "\n",
    "#实现U-Net使用 需要网络的跳连接\n",
    "def deconv2d(input_data , skip_input , output_size , filter_size=4 , dropout_rate=0.0):\n",
    "    h = UpSampling2D(size=2)(input_data)\n",
    "    h = Conv2D(output_size , filter_size , strides=(1,1) , padding='same')(h)\n",
    "    h = Activation('relu')(h)\n",
    "    \n",
    "    if dropout_rate:\n",
    "        h = Dropout(rate=dropout_rate)(h)\n",
    "    \n",
    "    h = InstanceNormalization()(h)\n",
    "    h =  Concatenate()([h , skip_input]) #跳连接具体实现\n",
    "\n",
    "    return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#G使用encoder-decoder结构 但是需要引入跳连接 即U-Net\n",
    "def generator(G_filters , name):\n",
    "    style = Input(shape=SHAPE) #输入一个风格的图像 生成另一个风格的图像\n",
    "    \n",
    "    #encoder\n",
    "    d1 = conv2d(style , G_filters , instance_norm=False)\n",
    "    d2 = conv2d(d1 , G_filters*2)\n",
    "    d3 = conv2d(d2 , G_filters*4)\n",
    "    d4 = conv2d(d3 , G_filters*8)\n",
    "    d5 = conv2d(d4 , G_filters*8)\n",
    "    d6 = conv2d(d5 , G_filters*8)\n",
    "    d7 = conv2d(d6 , G_filters*8)\n",
    " \n",
    "    #decoder\n",
    "    \n",
    "    u1 = deconv2d(d7 , d6 , G_filters*8)\n",
    "    u2 = deconv2d(u1 , d5 , G_filters*8)\n",
    "    u3 = deconv2d(u2 , d4 , G_filters*8)\n",
    "    u4 = deconv2d(u3 , d3 , G_filters*4)\n",
    "    u5 = deconv2d(u4 , d2 , G_filters*2)\n",
    "    u6 = deconv2d(u5 , d1 , G_filters)\n",
    "    \n",
    "    u7 = UpSampling2D(size=(2,2))(u6)\n",
    "    other_style = Conv2D(filters=CHANNEL , kernel_size=(4,4) , strides=(1,1) , padding='same' , activation='tanh')(u7) #还原后的图像\n",
    "    \n",
    "    return Model(style , other_style , name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(D_filters , name):\n",
    "    style = Input(shape=SHAPE)\n",
    "    \n",
    "    h1 = conv2d(style , output_size=D_filters , instance_norm=False)\n",
    "    h2 = conv2d(h1 , output_size=D_filters*2)\n",
    "    h3 = conv2d(h2 , output_size=D_filters*4)\n",
    "    h4 = conv2d(h3 , output_size=D_filters*8)\n",
    "    \n",
    "    validity =  Conv2D(1 , kernel_size=(4,4) , strides=(1,1) , padding='same')(h4)\n",
    "    \n",
    "    return Model(style , validity , name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)\n",
    "\n",
    "discriminator_apple = discriminator(D_filters , name='discriminator_apple')\n",
    "discriminator_apple.compile(optimizer = adam , loss='mse' , metrics=['accuracy'])\n",
    "discriminator_orange = discriminator(D_filters , name='discriminator_orange')\n",
    "discriminator_orange.compile(optimizer = adam , loss='mse' , metrics=['accuracy'])\n",
    "\n",
    "generator_apple2orange = generator(G_filters , name='generator_apple2orange')\n",
    "generator_orange2apple = generator(G_filters , name='generator_orange2apple')\n",
    "\n",
    "apples = Input(shape=SHAPE)\n",
    "oranges = Input(shape=SHAPE)\n",
    "\n",
    "fake_oranges = generator_apple2orange(apples)\n",
    "fake_apples = generator_orange2apple(oranges)\n",
    "\n",
    "apples_hat = generator_orange2apple(fake_oranges)\n",
    "oranges_hat = generator_apple2orange(fake_apples)\n",
    "\n",
    "#freeze D\n",
    "discriminator_apple.trainable = False\n",
    "discriminator_orange.trainable = False\n",
    "\n",
    "validity_apple = discriminator_apple(fake_apples)\n",
    "validity_orange = discriminator_orange(fake_oranges)\n",
    "\n",
    "#一共6个输出 最后4个输出希望和原图像一致 这样图像同时具有两个风格\n",
    "combined = Model([apples , oranges] , [validity_apple , validity_orange , fake_oranges , fake_apples , apples_hat , oranges_hat])\n",
    "combined.compile(optimizer=adam , loss=['mse' , 'mse' , 'mae' , 'mae' , 'mae' , 'mae'] , loss_weights=[1 ,1,10, 10 , 1, 1]) #cycleGAN的损失权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:26.839453 accu:0.157959 |mse1:127.972374 :mse2:4.310163 mae1:104.165298 mae2:0.629249 mae3:1.151195 mae4:0.844402\n",
      "epoch:1 loss:53.837662 accu:0.016846 |mse1:144.365829 :mse2:30.638023 mae1:98.452301 mae2:0.801013 mae3:0.552286 mae4:1.003998\n",
      "epoch:2 loss:31.830841 accu:0.065430 |mse1:195.412537 :mse2:2.052113 mae1:179.529007 mae2:0.913364 mae3:0.346781 mae4:0.344338\n",
      "epoch:3 loss:9.288846 accu:0.040527 |mse1:28.717392 :mse2:1.487355 mae1:12.682568 mae2:1.144615 mae3:0.183636 mae4:0.128674\n",
      "epoch:4 loss:10.061327 accu:0.177979 |mse1:15.776257 :mse2:1.153067 mae1:0.133939 mae2:1.126424 mae3:0.203821 mae4:0.071111\n",
      "epoch:5 loss:0.683095 accu:0.375977 |mse1:18.832788 :mse2:0.860998 mae1:3.972034 mae2:1.180704 mae3:0.095113 mae4:0.060779\n",
      "epoch:6 loss:0.931264 accu:0.147461 |mse1:14.891116 :mse2:0.257235 mae1:1.413712 mae2:1.120754 mae3:0.082633 mae4:0.064726\n",
      "epoch:7 loss:0.336615 accu:0.546387 |mse1:13.652836 :mse2:0.338684 mae1:0.596270 mae2:1.101540 mae3:0.055088 mae4:0.049380\n",
      "epoch:8 loss:0.477982 accu:0.312256 |mse1:12.339701 :mse2:0.478319 mae1:0.495672 mae2:0.976637 mae3:0.056865 mae4:0.056976\n",
      "epoch:9 loss:0.501669 accu:0.264893 |mse1:10.766524 :mse2:0.432045 mae1:0.515191 mae2:0.838430 mae3:0.054091 mae4:0.054313\n",
      "epoch:10 loss:0.446059 accu:0.330322 |mse1:11.088125 :mse2:0.426713 mae1:0.751258 mae2:0.841400 mae3:0.059029 mae4:0.060862\n",
      "epoch:11 loss:0.331641 accu:0.393311 |mse1:10.117395 :mse2:0.453316 mae1:0.742952 mae2:0.770220 mae3:0.040438 mae4:0.042558\n",
      "epoch:12 loss:0.275279 accu:0.496582 |mse1:10.312293 :mse2:0.455199 mae1:0.824481 mae2:0.774700 mae3:0.045995 mae4:0.048181\n",
      "epoch:13 loss:0.261686 accu:0.518311 |mse1:11.210731 :mse2:0.465332 mae1:0.986110 mae2:0.826266 mae3:0.060393 mae4:0.062630\n",
      "epoch:14 loss:0.255473 accu:0.520264 |mse1:10.853105 :mse2:0.470828 mae1:1.001615 mae2:0.802681 mae3:0.049633 mae4:0.051630\n",
      "epoch:15 loss:0.254301 accu:0.522949 |mse1:10.461379 :mse2:0.481479 mae1:1.025405 mae2:0.764359 mae3:0.049173 mae4:0.052050\n",
      "epoch:16 loss:0.260152 accu:0.510254 |mse1:11.373597 :mse2:0.493820 mae1:1.008785 mae2:0.847363 mae3:0.049576 mae4:0.051584\n",
      "epoch:17 loss:0.257527 accu:0.524902 |mse1:10.384659 :mse2:0.496032 mae1:0.926828 mae2:0.777211 mae3:0.037042 mae4:0.039763\n",
      "epoch:18 loss:0.267744 accu:0.507812 |mse1:10.998289 :mse2:0.506282 mae1:1.005898 mae2:0.808351 mae3:0.053555 mae4:0.055719\n",
      "epoch:19 loss:0.269408 accu:0.508301 |mse1:11.005955 :mse2:0.508934 mae1:0.968884 mae2:0.818031 mae3:0.047704 mae4:0.049915\n",
      "epoch:20 loss:0.312422 accu:0.489502 |mse1:11.730509 :mse2:0.516295 mae1:0.986388 mae2:0.871595 mae3:0.057720 mae4:0.059686\n",
      "epoch:21 loss:0.282481 accu:0.501465 |mse1:11.769812 :mse2:0.526783 mae1:0.987151 mae2:0.875868 mae3:0.056022 mae4:0.057751\n",
      "epoch:22 loss:0.268111 accu:0.517334 |mse1:11.142906 :mse2:0.533846 mae1:1.067943 mae2:0.805725 mae3:0.061117 mae4:0.063206\n",
      "epoch:23 loss:0.293885 accu:0.475342 |mse1:11.362087 :mse2:0.548828 mae1:0.852147 mae2:0.839244 mae3:0.065700 mae4:0.067700\n",
      "epoch:24 loss:0.274004 accu:0.512695 |mse1:10.736149 :mse2:0.535256 mae1:1.008711 mae2:0.804560 mae3:0.030756 mae4:0.032924\n",
      "epoch:25 loss:0.285329 accu:0.500000 |mse1:10.453472 :mse2:0.542050 mae1:0.951890 mae2:0.777563 mae3:0.036505 mae4:0.038997\n",
      "epoch:26 loss:0.258510 accu:0.526855 |mse1:10.403120 :mse2:0.559770 mae1:0.963170 mae2:0.753928 mae3:0.052796 mae4:0.055171\n",
      "epoch:27 loss:0.275952 accu:0.507080 |mse1:11.431242 :mse2:0.560398 mae1:1.008150 mae2:0.837947 mae3:0.058239 mae4:0.059981\n",
      "epoch:28 loss:0.267644 accu:0.513916 |mse1:10.785867 :mse2:0.559249 mae1:1.012216 mae2:0.790361 mae3:0.046846 mae4:0.048668\n",
      "epoch:29 loss:0.275706 accu:0.494141 |mse1:10.598141 :mse2:0.577315 mae1:0.918820 mae2:0.769508 mae3:0.057335 mae4:0.059246\n",
      "epoch:30 loss:0.299248 accu:0.476807 |mse1:10.855076 :mse2:0.568695 mae1:1.078403 mae2:0.797549 mae3:0.039301 mae4:0.041051\n",
      "epoch:31 loss:0.316072 accu:0.463867 |mse1:10.910256 :mse2:0.575094 mae1:1.034607 mae2:0.806772 mae3:0.038548 mae4:0.040565\n",
      "epoch:32 loss:0.307461 accu:0.469727 |mse1:10.179244 :mse2:0.583254 mae1:0.905115 mae2:0.745016 mae3:0.044987 mae4:0.046989\n",
      "epoch:33 loss:0.294942 accu:0.473389 |mse1:10.571105 :mse2:0.584501 mae1:1.031227 mae2:0.764647 mae3:0.049015 mae4:0.050308\n",
      "epoch:34 loss:0.343145 accu:0.440674 |mse1:10.188226 :mse2:0.596053 mae1:0.855669 mae2:0.741933 mae3:0.052342 mae4:0.053808\n",
      "epoch:35 loss:0.292377 accu:0.491455 |mse1:10.433902 :mse2:0.601842 mae1:0.947240 mae2:0.768319 mae3:0.039138 mae4:0.040655\n",
      "epoch:36 loss:0.290922 accu:0.485840 |mse1:10.163759 :mse2:0.598392 mae1:0.840938 mae2:0.751864 mae3:0.041136 mae4:0.043047\n",
      "epoch:37 loss:0.264975 accu:0.520508 |mse1:10.365109 :mse2:0.624600 mae1:0.937443 mae2:0.744547 mae3:0.055260 mae4:0.056954\n",
      "epoch:38 loss:0.280476 accu:0.498779 |mse1:10.130127 :mse2:0.614223 mae1:0.887206 mae2:0.728466 mae3:0.055833 mae4:0.057385\n",
      "epoch:39 loss:0.305156 accu:0.453369 |mse1:10.241314 :mse2:0.597314 mae1:0.843600 mae2:0.763448 mae3:0.036271 mae4:0.037498\n",
      "epoch:40 loss:0.303083 accu:0.458984 |mse1:10.186931 :mse2:0.645764 mae1:0.791609 mae2:0.754092 mae3:0.041181 mae4:0.042526\n",
      "epoch:41 loss:0.290107 accu:0.495850 |mse1:9.480734 :mse2:0.615510 mae1:0.816949 mae2:0.687746 mae3:0.044004 mae4:0.045607\n",
      "epoch:42 loss:0.305138 accu:0.438477 |mse1:9.633093 :mse2:0.629138 mae1:0.971680 mae2:0.681498 mae3:0.048452 mae4:0.049842\n",
      "epoch:43 loss:0.422751 accu:0.335938 |mse1:9.176296 :mse2:0.629839 mae1:0.939353 mae2:0.653533 mae3:0.037861 mae4:0.039185\n",
      "epoch:44 loss:0.398142 accu:0.288086 |mse1:9.590273 :mse2:0.634587 mae1:1.389125 mae2:0.642426 mae3:0.045193 mae4:0.045441\n",
      "epoch:45 loss:0.496505 accu:0.146973 |mse1:7.112081 :mse2:0.675271 mae1:1.117975 mae2:0.425215 mae3:0.057728 mae4:0.058274\n",
      "epoch:46 loss:0.442750 accu:0.183838 |mse1:9.329445 :mse2:0.600487 mae1:1.664350 mae2:0.603350 mae3:0.038255 mae4:0.038384\n",
      "epoch:47 loss:0.832853 accu:0.036865 |mse1:10.199759 :mse2:0.661124 mae1:3.259733 mae2:0.527657 mae3:0.043072 mae4:0.043275\n",
      "epoch:48 loss:0.617171 accu:0.244141 |mse1:6.736836 :mse2:0.637686 mae1:0.542339 mae2:0.457711 mae3:0.046689 mae4:0.046700\n",
      "epoch:49 loss:0.445312 accu:0.163086 |mse1:8.489748 :mse2:0.667986 mae1:0.992479 mae2:0.564163 mae3:0.055977 mae4:0.055700\n",
      "epoch:50 loss:0.598856 accu:0.039795 |mse1:5.662093 :mse2:0.662509 mae1:0.737576 mae2:0.349303 mae3:0.037893 mae4:0.038012\n",
      "epoch:51 loss:0.344249 accu:0.395752 |mse1:8.711568 :mse2:0.666390 mae1:1.025504 mae2:0.593195 mae3:0.044344 mae4:0.043872\n",
      "epoch:52 loss:0.388997 accu:0.334961 |mse1:9.093352 :mse2:0.671299 mae1:0.881884 mae2:0.631257 mae3:0.053354 mae4:0.052870\n",
      "epoch:53 loss:0.435297 accu:0.153076 |mse1:7.825261 :mse2:0.665105 mae1:0.926195 mae2:0.517202 mae3:0.048894 mae4:0.048592\n",
      "epoch:54 loss:0.341424 accu:0.397461 |mse1:9.068962 :mse2:0.690858 mae1:0.926007 mae2:0.620355 mae3:0.056372 mae4:0.055811\n",
      "epoch:55 loss:0.424391 accu:0.230957 |mse1:8.135409 :mse2:0.680647 mae1:0.874268 mae2:0.553370 mae3:0.044526 mae4:0.044138\n",
      "epoch:56 loss:0.356915 accu:0.386963 |mse1:8.619721 :mse2:0.681159 mae1:0.930402 mae2:0.594306 mae3:0.042315 mae4:0.041794\n",
      "epoch:57 loss:0.452518 accu:0.151123 |mse1:7.566209 :mse2:0.690720 mae1:0.837875 mae2:0.501249 mae3:0.046977 mae4:0.046596\n",
      "epoch:58 loss:0.355352 accu:0.401367 |mse1:8.118752 :mse2:0.698341 mae1:0.855505 mae2:0.538179 mae3:0.057896 mae4:0.057442\n",
      "epoch:59 loss:0.424001 accu:0.155029 |mse1:6.022837 :mse2:0.724238 mae1:0.705774 mae2:0.353438 mae3:0.063585 mae4:0.063455\n",
      "epoch:60 loss:0.335107 accu:0.482422 |mse1:8.436601 :mse2:0.686685 mae1:1.225186 mae2:0.557680 mae3:0.035070 mae4:0.034582\n",
      "epoch:61 loss:0.405152 accu:0.314453 |mse1:8.743576 :mse2:0.718164 mae1:0.885690 mae2:0.607222 mae3:0.041389 mae4:0.040803\n",
      "epoch:62 loss:0.515394 accu:0.106201 |mse1:7.371060 :mse2:0.698111 mae1:0.972695 mae2:0.469132 mae3:0.048608 mae4:0.048204\n",
      "epoch:63 loss:0.386238 accu:0.358887 |mse1:7.552649 :mse2:0.704210 mae1:0.923952 mae2:0.502100 mae3:0.036076 mae4:0.035702\n",
      "epoch:64 loss:0.356829 accu:0.395020 |mse1:8.807028 :mse2:0.683997 mae1:0.955056 mae2:0.613299 mae3:0.038100 mae4:0.037508\n",
      "epoch:65 loss:0.360244 accu:0.349854 |mse1:8.341033 :mse2:0.732993 mae1:0.880555 mae2:0.564226 mae3:0.046772 mae4:0.046252\n",
      "epoch:66 loss:0.255457 accu:0.491455 |mse1:8.955188 :mse2:0.737722 mae1:0.910476 mae2:0.599055 mae3:0.064438 mae4:0.063896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:67 loss:0.316215 accu:0.440186 |mse1:7.552331 :mse2:0.786475 mae1:0.822550 mae2:0.485926 mae3:0.053715 mae4:0.053356\n",
      "epoch:68 loss:0.371071 accu:0.369873 |mse1:7.224988 :mse2:0.708131 mae1:0.854384 mae2:0.469102 mae3:0.045407 mae4:0.045151\n",
      "epoch:69 loss:0.243915 accu:0.519287 |mse1:9.097054 :mse2:0.772256 mae1:1.006557 mae2:0.613222 mae3:0.051431 mae4:0.050853\n",
      "epoch:70 loss:0.406780 accu:0.237305 |mse1:6.797404 :mse2:0.751241 mae1:0.764282 mae2:0.437165 mae3:0.042507 mae4:0.042203\n",
      "epoch:71 loss:0.314644 accu:0.381836 |mse1:6.657200 :mse2:0.761157 mae1:0.962625 mae2:0.396271 mae3:0.051784 mae4:0.051537\n",
      "epoch:72 loss:0.445719 accu:0.181152 |mse1:6.817579 :mse2:0.767745 mae1:0.972036 mae2:0.420346 mae3:0.040961 mae4:0.040767\n",
      "epoch:73 loss:0.406635 accu:0.230225 |mse1:7.711053 :mse2:0.791524 mae1:1.111583 mae2:0.487156 mae3:0.040325 mae4:0.039995\n",
      "epoch:74 loss:0.330644 accu:0.418213 |mse1:8.513199 :mse2:0.747661 mae1:0.848883 mae2:0.575633 mae3:0.052576 mae4:0.052021\n",
      "epoch:75 loss:0.372184 accu:0.233643 |mse1:6.986129 :mse2:0.791399 mae1:0.961040 mae2:0.417064 mae3:0.057998 mae4:0.057715\n",
      "epoch:76 loss:0.415481 accu:0.244385 |mse1:6.252873 :mse2:0.818966 mae1:0.907420 mae2:0.370980 mae3:0.039956 mae4:0.039708\n",
      "epoch:77 loss:0.381027 accu:0.258789 |mse1:6.446003 :mse2:0.736096 mae1:0.992794 mae2:0.393317 mae3:0.035066 mae4:0.034775\n",
      "epoch:78 loss:0.401125 accu:0.211426 |mse1:6.304546 :mse2:0.751938 mae1:1.008775 mae2:0.376066 mae3:0.036514 mae4:0.036241\n",
      "epoch:79 loss:0.287146 accu:0.456787 |mse1:7.917974 :mse2:0.809765 mae1:0.987458 mae2:0.513309 mae3:0.042650 mae4:0.042230\n",
      "epoch:80 loss:0.320142 accu:0.441895 |mse1:8.571082 :mse2:0.803039 mae1:1.007297 mae2:0.572170 mae3:0.042048 mae4:0.041512\n",
      "epoch:81 loss:0.364957 accu:0.390137 |mse1:6.331668 :mse2:0.826985 mae1:1.009643 mae2:0.350291 mae3:0.057954 mae4:0.057730\n",
      "epoch:82 loss:0.302160 accu:0.447021 |mse1:7.728826 :mse2:0.864694 mae1:0.989735 mae2:0.478751 mae3:0.054944 mae4:0.054467\n",
      "epoch:83 loss:0.378848 accu:0.316650 |mse1:7.493954 :mse2:0.764508 mae1:1.012424 mae2:0.483584 mae3:0.035997 mae4:0.035560\n",
      "epoch:84 loss:0.717267 accu:0.295654 |mse1:7.242584 :mse2:0.889176 mae1:1.076177 mae2:0.423306 mae3:0.056864 mae4:0.056881\n",
      "epoch:85 loss:0.389646 accu:0.426514 |mse1:7.076418 :mse2:0.789987 mae1:1.197040 mae2:0.413746 mae3:0.048663 mae4:0.048386\n",
      "epoch:86 loss:0.464534 accu:0.425049 |mse1:7.721414 :mse2:0.928638 mae1:1.081162 mae2:0.465301 mae3:0.053535 mae4:0.053233\n",
      "epoch:87 loss:0.512531 accu:0.201416 |mse1:6.991023 :mse2:0.923099 mae1:0.923223 mae2:0.431729 mae3:0.035467 mae4:0.035153\n",
      "epoch:88 loss:0.457626 accu:0.272705 |mse1:6.142643 :mse2:0.789557 mae1:0.790854 mae2:0.371016 mae3:0.043710 mae4:0.043592\n",
      "epoch:89 loss:0.541932 accu:0.158691 |mse1:6.307096 :mse2:0.921079 mae1:0.690718 mae2:0.379651 mae3:0.047092 mae4:0.047105\n",
      "epoch:90 loss:0.233625 accu:0.736572 |mse1:8.038872 :mse2:0.881858 mae1:0.884294 mae2:0.492336 mae3:0.077163 mae4:0.076800\n",
      "epoch:91 loss:0.698043 accu:0.128174 |mse1:5.670742 :mse2:0.832456 mae1:0.590033 mae2:0.343211 mae3:0.043622 mae4:0.043572\n",
      "epoch:92 loss:0.281766 accu:0.482910 |mse1:7.305843 :mse2:0.840225 mae1:0.985561 mae2:0.458471 mae3:0.039236 mae4:0.038907\n",
      "epoch:93 loss:0.291938 accu:0.534180 |mse1:7.550336 :mse2:0.913404 mae1:0.723081 mae2:0.473864 mae3:0.063279 mae4:0.063019\n",
      "epoch:94 loss:0.468877 accu:0.214355 |mse1:6.927572 :mse2:0.862623 mae1:0.678723 mae2:0.443595 mae3:0.046030 mae4:0.046011\n",
      "epoch:95 loss:0.279146 accu:0.524902 |mse1:6.816900 :mse2:0.896499 mae1:0.720019 mae2:0.414283 mae3:0.058517 mae4:0.058365\n",
      "epoch:96 loss:0.581995 accu:0.364258 |mse1:6.545302 :mse2:0.902219 mae1:0.495504 mae2:0.412425 mae3:0.055982 mae4:0.056023\n",
      "epoch:97 loss:0.304999 accu:0.591797 |mse1:6.951530 :mse2:0.837050 mae1:0.905178 mae2:0.424775 mae3:0.048105 mae4:0.047868\n",
      "epoch:98 loss:0.418745 accu:0.333740 |mse1:5.895912 :mse2:0.914899 mae1:0.621373 mae2:0.346267 mae3:0.049950 mae4:0.049797\n",
      "epoch:99 loss:0.295472 accu:0.547363 |mse1:6.428034 :mse2:0.865615 mae1:0.863907 mae2:0.379783 mae3:0.047056 mae4:0.046838\n",
      "epoch:100 loss:0.416647 accu:0.390869 |mse1:6.196128 :mse2:0.914375 mae1:0.576943 mae2:0.377295 mae3:0.050561 mae4:0.050449\n",
      "epoch:101 loss:0.285673 accu:0.506592 |mse1:6.930982 :mse2:0.910150 mae1:0.740646 mae2:0.415089 mae3:0.064861 mae4:0.064752\n",
      "epoch:102 loss:0.532208 accu:0.170166 |mse1:5.975855 :mse2:0.752774 mae1:0.642194 mae2:0.379048 mae3:0.037400 mae4:0.037298\n",
      "epoch:103 loss:0.367512 accu:0.366455 |mse1:6.517591 :mse2:0.922427 mae1:0.753694 mae2:0.404056 mae3:0.035876 mae4:0.035661\n",
      "epoch:104 loss:0.448345 accu:0.187988 |mse1:6.540579 :mse2:1.013355 mae1:0.772967 mae2:0.402383 mae3:0.029575 mae4:0.029378\n",
      "epoch:105 loss:0.484731 accu:0.354492 |mse1:5.915333 :mse2:0.790964 mae1:0.594757 mae2:0.368906 mae3:0.043077 mae4:0.043031\n",
      "epoch:106 loss:0.411991 accu:0.364990 |mse1:5.834343 :mse2:0.868067 mae1:0.680663 mae2:0.339823 mae3:0.049956 mae4:0.049919\n",
      "epoch:107 loss:0.318698 accu:0.496094 |mse1:6.696975 :mse2:0.875971 mae1:0.720678 mae2:0.407166 mae3:0.056675 mae4:0.056492\n",
      "epoch:108 loss:0.494199 accu:0.186768 |mse1:6.098657 :mse2:0.937678 mae1:0.720887 mae2:0.374467 mae3:0.028893 mae4:0.028748\n",
      "epoch:109 loss:0.159724 accu:0.770508 |mse1:7.354351 :mse2:0.953652 mae1:0.812803 mae2:0.439856 mae3:0.067357 mae4:0.067140\n",
      "epoch:110 loss:0.736423 accu:0.154541 |mse1:6.721097 :mse2:0.803065 mae1:0.436871 mae2:0.442826 mae3:0.056378 mae4:0.056419\n",
      "epoch:111 loss:0.424011 accu:0.162598 |mse1:5.347317 :mse2:0.924731 mae1:0.745251 mae2:0.295129 mae3:0.039485 mae4:0.039475\n",
      "epoch:112 loss:0.248765 accu:0.641113 |mse1:7.349266 :mse2:0.860240 mae1:0.854604 mae2:0.460894 mae3:0.050399 mae4:0.050178\n",
      "epoch:113 loss:0.355225 accu:0.416016 |mse1:6.117352 :mse2:0.882758 mae1:0.698099 mae2:0.364369 mae3:0.047325 mae4:0.047183\n",
      "epoch:114 loss:0.396848 accu:0.411621 |mse1:6.810575 :mse2:1.067520 mae1:0.905475 mae2:0.413666 mae3:0.026079 mae4:0.025889\n",
      "epoch:115 loss:0.343173 accu:0.454346 |mse1:6.720513 :mse2:0.858741 mae1:0.651052 mae2:0.417381 mae3:0.056525 mae4:0.056347\n",
      "epoch:116 loss:0.500386 accu:0.269775 |mse1:6.410247 :mse2:0.735043 mae1:0.767915 mae2:0.416570 mae3:0.029360 mae4:0.029239\n",
      "epoch:117 loss:0.247936 accu:0.623047 |mse1:6.733525 :mse2:0.961498 mae1:0.695390 mae2:0.420029 mae3:0.041182 mae4:0.041019\n",
      "epoch:118 loss:0.395433 accu:0.254395 |mse1:6.485504 :mse2:0.864100 mae1:0.617078 mae2:0.413247 mae3:0.042010 mae4:0.041923\n",
      "epoch:119 loss:0.381010 accu:0.279053 |mse1:6.667455 :mse2:0.918656 mae1:0.679329 mae2:0.424951 mae3:0.036247 mae4:0.036148\n",
      "epoch:120 loss:0.424980 accu:0.446777 |mse1:6.981204 :mse2:1.053927 mae1:0.549286 mae2:0.415844 mae3:0.073409 mae4:0.073315\n",
      "epoch:121 loss:0.751515 accu:0.062500 |mse1:5.412139 :mse2:0.544093 mae1:0.570768 mae2:0.352306 mae3:0.038757 mae4:0.038741\n",
      "epoch:122 loss:0.133944 accu:0.815674 |mse1:8.108436 :mse2:1.110629 mae1:0.979203 mae2:0.487298 mae3:0.058785 mae4:0.058547\n",
      "epoch:123 loss:0.411064 accu:0.312012 |mse1:6.867914 :mse2:0.654855 mae1:0.687797 mae2:0.456580 mae3:0.044933 mae4:0.044718\n",
      "epoch:124 loss:0.228472 accu:0.599854 |mse1:7.274873 :mse2:1.053301 mae1:0.703523 mae2:0.442195 mae3:0.058935 mae4:0.058767\n",
      "epoch:125 loss:0.474700 accu:0.191895 |mse1:5.831175 :mse2:0.716194 mae1:0.642867 mae2:0.373346 mae3:0.033076 mae4:0.033006\n",
      "epoch:126 loss:0.437729 accu:0.454834 |mse1:7.168375 :mse2:1.065838 mae1:0.544357 mae2:0.446663 mae3:0.059388 mae4:0.059375\n",
      "epoch:127 loss:0.270680 accu:0.631104 |mse1:7.204643 :mse2:0.756630 mae1:0.810744 mae2:0.447217 mae3:0.064649 mae4:0.064536\n",
      "epoch:128 loss:0.281170 accu:0.605225 |mse1:6.774330 :mse2:0.969812 mae1:0.652800 mae2:0.406510 mae3:0.060646 mae4:0.060542\n",
      "epoch:129 loss:0.613328 accu:0.096191 |mse1:5.725784 :mse2:0.636215 mae1:0.555663 mae2:0.379388 mae3:0.032915 mae4:0.032871\n",
      "epoch:130 loss:0.253700 accu:0.612793 |mse1:6.369101 :mse2:0.986890 mae1:0.757110 mae2:0.374161 mae3:0.046221 mae4:0.046089\n",
      "epoch:131 loss:0.431704 accu:0.495117 |mse1:6.179973 :mse2:0.884746 mae1:0.490496 mae2:0.367985 mae3:0.069514 mae4:0.069448\n",
      "epoch:132 loss:0.398183 accu:0.264160 |mse1:5.722812 :mse2:0.857649 mae1:0.775165 mae2:0.329788 mae3:0.041802 mae4:0.041703\n",
      "epoch:133 loss:0.387484 accu:0.423340 |mse1:5.276088 :mse2:0.844161 mae1:0.632420 mae2:0.305269 mae3:0.039778 mae4:0.039696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:134 loss:0.160127 accu:0.801758 |mse1:7.209192 :mse2:0.959266 mae1:0.864592 mae2:0.427032 mae3:0.061927 mae4:0.061772\n",
      "epoch:135 loss:0.456590 accu:0.203857 |mse1:6.473062 :mse2:0.805270 mae1:0.622126 mae2:0.422802 mae3:0.035889 mae4:0.035746\n",
      "epoch:136 loss:0.427980 accu:0.289307 |mse1:5.774258 :mse2:0.820599 mae1:0.584970 mae2:0.366442 mae3:0.030838 mae4:0.030773\n",
      "epoch:137 loss:0.357896 accu:0.262451 |mse1:5.586280 :mse2:0.832122 mae1:0.708997 mae2:0.337417 mae3:0.030411 mae4:0.030339\n",
      "epoch:138 loss:0.423350 accu:0.368896 |mse1:5.847128 :mse2:0.906239 mae1:0.532478 mae2:0.364403 mae3:0.036381 mae4:0.036312\n",
      "epoch:139 loss:0.341536 accu:0.461426 |mse1:6.175719 :mse2:0.953649 mae1:0.646293 mae2:0.358235 mae3:0.057851 mae4:0.057794\n",
      "epoch:140 loss:0.544469 accu:0.159912 |mse1:5.563390 :mse2:0.623981 mae1:0.652575 mae2:0.358641 mae3:0.031079 mae4:0.030984\n",
      "epoch:141 loss:0.197705 accu:0.760010 |mse1:6.549042 :mse2:1.011944 mae1:0.776501 mae2:0.390446 mae3:0.041965 mae4:0.041842\n",
      "epoch:142 loss:0.277425 accu:0.537354 |mse1:6.860712 :mse2:0.845897 mae1:0.636747 mae2:0.425765 mae3:0.062762 mae4:0.062614\n",
      "epoch:143 loss:0.237535 accu:0.669678 |mse1:7.385545 :mse2:0.826548 mae1:0.805418 mae2:0.472166 mae3:0.050412 mae4:0.050209\n",
      "epoch:144 loss:0.404758 accu:0.204102 |mse1:5.633381 :mse2:0.796436 mae1:0.540484 mae2:0.360832 mae3:0.029638 mae4:0.029584\n",
      "epoch:145 loss:0.331190 accu:0.447998 |mse1:6.451028 :mse2:0.925069 mae1:0.589135 mae2:0.403020 mae3:0.046087 mae4:0.046024\n",
      "epoch:146 loss:0.418494 accu:0.212646 |mse1:5.683052 :mse2:0.765252 mae1:0.642303 mae2:0.355027 mae3:0.033761 mae4:0.033686\n",
      "epoch:147 loss:0.221023 accu:0.629639 |mse1:6.642200 :mse2:1.015972 mae1:0.721416 mae2:0.393551 mae3:0.051913 mae4:0.051792\n",
      "epoch:148 loss:0.443454 accu:0.168701 |mse1:5.245759 :mse2:0.741208 mae1:0.578494 mae2:0.311876 mae3:0.045184 mae4:0.045133\n",
      "epoch:149 loss:0.267871 accu:0.564209 |mse1:5.948360 :mse2:0.930544 mae1:0.726310 mae2:0.347670 mae3:0.042163 mae4:0.042079\n",
      "epoch:150 loss:0.154724 accu:0.809814 |mse1:6.656296 :mse2:0.896416 mae1:0.782281 mae2:0.394993 mae3:0.056880 mae4:0.056749\n",
      "epoch:151 loss:0.278607 accu:0.464600 |mse1:5.608228 :mse2:0.925542 mae1:0.620251 mae2:0.325897 mae3:0.043056 mae4:0.042966\n",
      "epoch:152 loss:0.331243 accu:0.300537 |mse1:7.027104 :mse2:0.862343 mae1:0.730093 mae2:0.458288 mae3:0.035588 mae4:0.035493\n",
      "epoch:153 loss:0.369208 accu:0.442139 |mse1:6.341400 :mse2:0.859092 mae1:0.540863 mae2:0.411451 mae3:0.038118 mae4:0.038092\n",
      "epoch:154 loss:0.180110 accu:0.734131 |mse1:6.640432 :mse2:0.982474 mae1:0.715892 mae2:0.395115 mae3:0.053656 mae4:0.053558\n",
      "epoch:155 loss:0.412188 accu:0.466553 |mse1:6.488798 :mse2:0.805726 mae1:0.488012 mae2:0.418069 mae3:0.054265 mae4:0.054191\n",
      "epoch:156 loss:0.317515 accu:0.493652 |mse1:6.512419 :mse2:0.941685 mae1:0.660294 mae2:0.391948 mae3:0.055251 mae4:0.055197\n",
      "epoch:157 loss:0.143961 accu:0.798828 |mse1:7.280274 :mse2:0.904151 mae1:0.882269 mae2:0.437017 mae3:0.061666 mae4:0.061497\n",
      "epoch:158 loss:0.403719 accu:0.237793 |mse1:6.003257 :mse2:0.911305 mae1:0.618733 mae2:0.367529 mae3:0.038836 mae4:0.038756\n",
      "epoch:159 loss:0.231884 accu:0.629639 |mse1:6.536590 :mse2:0.889768 mae1:0.825729 mae2:0.396086 mae3:0.041926 mae4:0.041810\n",
      "epoch:160 loss:0.279833 accu:0.532959 |mse1:6.250361 :mse2:0.908130 mae1:0.625402 mae2:0.377521 mae3:0.050889 mae4:0.050803\n",
      "epoch:161 loss:0.194126 accu:0.728760 |mse1:7.037118 :mse2:0.894409 mae1:0.741558 mae2:0.437677 mae3:0.053065 mae4:0.052955\n",
      "epoch:162 loss:0.411263 accu:0.478271 |mse1:6.096359 :mse2:0.909070 mae1:0.495876 mae2:0.378657 mae3:0.048125 mae4:0.048089\n",
      "epoch:163 loss:0.282253 accu:0.417969 |mse1:5.899960 :mse2:1.005191 mae1:0.742810 mae2:0.342616 mae3:0.034278 mae4:0.034219\n",
      "epoch:164 loss:0.305177 accu:0.482422 |mse1:6.434428 :mse2:0.894138 mae1:0.575411 mae2:0.399516 mae3:0.051159 mae4:0.051096\n",
      "epoch:165 loss:0.441076 accu:0.221924 |mse1:5.546307 :mse2:0.767726 mae1:0.609465 mae2:0.342889 mae3:0.036217 mae4:0.036167\n",
      "epoch:166 loss:0.467700 accu:0.438232 |mse1:5.137260 :mse2:0.978401 mae1:0.510458 mae2:0.295371 mae3:0.036946 mae4:0.036942\n",
      "epoch:167 loss:0.205329 accu:0.731934 |mse1:6.547607 :mse2:0.897870 mae1:0.891711 mae2:0.387777 mae3:0.044320 mae4:0.044225\n",
      "epoch:168 loss:0.258905 accu:0.601318 |mse1:5.975796 :mse2:0.892054 mae1:0.696423 mae2:0.358935 mae3:0.039483 mae4:0.039378\n",
      "epoch:169 loss:0.156584 accu:0.807861 |mse1:7.853287 :mse2:0.949236 mae1:0.942754 mae2:0.493026 mae3:0.048231 mae4:0.048082\n",
      "epoch:170 loss:0.253221 accu:0.669678 |mse1:6.628538 :mse2:0.889028 mae1:0.610372 mae2:0.420882 mae3:0.045047 mae4:0.044934\n",
      "epoch:171 loss:0.490377 accu:0.219727 |mse1:5.814198 :mse2:0.983486 mae1:0.467170 mae2:0.363877 mae3:0.032958 mae4:0.032951\n",
      "epoch:172 loss:0.181701 accu:0.775879 |mse1:6.300231 :mse2:0.923064 mae1:0.846700 mae2:0.364726 mae3:0.046678 mae4:0.046590\n",
      "epoch:173 loss:0.417140 accu:0.241699 |mse1:6.208096 :mse2:0.813970 mae1:0.534159 mae2:0.400848 mae3:0.040478 mae4:0.040395\n",
      "epoch:174 loss:0.331192 accu:0.264648 |mse1:6.058731 :mse2:0.924156 mae1:0.630223 mae2:0.377449 mae3:0.031919 mae4:0.031863\n",
      "epoch:175 loss:0.231362 accu:0.634521 |mse1:7.333476 :mse2:1.101449 mae1:0.759213 mae2:0.425918 mae3:0.071020 mae4:0.070868\n",
      "epoch:176 loss:0.539256 accu:0.333984 |mse1:6.118689 :mse2:0.547790 mae1:0.661662 mae2:0.405778 mae3:0.040071 mae4:0.039958\n",
      "epoch:177 loss:0.332613 accu:0.532227 |mse1:6.488493 :mse2:1.103179 mae1:0.521095 mae2:0.393951 mae3:0.048365 mae4:0.048335\n",
      "epoch:178 loss:0.327631 accu:0.446533 |mse1:6.276586 :mse2:0.744886 mae1:0.683381 mae2:0.390066 mae3:0.050862 mae4:0.050805\n",
      "epoch:179 loss:0.146334 accu:0.816650 |mse1:7.732574 :mse2:1.026630 mae1:0.775601 mae2:0.482703 mae3:0.056049 mae4:0.055894\n",
      "epoch:180 loss:0.139289 accu:0.858398 |mse1:7.128537 :mse2:0.880284 mae1:0.746576 mae2:0.436574 mae3:0.062610 mae4:0.062492\n",
      "epoch:181 loss:0.317606 accu:0.551514 |mse1:6.917024 :mse2:0.988440 mae1:0.551957 mae2:0.443157 mae3:0.045382 mae4:0.045301\n",
      "epoch:182 loss:0.225263 accu:0.589600 |mse1:6.705042 :mse2:0.887354 mae1:0.632887 mae2:0.422703 mae3:0.048423 mae4:0.048338\n",
      "epoch:183 loss:0.274769 accu:0.577393 |mse1:6.763564 :mse2:0.988223 mae1:0.548772 mae2:0.416915 mae3:0.058833 mae4:0.058760\n",
      "epoch:184 loss:0.302925 accu:0.392334 |mse1:6.526396 :mse2:0.879630 mae1:0.638007 mae2:0.409295 mae3:0.045861 mae4:0.045798\n",
      "epoch:185 loss:0.377482 accu:0.262939 |mse1:5.569227 :mse2:1.016502 mae1:0.523964 mae2:0.334476 mae3:0.031756 mae4:0.031713\n",
      "epoch:186 loss:0.216022 accu:0.645752 |mse1:5.711418 :mse2:0.880779 mae1:0.753950 mae2:0.328885 mae3:0.041059 mae4:0.040979\n",
      "epoch:187 loss:0.435948 accu:0.385498 |mse1:5.371445 :mse2:1.126865 mae1:0.453945 mae2:0.312824 mae3:0.032011 mae4:0.031991\n",
      "epoch:188 loss:0.237244 accu:0.634521 |mse1:6.207811 :mse2:0.927965 mae1:0.780491 mae2:0.349368 mae3:0.059054 mae4:0.058972\n",
      "epoch:189 loss:0.362734 accu:0.406250 |mse1:5.707803 :mse2:0.797216 mae1:0.684558 mae2:0.332908 mae3:0.050372 mae4:0.050283\n",
      "epoch:190 loss:0.187880 accu:0.678223 |mse1:5.864563 :mse2:0.973266 mae1:0.703124 mae2:0.329438 mae3:0.050591 mae4:0.050523\n",
      "epoch:191 loss:0.278869 accu:0.581299 |mse1:6.397853 :mse2:0.852880 mae1:0.673671 mae2:0.393307 mae3:0.048980 mae4:0.048908\n",
      "epoch:192 loss:0.250473 accu:0.590820 |mse1:6.208611 :mse2:0.991788 mae1:0.593695 mae2:0.367241 mae3:0.053115 mae4:0.053060\n",
      "epoch:193 loss:0.225475 accu:0.660400 |mse1:6.745843 :mse2:0.970832 mae1:0.686162 mae2:0.418496 mae3:0.043898 mae4:0.043805\n",
      "epoch:194 loss:0.207141 accu:0.701904 |mse1:6.475930 :mse2:1.004397 mae1:0.655293 mae2:0.398740 mae3:0.038712 mae4:0.038647\n",
      "epoch:195 loss:0.327004 accu:0.509277 |mse1:6.181091 :mse2:0.923886 mae1:0.576881 mae2:0.380996 mae3:0.044939 mae4:0.044911\n",
      "epoch:196 loss:0.307813 accu:0.518066 |mse1:6.117455 :mse2:0.924830 mae1:0.723692 mae2:0.365926 mae3:0.040347 mae4:0.040293\n",
      "epoch:197 loss:0.190765 accu:0.748047 |mse1:6.672546 :mse2:0.972908 mae1:0.717872 mae2:0.404315 mae3:0.048478 mae4:0.048410\n",
      "epoch:198 loss:0.255081 accu:0.613037 |mse1:5.862176 :mse2:0.913283 mae1:0.625307 mae2:0.345998 mae3:0.047260 mae4:0.047194\n",
      "epoch:199 loss:0.147855 accu:0.823975 |mse1:6.191754 :mse2:0.963235 mae1:0.806793 mae2:0.358086 mae3:0.043157 mae4:0.043089\n",
      "epoch:200 loss:0.416049 accu:0.487549 |mse1:5.975064 :mse2:1.038446 mae1:0.524667 mae2:0.361277 mae3:0.039725 mae4:0.039714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:201 loss:0.252432 accu:0.604492 |mse1:5.909969 :mse2:0.914918 mae1:0.738307 mae2:0.355957 mae3:0.031103 mae4:0.031062\n",
      "epoch:202 loss:0.152767 accu:0.806885 |mse1:6.410722 :mse2:1.025745 mae1:0.652340 mae2:0.395254 mae3:0.034925 mae4:0.034818\n",
      "epoch:203 loss:0.250755 accu:0.725098 |mse1:6.889791 :mse2:0.955908 mae1:0.795326 mae2:0.414414 mae3:0.051963 mae4:0.051876\n",
      "epoch:204 loss:0.611001 accu:0.114502 |mse1:6.868327 :mse2:0.740222 mae1:0.527899 mae2:0.475644 mae3:0.033122 mae4:0.033086\n",
      "epoch:205 loss:0.192312 accu:0.632812 |mse1:6.464798 :mse2:1.034609 mae1:0.688270 mae2:0.380120 mae3:0.049684 mae4:0.049617\n",
      "epoch:206 loss:0.505937 accu:0.184326 |mse1:5.886548 :mse2:0.831780 mae1:0.487260 mae2:0.382981 mae3:0.032114 mae4:0.032087\n",
      "epoch:207 loss:0.165440 accu:0.699707 |mse1:6.294809 :mse2:1.007177 mae1:0.689899 mae2:0.373683 mae3:0.043610 mae4:0.043557\n",
      "epoch:208 loss:0.259340 accu:0.576660 |mse1:6.482959 :mse2:0.977935 mae1:0.579205 mae2:0.396991 mae3:0.050698 mae4:0.050623\n",
      "epoch:209 loss:0.185704 accu:0.742432 |mse1:7.122281 :mse2:0.927278 mae1:0.724104 mae2:0.453858 mae3:0.043435 mae4:0.043347\n",
      "epoch:210 loss:0.263637 accu:0.625000 |mse1:6.417333 :mse2:1.098786 mae1:0.662782 mae2:0.387925 mae3:0.034192 mae4:0.034136\n",
      "epoch:211 loss:0.250713 accu:0.602539 |mse1:6.832232 :mse2:0.912938 mae1:0.580417 mae2:0.430030 mae3:0.055483 mae4:0.055419\n",
      "epoch:212 loss:0.190779 accu:0.775879 |mse1:7.245090 :mse2:0.960891 mae1:0.722808 mae2:0.459911 mae3:0.044806 mae4:0.044701\n",
      "epoch:213 loss:0.388569 accu:0.538086 |mse1:6.190450 :mse2:0.957169 mae1:0.576534 mae2:0.386326 mae3:0.036405 mae4:0.036398\n",
      "epoch:214 loss:0.227902 accu:0.606201 |mse1:5.871547 :mse2:0.978716 mae1:0.768516 mae2:0.325071 mae3:0.049406 mae4:0.049359\n",
      "epoch:215 loss:0.282213 accu:0.475586 |mse1:5.908966 :mse2:0.879520 mae1:0.659863 mae2:0.354963 mae3:0.041627 mae4:0.041571\n",
      "epoch:216 loss:0.247628 accu:0.560303 |mse1:6.012628 :mse2:1.017303 mae1:0.603105 mae2:0.349156 mae3:0.049621 mae4:0.049561\n",
      "epoch:217 loss:0.175191 accu:0.762695 |mse1:5.964307 :mse2:0.897402 mae1:0.760439 mae2:0.340797 mae3:0.050019 mae4:0.049946\n",
      "epoch:218 loss:0.165252 accu:0.781250 |mse1:6.859643 :mse2:0.992547 mae1:0.680676 mae2:0.416869 mae3:0.053512 mae4:0.053421\n",
      "epoch:219 loss:0.264405 accu:0.614014 |mse1:6.258490 :mse2:0.904237 mae1:0.583428 mae2:0.386243 mae3:0.046965 mae4:0.046925\n",
      "epoch:220 loss:0.232636 accu:0.607178 |mse1:6.294690 :mse2:0.972412 mae1:0.629906 mae2:0.377795 mae3:0.049053 mae4:0.049001\n",
      "epoch:221 loss:0.221124 accu:0.629150 |mse1:6.269856 :mse2:0.975845 mae1:0.563094 mae2:0.370330 mae3:0.059177 mae4:0.059143\n",
      "epoch:222 loss:0.212101 accu:0.753174 |mse1:7.171811 :mse2:0.915271 mae1:0.704550 mae2:0.458658 mae3:0.045330 mae4:0.045255\n"
     ]
    }
   ],
   "source": [
    "#tuple类型相加 相当于cat连接\n",
    "real_labels = np.ones(shape=(BATCH_SIZE , )+disc_patch) \n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , )+disc_patch)\n",
    "\n",
    "for i in range(1001):\n",
    "    edges , shoes = load_image()\n",
    "    \n",
    "    fake_shoes = generator_apple2orange.predict(edges)\n",
    "    fake_edges = generator_orange2apple.predict(shoes)\n",
    "    #训练判别器\n",
    "    edges_loss = discriminator_apple.train_on_batch(edges , real_labels)\n",
    "    fake_edges_loss = discriminator_apple.train_on_batch(fake_edges , fake_labels)\n",
    "    loss_edges = np.add(edges_loss , fake_edges_loss)/2\n",
    "\n",
    "    shoes_loss = discriminator_orange.train_on_batch(shoes , real_labels)\n",
    "    fake_shoes_loss = discriminator_orange.train_on_batch(fake_shoes , fake_labels)\n",
    "    loss_shoes = np.add(shoes_loss , fake_shoes_loss)/2\n",
    "\n",
    "    loss = np.add(loss_edges , loss_shoes)/2\n",
    "\n",
    "    #训练生成器\n",
    "    generator_loss = combined.train_on_batch([edges , shoes] , [real_labels , real_labels , shoes , edges , edges , shoes])\n",
    "    \n",
    "    print('epoch:%d loss:%f accu:%f | mse1:%f :mse2:%f mae1:%f mae2:%f mae3:%f mae4:%f' % (i , loss[0] , loss[1] , generator_loss[0] , generator_loss[1] , generator_loss[2] , generator_loss[3] , generator_loss[4] , generator_loss[5]))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        write_image(i)\n",
    "\n",
    "write_image(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 512, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.misc.imread('../dataset/edges2shoes/train/10000_AB.jpg' , mode='RGB').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
