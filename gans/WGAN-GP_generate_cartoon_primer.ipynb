{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.initializers import truncated_normal , random_normal , constant\n",
    "\n",
    "from keras.optimizers import Adam , RMSprop\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 96\n",
    "HEIGHT = 96\n",
    "CHANNEL = 3\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = 'faces/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n",
    "\n",
    "#为WGAN增加的\n",
    "N_CRITIC = 5 #训练G时使用\n",
    "CLIP_VALUE = 0.01 #更新G的权重参数时进行截断使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(X_train , y_train),(X_test , y_test) = mnist.load_data()\\nX_train = X_train/127.5-1\\nX_train = np.expand_dims(X_train , 3)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(X_train , y_train),(X_test , y_test) = mnist.load_data()\n",
    "X_train = X_train/127.5-1\n",
    "X_train = np.expand_dims(X_train , 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_mnist():\\n    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\\n    \\ndef write_image_mnist(epoch):\\n    \\n    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\\n    generated_image = generator_i.predict(noise)\\n    generated_image = generated_image*0.5+0.5\\n    \\n    fig , axes = plt.pyplot.subplots(ROW , COL)\\n    \\n    count=0\\n    \\n    for i in range(ROW):\\n        for j in range(COL):\\n            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\\n            axes[i][j].axis('off')\\n            count += 1\\n            \\n    fig.savefig('mnist_wgan/No.%d.png' % epoch)\\n    plt.pyplot.close()\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_mnist():\n",
    "    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\n",
    "    \n",
    "def write_image_mnist(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = generated_image*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('mnist_wgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('generated_faces_wgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(output_size):\n",
    "    return Conv2D(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def dense(output_size):\n",
    "    return Dense(output_size , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def deconv2d(output_size):\n",
    "    return Conv2DTranspose(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def batch_norm():\n",
    "    return BatchNormalization(momentum=0.9 , epsilon=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "    \n",
    "    #cartoon 图像使用 96*96*3\n",
    "    model.add(Dense(6*6*8*64 , input_shape=(LATENT_DIM,) , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0)))\n",
    "    \n",
    "    model.add(Reshape((6, 6, 64*8)))\n",
    "    \n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(deconv2d(64*4))\n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(deconv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(deconv2d(64*1))\n",
    "    model.add(batch_norm())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(deconv2d(3))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    image = model(noise)\n",
    "    \n",
    "    return Model(noise , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def critic():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='critic')\n",
    "    \n",
    "    model.add(Conv2D(filters=64 , kernel_size=(5,5) , strides=(2,2) , padding='same' , input_shape=(WIDTH , HEIGHT , CHANNEL) , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0) , name='conv1'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(conv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(conv2d(64*4))\n",
    "    model.add(batch_norm())  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    \n",
    "    model.add(conv2d(64*8))\n",
    "    model.add(batch_norm())  \n",
    "    model.add(LeakyReLU(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #===\n",
    "    #如果没有下面的两个FC层 训练时发生损失不下降 且生成不出图像\n",
    "    #model.add(dense(1024))\n",
    "    #model.add(LeakyReLU(0.2))\n",
    "    \n",
    "    model.add(dense(256))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    #===\n",
    "    model.add(dense(1))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    validity = model(image)\n",
    "    \n",
    "    return Model(image , validity , name='critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsprop = RMSprop(lr=0.00005)\n",
    "\n",
    "def wgan_loss(y_true , y_pred):\n",
    "    return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 48, 48, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 256)       819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4718848   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,029,249\n",
      "Trainable params: 9,027,457\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 18432)             1861632   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 12, 12, 256)       3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 96, 96, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 96, 96, 3)         0         \n",
      "=================================================================\n",
      "Total params: 6,171,523\n",
      "Trainable params: 6,169,603\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_i = critic()\n",
    "critic_i.compile(optimizer=rmsprop , loss=wgan_loss , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator()\n",
    "\n",
    "z = Input(shape=(LATENT_DIM , ) , name='z')\n",
    "image = generator_i(z)\n",
    "validity = critic_i(image)\n",
    "\n",
    "combined_model_i = Model(z , validity)\n",
    "combined_model_i.compile(optimizer=rmsprop , loss=wgan_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:1.001127 gene_loss:1.000315\n",
      "epoch:1 loss:0.999923 gene_loss:1.008464\n",
      "epoch:2 loss:0.998405 gene_loss:1.016632\n",
      "epoch:3 loss:0.996507 gene_loss:1.025243\n",
      "epoch:4 loss:0.995589 gene_loss:1.032789\n",
      "epoch:5 loss:0.996031 gene_loss:1.037650\n",
      "epoch:6 loss:0.997686 gene_loss:1.039668\n",
      "epoch:7 loss:1.002765 gene_loss:1.038513\n",
      "epoch:8 loss:1.008644 gene_loss:1.032182\n",
      "epoch:9 loss:1.020407 gene_loss:1.020413\n",
      "epoch:10 loss:1.036939 gene_loss:1.004989\n",
      "epoch:11 loss:1.054953 gene_loss:0.989337\n",
      "epoch:12 loss:1.072437 gene_loss:0.969723\n",
      "epoch:13 loss:1.094888 gene_loss:0.951461\n",
      "epoch:14 loss:1.118298 gene_loss:0.930042\n",
      "epoch:15 loss:1.140145 gene_loss:0.908196\n",
      "epoch:16 loss:1.169432 gene_loss:0.890403\n",
      "epoch:17 loss:1.203541 gene_loss:0.868130\n",
      "epoch:18 loss:1.237040 gene_loss:0.851855\n",
      "epoch:19 loss:1.275418 gene_loss:0.839845\n",
      "epoch:20 loss:1.317360 gene_loss:0.850781\n",
      "epoch:21 loss:1.356701 gene_loss:0.822613\n",
      "epoch:22 loss:1.406108 gene_loss:0.803189\n",
      "epoch:23 loss:1.453833 gene_loss:0.730712\n",
      "epoch:24 loss:1.503145 gene_loss:0.660487\n",
      "epoch:25 loss:1.556286 gene_loss:0.572624\n",
      "epoch:26 loss:1.612494 gene_loss:0.497428\n",
      "epoch:27 loss:1.685369 gene_loss:0.424345\n",
      "epoch:28 loss:1.747935 gene_loss:0.352773\n",
      "epoch:29 loss:1.812506 gene_loss:0.294590\n",
      "epoch:30 loss:1.883172 gene_loss:0.234050\n",
      "epoch:31 loss:1.951581 gene_loss:0.175053\n",
      "epoch:32 loss:2.001967 gene_loss:0.118656\n",
      "epoch:33 loss:2.080202 gene_loss:0.063890\n",
      "epoch:34 loss:2.133182 gene_loss:0.013542\n",
      "epoch:35 loss:2.206329 gene_loss:-0.040695\n",
      "epoch:36 loss:2.257242 gene_loss:-0.085917\n",
      "epoch:37 loss:2.301052 gene_loss:-0.125414\n",
      "epoch:38 loss:2.361723 gene_loss:-0.164860\n",
      "epoch:39 loss:2.394656 gene_loss:-0.193745\n",
      "epoch:40 loss:2.442725 gene_loss:-0.236362\n",
      "epoch:41 loss:2.486637 gene_loss:-0.274904\n",
      "epoch:42 loss:2.527537 gene_loss:-0.313721\n",
      "epoch:43 loss:2.566086 gene_loss:-0.345679\n",
      "epoch:44 loss:2.604955 gene_loss:-0.383209\n",
      "epoch:45 loss:2.643816 gene_loss:-0.419208\n",
      "epoch:46 loss:2.685416 gene_loss:-0.459048\n",
      "epoch:47 loss:2.719356 gene_loss:-0.491216\n",
      "epoch:48 loss:2.763459 gene_loss:-0.531942\n",
      "epoch:49 loss:2.803291 gene_loss:-0.571226\n",
      "epoch:50 loss:2.831265 gene_loss:-0.603684\n",
      "epoch:51 loss:2.880215 gene_loss:-0.647703\n",
      "epoch:52 loss:2.910245 gene_loss:-0.686189\n",
      "epoch:53 loss:2.952039 gene_loss:-0.727049\n",
      "epoch:54 loss:2.989157 gene_loss:-0.760937\n",
      "epoch:55 loss:3.020949 gene_loss:-0.797674\n",
      "epoch:56 loss:3.061838 gene_loss:-0.833395\n",
      "epoch:57 loss:3.091522 gene_loss:-0.867302\n",
      "epoch:58 loss:3.131670 gene_loss:-0.905598\n",
      "epoch:59 loss:3.167137 gene_loss:-0.940843\n",
      "epoch:60 loss:3.196517 gene_loss:-0.974877\n",
      "epoch:61 loss:3.228098 gene_loss:-1.004140\n",
      "epoch:62 loss:3.261901 gene_loss:-1.039706\n",
      "epoch:63 loss:3.291902 gene_loss:-1.074041\n",
      "epoch:64 loss:3.305033 gene_loss:-1.104000\n",
      "epoch:65 loss:3.347105 gene_loss:-1.139228\n",
      "epoch:66 loss:3.366226 gene_loss:-1.172079\n",
      "epoch:67 loss:3.397232 gene_loss:-1.202658\n",
      "epoch:68 loss:3.419755 gene_loss:-1.234471\n",
      "epoch:69 loss:3.439651 gene_loss:-1.265345\n",
      "epoch:70 loss:3.459563 gene_loss:-1.295263\n",
      "epoch:71 loss:3.479715 gene_loss:-1.322616\n",
      "epoch:72 loss:3.497910 gene_loss:-1.352348\n",
      "epoch:73 loss:3.512295 gene_loss:-1.377490\n",
      "epoch:74 loss:3.530406 gene_loss:-1.406533\n",
      "epoch:75 loss:3.546001 gene_loss:-1.433041\n",
      "epoch:76 loss:3.561554 gene_loss:-1.457602\n",
      "epoch:77 loss:3.574245 gene_loss:-1.482438\n",
      "epoch:78 loss:3.583889 gene_loss:-1.501033\n",
      "epoch:79 loss:3.597928 gene_loss:-1.518293\n",
      "epoch:80 loss:3.609708 gene_loss:-1.534047\n",
      "epoch:81 loss:3.620538 gene_loss:-1.547837\n",
      "epoch:82 loss:3.621978 gene_loss:-1.557613\n",
      "epoch:83 loss:3.632392 gene_loss:-1.567283\n",
      "epoch:84 loss:3.640275 gene_loss:-1.577163\n",
      "epoch:85 loss:3.638925 gene_loss:-1.584474\n",
      "epoch:86 loss:3.650624 gene_loss:-1.592695\n",
      "epoch:87 loss:3.654027 gene_loss:-1.601669\n",
      "epoch:88 loss:3.656643 gene_loss:-1.609076\n",
      "epoch:89 loss:3.660707 gene_loss:-1.615210\n",
      "epoch:90 loss:3.668849 gene_loss:-1.622658\n",
      "epoch:91 loss:3.674229 gene_loss:-1.629495\n",
      "epoch:92 loss:3.672052 gene_loss:-1.635068\n",
      "epoch:93 loss:3.679713 gene_loss:-1.642030\n",
      "epoch:94 loss:3.686223 gene_loss:-1.647076\n",
      "epoch:95 loss:3.685687 gene_loss:-1.652333\n",
      "epoch:96 loss:3.692209 gene_loss:-1.658522\n",
      "epoch:97 loss:3.695032 gene_loss:-1.661148\n",
      "epoch:98 loss:3.699382 gene_loss:-1.668200\n",
      "epoch:99 loss:3.697753 gene_loss:-1.670521\n",
      "epoch:100 loss:3.704312 gene_loss:-1.676044\n",
      "epoch:101 loss:3.706075 gene_loss:-1.679758\n",
      "epoch:102 loss:3.710616 gene_loss:-1.684948\n",
      "epoch:103 loss:3.711942 gene_loss:-1.688587\n",
      "epoch:104 loss:3.713815 gene_loss:-1.691820\n",
      "epoch:105 loss:3.718537 gene_loss:-1.694379\n",
      "epoch:106 loss:3.718784 gene_loss:-1.697162\n",
      "epoch:107 loss:3.723544 gene_loss:-1.702358\n",
      "epoch:108 loss:3.723925 gene_loss:-1.705518\n",
      "epoch:109 loss:3.718512 gene_loss:-1.708559\n",
      "epoch:110 loss:3.726033 gene_loss:-1.710948\n",
      "epoch:111 loss:3.731574 gene_loss:-1.714330\n",
      "epoch:112 loss:3.731493 gene_loss:-1.715525\n",
      "epoch:113 loss:3.737200 gene_loss:-1.718676\n",
      "epoch:114 loss:3.736330 gene_loss:-1.721133\n",
      "epoch:115 loss:3.739796 gene_loss:-1.724938\n",
      "epoch:116 loss:3.740709 gene_loss:-1.726935\n",
      "epoch:117 loss:3.744138 gene_loss:-1.727981\n",
      "epoch:118 loss:3.738711 gene_loss:-1.731167\n",
      "epoch:119 loss:3.746326 gene_loss:-1.733731\n",
      "epoch:120 loss:3.743378 gene_loss:-1.733890\n",
      "epoch:121 loss:3.746751 gene_loss:-1.737739\n",
      "epoch:122 loss:3.752242 gene_loss:-1.739177\n",
      "epoch:123 loss:3.751496 gene_loss:-1.741562\n",
      "epoch:124 loss:3.751567 gene_loss:-1.743072\n",
      "epoch:125 loss:3.752934 gene_loss:-1.743101\n",
      "epoch:126 loss:3.752802 gene_loss:-1.746555\n",
      "epoch:127 loss:3.755114 gene_loss:-1.747952\n",
      "epoch:128 loss:3.756232 gene_loss:-1.749532\n",
      "epoch:129 loss:3.758331 gene_loss:-1.751667\n",
      "epoch:130 loss:3.759072 gene_loss:-1.753166\n",
      "epoch:131 loss:3.760091 gene_loss:-1.754739\n",
      "epoch:132 loss:3.757671 gene_loss:-1.755946\n",
      "epoch:133 loss:3.758915 gene_loss:-1.756671\n",
      "epoch:134 loss:3.762896 gene_loss:-1.758253\n",
      "epoch:135 loss:3.763041 gene_loss:-1.760204\n",
      "epoch:136 loss:3.763673 gene_loss:-1.761023\n",
      "epoch:137 loss:3.765809 gene_loss:-1.762621\n",
      "epoch:138 loss:3.765909 gene_loss:-1.763988\n",
      "epoch:139 loss:3.767782 gene_loss:-1.765033\n",
      "epoch:140 loss:3.766973 gene_loss:-1.767235\n",
      "epoch:141 loss:3.768476 gene_loss:-1.766695\n",
      "epoch:142 loss:3.769555 gene_loss:-1.769090\n",
      "epoch:143 loss:3.770428 gene_loss:-1.769086\n",
      "epoch:144 loss:3.770415 gene_loss:-1.770559\n",
      "epoch:145 loss:3.773104 gene_loss:-1.771285\n",
      "epoch:146 loss:3.770839 gene_loss:-1.771626\n",
      "epoch:147 loss:3.773201 gene_loss:-1.772798\n",
      "epoch:148 loss:3.774508 gene_loss:-1.773865\n",
      "epoch:149 loss:3.772790 gene_loss:-1.773795\n",
      "epoch:150 loss:3.774410 gene_loss:-1.775942\n",
      "epoch:151 loss:3.773492 gene_loss:-1.776581\n",
      "epoch:152 loss:3.778175 gene_loss:-1.778020\n",
      "epoch:153 loss:3.778028 gene_loss:-1.779490\n",
      "epoch:154 loss:3.778432 gene_loss:-1.779980\n",
      "epoch:155 loss:3.774944 gene_loss:-1.780441\n",
      "epoch:156 loss:3.779183 gene_loss:-1.781442\n",
      "epoch:157 loss:3.778644 gene_loss:-1.782157\n",
      "epoch:158 loss:3.779918 gene_loss:-1.782305\n",
      "epoch:159 loss:3.775959 gene_loss:-1.783312\n",
      "epoch:160 loss:3.779424 gene_loss:-1.781494\n",
      "epoch:161 loss:3.781421 gene_loss:-1.783125\n",
      "epoch:162 loss:3.782522 gene_loss:-1.783978\n",
      "epoch:163 loss:3.782725 gene_loss:-1.784809\n",
      "epoch:164 loss:3.784337 gene_loss:-1.784907\n",
      "epoch:165 loss:3.783313 gene_loss:-1.785260\n",
      "epoch:166 loss:3.782701 gene_loss:-1.786436\n",
      "epoch:167 loss:3.783513 gene_loss:-1.786444\n",
      "epoch:168 loss:3.785626 gene_loss:-1.787279\n",
      "epoch:169 loss:3.783378 gene_loss:-1.787020\n",
      "epoch:170 loss:3.783691 gene_loss:-1.786552\n",
      "epoch:171 loss:3.786346 gene_loss:-1.787113\n",
      "epoch:172 loss:3.783550 gene_loss:-1.786677\n",
      "epoch:173 loss:3.785024 gene_loss:-1.786784\n",
      "epoch:174 loss:3.783576 gene_loss:-1.785909\n",
      "epoch:175 loss:3.777684 gene_loss:-1.783806\n",
      "epoch:176 loss:3.781007 gene_loss:-1.782220\n",
      "epoch:177 loss:3.778171 gene_loss:-1.780376\n",
      "epoch:178 loss:3.774236 gene_loss:-1.769662\n",
      "epoch:179 loss:3.768580 gene_loss:-1.776726\n",
      "epoch:180 loss:3.770544 gene_loss:-1.775689\n",
      "epoch:181 loss:3.773213 gene_loss:-1.776524\n",
      "epoch:182 loss:3.771810 gene_loss:-1.771215\n",
      "epoch:183 loss:3.776865 gene_loss:-1.778102\n",
      "epoch:184 loss:3.777560 gene_loss:-1.780622\n",
      "epoch:185 loss:3.772562 gene_loss:-1.772293\n",
      "epoch:186 loss:3.774610 gene_loss:-1.784094\n",
      "epoch:187 loss:3.780362 gene_loss:-1.785410\n",
      "epoch:188 loss:3.779997 gene_loss:-1.784518\n",
      "epoch:189 loss:3.780565 gene_loss:-1.787578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:190 loss:3.781750 gene_loss:-1.784286\n",
      "epoch:191 loss:3.784108 gene_loss:-1.790191\n",
      "epoch:192 loss:3.782423 gene_loss:-1.781964\n",
      "epoch:193 loss:3.785064 gene_loss:-1.790474\n",
      "epoch:194 loss:3.784670 gene_loss:-1.788048\n",
      "epoch:195 loss:3.786098 gene_loss:-1.792413\n",
      "epoch:196 loss:3.782448 gene_loss:-1.791641\n",
      "epoch:197 loss:3.785310 gene_loss:-1.792556\n",
      "epoch:198 loss:3.789949 gene_loss:-1.795129\n",
      "epoch:199 loss:3.783939 gene_loss:-1.789996\n",
      "epoch:200 loss:3.788069 gene_loss:-1.792017\n",
      "epoch:201 loss:3.788654 gene_loss:-1.796126\n",
      "epoch:202 loss:3.787542 gene_loss:-1.795836\n",
      "epoch:203 loss:3.787554 gene_loss:-1.793746\n",
      "epoch:204 loss:3.787971 gene_loss:-1.796203\n",
      "epoch:205 loss:3.789522 gene_loss:-1.796383\n",
      "epoch:206 loss:3.789460 gene_loss:-1.794634\n",
      "epoch:207 loss:3.786931 gene_loss:-1.793903\n",
      "epoch:208 loss:3.790461 gene_loss:-1.789697\n",
      "epoch:209 loss:3.788538 gene_loss:-1.782940\n",
      "epoch:210 loss:3.789866 gene_loss:-1.781554\n",
      "epoch:211 loss:3.791476 gene_loss:-1.776756\n",
      "epoch:212 loss:3.791499 gene_loss:-1.775890\n",
      "epoch:213 loss:3.791672 gene_loss:-1.775068\n",
      "epoch:214 loss:3.791849 gene_loss:-1.769117\n",
      "epoch:215 loss:3.789423 gene_loss:-1.755816\n",
      "epoch:216 loss:3.792775 gene_loss:-1.741651\n",
      "epoch:217 loss:3.791662 gene_loss:-1.697898\n",
      "epoch:218 loss:3.793099 gene_loss:-1.673859\n",
      "epoch:219 loss:3.793225 gene_loss:-1.619380\n",
      "epoch:220 loss:3.793324 gene_loss:-1.584662\n",
      "epoch:221 loss:3.790112 gene_loss:-1.555686\n",
      "epoch:222 loss:3.791212 gene_loss:-1.563794\n",
      "epoch:223 loss:3.790455 gene_loss:-1.574137\n",
      "epoch:224 loss:3.783873 gene_loss:-1.543546\n",
      "epoch:225 loss:3.787496 gene_loss:-1.601482\n",
      "epoch:226 loss:3.788048 gene_loss:-1.669103\n",
      "epoch:227 loss:3.786511 gene_loss:-1.727823\n",
      "epoch:228 loss:3.789989 gene_loss:-1.758373\n",
      "epoch:229 loss:3.792440 gene_loss:-1.778761\n",
      "epoch:230 loss:3.792098 gene_loss:-1.787700\n",
      "epoch:231 loss:3.787804 gene_loss:-1.787287\n",
      "epoch:232 loss:3.798922 gene_loss:-1.802724\n",
      "epoch:233 loss:3.789526 gene_loss:-1.798803\n",
      "epoch:234 loss:3.793625 gene_loss:-1.803987\n",
      "epoch:235 loss:3.798327 gene_loss:-1.808477\n",
      "epoch:236 loss:3.794465 gene_loss:-1.796440\n",
      "epoch:237 loss:3.799207 gene_loss:-1.809375\n",
      "epoch:238 loss:3.799518 gene_loss:-1.808632\n",
      "epoch:239 loss:3.797750 gene_loss:-1.809079\n",
      "epoch:240 loss:3.798042 gene_loss:-1.809800\n",
      "epoch:241 loss:3.802856 gene_loss:-1.813064\n",
      "epoch:242 loss:3.799866 gene_loss:-1.809214\n",
      "epoch:243 loss:3.800783 gene_loss:-1.811902\n",
      "epoch:244 loss:3.802338 gene_loss:-1.813879\n",
      "epoch:245 loss:3.797843 gene_loss:-1.804554\n",
      "epoch:246 loss:3.803388 gene_loss:-1.814116\n",
      "epoch:247 loss:3.801426 gene_loss:-1.811881\n",
      "epoch:248 loss:3.802654 gene_loss:-1.815857\n",
      "epoch:249 loss:3.802880 gene_loss:-1.814541\n",
      "epoch:250 loss:3.802213 gene_loss:-1.814086\n",
      "epoch:251 loss:3.802032 gene_loss:-1.815085\n",
      "epoch:252 loss:3.801357 gene_loss:-1.814095\n",
      "epoch:253 loss:3.803982 gene_loss:-1.814271\n",
      "epoch:254 loss:3.805248 gene_loss:-1.817162\n",
      "epoch:255 loss:3.804402 gene_loss:-1.816410\n",
      "epoch:256 loss:3.803926 gene_loss:-1.817124\n",
      "epoch:257 loss:3.803294 gene_loss:-1.812454\n",
      "epoch:258 loss:3.804286 gene_loss:-1.817371\n",
      "epoch:259 loss:3.802251 gene_loss:-1.814258\n",
      "epoch:260 loss:3.805409 gene_loss:-1.817971\n",
      "epoch:261 loss:3.805225 gene_loss:-1.816161\n",
      "epoch:262 loss:3.806946 gene_loss:-1.816921\n",
      "epoch:263 loss:3.805524 gene_loss:-1.816259\n",
      "epoch:264 loss:3.804509 gene_loss:-1.817583\n",
      "epoch:265 loss:3.805585 gene_loss:-1.817454\n",
      "epoch:266 loss:3.806437 gene_loss:-1.814334\n",
      "epoch:267 loss:3.807036 gene_loss:-1.816990\n",
      "epoch:268 loss:3.806892 gene_loss:-1.817356\n",
      "epoch:269 loss:3.805297 gene_loss:-1.816488\n",
      "epoch:270 loss:3.805820 gene_loss:-1.816895\n",
      "epoch:271 loss:3.806757 gene_loss:-1.815217\n",
      "epoch:272 loss:3.805600 gene_loss:-1.816739\n",
      "epoch:273 loss:3.806815 gene_loss:-1.815963\n",
      "epoch:274 loss:3.804434 gene_loss:-1.814006\n",
      "epoch:275 loss:3.805810 gene_loss:-1.814140\n",
      "epoch:276 loss:3.805981 gene_loss:-1.813401\n",
      "epoch:277 loss:3.806278 gene_loss:-1.813609\n",
      "epoch:278 loss:3.802361 gene_loss:-1.810801\n",
      "epoch:279 loss:3.804833 gene_loss:-1.813118\n",
      "epoch:280 loss:3.805072 gene_loss:-1.809875\n",
      "epoch:281 loss:3.804398 gene_loss:-1.812930\n",
      "epoch:282 loss:3.804790 gene_loss:-1.809503\n",
      "epoch:283 loss:3.805149 gene_loss:-1.813262\n",
      "epoch:284 loss:3.804502 gene_loss:-1.811430\n",
      "epoch:285 loss:3.804041 gene_loss:-1.812493\n",
      "epoch:286 loss:3.803939 gene_loss:-1.814210\n",
      "epoch:287 loss:3.801243 gene_loss:-1.809475\n",
      "epoch:288 loss:3.805312 gene_loss:-1.814116\n",
      "epoch:289 loss:3.804898 gene_loss:-1.811853\n",
      "epoch:290 loss:3.804535 gene_loss:-1.813706\n",
      "epoch:291 loss:3.804664 gene_loss:-1.813694\n",
      "epoch:292 loss:3.800877 gene_loss:-1.814005\n",
      "epoch:293 loss:3.806323 gene_loss:-1.814179\n",
      "epoch:294 loss:3.805294 gene_loss:-1.813245\n",
      "epoch:295 loss:3.805512 gene_loss:-1.812339\n",
      "epoch:296 loss:3.805663 gene_loss:-1.813824\n",
      "epoch:297 loss:3.804857 gene_loss:-1.812903\n",
      "epoch:298 loss:3.804713 gene_loss:-1.811655\n",
      "epoch:299 loss:3.804136 gene_loss:-1.811657\n",
      "epoch:300 loss:3.801135 gene_loss:-1.804302\n",
      "epoch:301 loss:3.803591 gene_loss:-1.806563\n",
      "epoch:302 loss:3.802199 gene_loss:-1.802736\n",
      "epoch:303 loss:3.801551 gene_loss:-1.793342\n",
      "epoch:304 loss:3.800884 gene_loss:-1.779327\n",
      "epoch:305 loss:3.802865 gene_loss:-1.766836\n",
      "epoch:306 loss:3.801784 gene_loss:-1.744412\n",
      "epoch:307 loss:3.797301 gene_loss:-1.731097\n",
      "epoch:308 loss:3.797945 gene_loss:-1.731108\n",
      "epoch:309 loss:3.795601 gene_loss:-1.727599\n",
      "epoch:310 loss:3.789770 gene_loss:-1.730975\n",
      "epoch:311 loss:3.790691 gene_loss:-1.704527\n",
      "epoch:312 loss:3.792439 gene_loss:-1.687754\n",
      "epoch:313 loss:3.787770 gene_loss:-1.550130\n",
      "epoch:314 loss:3.764557 gene_loss:-1.501040\n",
      "epoch:315 loss:3.773172 gene_loss:-1.429505\n",
      "epoch:316 loss:3.731933 gene_loss:-1.285518\n",
      "epoch:317 loss:3.733855 gene_loss:-1.238975\n",
      "epoch:318 loss:3.726767 gene_loss:-1.262404\n",
      "epoch:319 loss:3.698910 gene_loss:-1.132396\n",
      "epoch:320 loss:3.731038 gene_loss:-1.120819\n",
      "epoch:321 loss:3.724544 gene_loss:-1.074607\n",
      "epoch:322 loss:3.723024 gene_loss:-1.022977\n",
      "epoch:323 loss:3.663061 gene_loss:-0.532009\n",
      "epoch:324 loss:3.722918 gene_loss:-0.985017\n",
      "epoch:325 loss:3.724569 gene_loss:-0.370192\n",
      "epoch:326 loss:3.731081 gene_loss:-0.687316\n",
      "epoch:327 loss:3.703062 gene_loss:-0.581824\n",
      "epoch:328 loss:3.669590 gene_loss:-1.393511\n",
      "epoch:329 loss:3.721436 gene_loss:-1.615278\n",
      "epoch:330 loss:3.743350 gene_loss:-1.660924\n",
      "epoch:331 loss:3.730741 gene_loss:-1.706736\n",
      "epoch:332 loss:3.756828 gene_loss:-1.758565\n",
      "epoch:333 loss:3.752987 gene_loss:-1.757255\n",
      "epoch:334 loss:3.774124 gene_loss:-1.785119\n",
      "epoch:335 loss:3.770432 gene_loss:-1.770136\n",
      "epoch:336 loss:3.780846 gene_loss:-1.790696\n",
      "epoch:337 loss:3.776034 gene_loss:-1.786903\n",
      "epoch:338 loss:3.788935 gene_loss:-1.800673\n",
      "epoch:339 loss:3.785104 gene_loss:-1.797357\n",
      "epoch:340 loss:3.776599 gene_loss:-1.791049\n",
      "epoch:341 loss:3.795256 gene_loss:-1.807307\n",
      "epoch:342 loss:3.775424 gene_loss:-1.786758\n",
      "epoch:343 loss:3.794486 gene_loss:-1.804383\n",
      "epoch:344 loss:3.799982 gene_loss:-1.813679\n",
      "epoch:345 loss:3.767846 gene_loss:-1.801992\n",
      "epoch:346 loss:3.797054 gene_loss:-1.808766\n",
      "epoch:347 loss:3.793097 gene_loss:-1.806633\n",
      "epoch:348 loss:3.799021 gene_loss:-1.813970\n",
      "epoch:349 loss:3.798822 gene_loss:-1.806240\n",
      "epoch:350 loss:3.802574 gene_loss:-1.813031\n",
      "epoch:351 loss:3.805068 gene_loss:-1.814529\n",
      "epoch:352 loss:3.795668 gene_loss:-1.812366\n",
      "epoch:353 loss:3.805683 gene_loss:-1.817692\n",
      "epoch:354 loss:3.802242 gene_loss:-1.808895\n",
      "epoch:355 loss:3.802924 gene_loss:-1.804066\n",
      "epoch:356 loss:3.805444 gene_loss:-1.819725\n",
      "epoch:357 loss:3.806638 gene_loss:-1.814448\n",
      "epoch:358 loss:3.803907 gene_loss:-1.813150\n",
      "epoch:359 loss:3.806321 gene_loss:-1.819324\n",
      "epoch:360 loss:3.798701 gene_loss:-1.812038\n",
      "epoch:361 loss:3.805667 gene_loss:-1.816938\n",
      "epoch:362 loss:3.806261 gene_loss:-1.819701\n",
      "epoch:363 loss:3.807879 gene_loss:-1.815219\n",
      "epoch:364 loss:3.803479 gene_loss:-1.816656\n",
      "epoch:365 loss:3.807642 gene_loss:-1.821947\n",
      "epoch:366 loss:3.803086 gene_loss:-1.811975\n",
      "epoch:367 loss:3.808173 gene_loss:-1.818294\n",
      "epoch:368 loss:3.807677 gene_loss:-1.821683\n",
      "epoch:369 loss:3.802689 gene_loss:-1.816927\n",
      "epoch:370 loss:3.807250 gene_loss:-1.821561\n",
      "epoch:371 loss:3.804686 gene_loss:-1.817196\n",
      "epoch:372 loss:3.810136 gene_loss:-1.822657\n",
      "epoch:373 loss:3.808291 gene_loss:-1.821082\n",
      "epoch:374 loss:3.805915 gene_loss:-1.812589\n",
      "epoch:375 loss:3.807537 gene_loss:-1.822759\n",
      "epoch:376 loss:3.808214 gene_loss:-1.820889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:377 loss:3.807688 gene_loss:-1.822373\n",
      "epoch:378 loss:3.809383 gene_loss:-1.819719\n",
      "epoch:379 loss:3.810994 gene_loss:-1.821356\n",
      "epoch:380 loss:3.807566 gene_loss:-1.813003\n",
      "epoch:381 loss:3.807838 gene_loss:-1.824113\n",
      "epoch:382 loss:3.810173 gene_loss:-1.822834\n",
      "epoch:383 loss:3.810127 gene_loss:-1.820177\n",
      "epoch:384 loss:3.809260 gene_loss:-1.822213\n",
      "epoch:385 loss:3.807931 gene_loss:-1.823729\n",
      "epoch:386 loss:3.810270 gene_loss:-1.823319\n",
      "epoch:387 loss:3.809247 gene_loss:-1.823682\n",
      "epoch:388 loss:3.810556 gene_loss:-1.823124\n",
      "epoch:389 loss:3.810388 gene_loss:-1.821549\n",
      "epoch:390 loss:3.811195 gene_loss:-1.823811\n",
      "epoch:391 loss:3.808638 gene_loss:-1.822421\n",
      "epoch:392 loss:3.812251 gene_loss:-1.824443\n",
      "epoch:393 loss:3.809811 gene_loss:-1.820828\n",
      "epoch:394 loss:3.808287 gene_loss:-1.823893\n",
      "epoch:395 loss:3.811481 gene_loss:-1.825128\n",
      "epoch:396 loss:3.810811 gene_loss:-1.818863\n",
      "epoch:397 loss:3.809837 gene_loss:-1.823882\n",
      "epoch:398 loss:3.812311 gene_loss:-1.823926\n",
      "epoch:399 loss:3.810484 gene_loss:-1.824271\n",
      "epoch:400 loss:3.809026 gene_loss:-1.820382\n",
      "epoch:401 loss:3.812315 gene_loss:-1.824237\n",
      "epoch:402 loss:3.812309 gene_loss:-1.820601\n",
      "epoch:403 loss:3.811762 gene_loss:-1.824065\n",
      "epoch:404 loss:3.808608 gene_loss:-1.818965\n",
      "epoch:405 loss:3.811741 gene_loss:-1.823661\n",
      "epoch:406 loss:3.810991 gene_loss:-1.823193\n",
      "epoch:407 loss:3.810102 gene_loss:-1.822387\n",
      "epoch:408 loss:3.809148 gene_loss:-1.819318\n",
      "epoch:409 loss:3.809639 gene_loss:-1.815684\n",
      "epoch:410 loss:3.808734 gene_loss:-1.801108\n",
      "epoch:411 loss:3.809551 gene_loss:-1.788847\n",
      "epoch:412 loss:3.805691 gene_loss:-1.769025\n",
      "epoch:413 loss:3.805843 gene_loss:-1.763325\n",
      "epoch:414 loss:3.806041 gene_loss:-1.730469\n",
      "epoch:415 loss:3.797349 gene_loss:-1.722558\n",
      "epoch:416 loss:3.792761 gene_loss:-1.682653\n",
      "epoch:417 loss:3.765044 gene_loss:-1.656629\n",
      "epoch:418 loss:3.764517 gene_loss:-1.659832\n",
      "epoch:419 loss:3.772322 gene_loss:-1.670825\n",
      "epoch:420 loss:3.721315 gene_loss:-1.678378\n",
      "epoch:421 loss:3.782853 gene_loss:-1.698071\n",
      "epoch:422 loss:3.776870 gene_loss:-1.724171\n",
      "epoch:423 loss:3.780685 gene_loss:-1.736338\n",
      "epoch:424 loss:3.788888 gene_loss:-1.761761\n",
      "epoch:425 loss:3.791326 gene_loss:-1.764488\n",
      "epoch:426 loss:3.798699 gene_loss:-1.784420\n",
      "epoch:427 loss:3.798619 gene_loss:-1.792239\n",
      "epoch:428 loss:3.797789 gene_loss:-1.800712\n",
      "epoch:429 loss:3.801410 gene_loss:-1.799001\n",
      "epoch:430 loss:3.801874 gene_loss:-1.801835\n",
      "epoch:431 loss:3.802903 gene_loss:-1.811213\n",
      "epoch:432 loss:3.804639 gene_loss:-1.809271\n",
      "epoch:433 loss:3.804070 gene_loss:-1.802724\n",
      "epoch:434 loss:3.805688 gene_loss:-1.815627\n",
      "epoch:435 loss:3.807611 gene_loss:-1.817244\n",
      "epoch:436 loss:3.806759 gene_loss:-1.816445\n",
      "epoch:437 loss:3.806688 gene_loss:-1.816818\n",
      "epoch:438 loss:3.798575 gene_loss:-1.813854\n",
      "epoch:439 loss:3.809026 gene_loss:-1.819844\n",
      "epoch:440 loss:3.808912 gene_loss:-1.816542\n",
      "epoch:441 loss:3.807107 gene_loss:-1.819681\n",
      "epoch:442 loss:3.808013 gene_loss:-1.811127\n",
      "epoch:443 loss:3.809299 gene_loss:-1.822712\n",
      "epoch:444 loss:3.808653 gene_loss:-1.818496\n",
      "epoch:445 loss:3.807962 gene_loss:-1.820234\n",
      "epoch:446 loss:3.809407 gene_loss:-1.821910\n",
      "epoch:447 loss:3.810052 gene_loss:-1.821091\n",
      "epoch:448 loss:3.810137 gene_loss:-1.822010\n",
      "epoch:449 loss:3.808286 gene_loss:-1.814508\n",
      "epoch:450 loss:3.811749 gene_loss:-1.823100\n",
      "epoch:451 loss:3.809365 gene_loss:-1.821501\n",
      "epoch:452 loss:3.808923 gene_loss:-1.822903\n",
      "epoch:453 loss:3.810792 gene_loss:-1.822475\n",
      "epoch:454 loss:3.810936 gene_loss:-1.822978\n",
      "epoch:455 loss:3.811069 gene_loss:-1.823602\n",
      "epoch:456 loss:3.811120 gene_loss:-1.822931\n",
      "epoch:457 loss:3.811533 gene_loss:-1.823067\n",
      "epoch:458 loss:3.810719 gene_loss:-1.824116\n",
      "epoch:459 loss:3.809771 gene_loss:-1.819062\n",
      "epoch:460 loss:3.812265 gene_loss:-1.824535\n",
      "epoch:461 loss:3.811483 gene_loss:-1.824527\n",
      "epoch:462 loss:3.810886 gene_loss:-1.820613\n",
      "epoch:463 loss:3.812879 gene_loss:-1.824688\n",
      "epoch:464 loss:3.809874 gene_loss:-1.822417\n",
      "epoch:465 loss:3.812116 gene_loss:-1.822653\n",
      "epoch:466 loss:3.813798 gene_loss:-1.824301\n",
      "epoch:467 loss:3.811842 gene_loss:-1.819504\n",
      "epoch:468 loss:3.812858 gene_loss:-1.824010\n",
      "epoch:469 loss:3.813553 gene_loss:-1.823560\n",
      "epoch:470 loss:3.811533 gene_loss:-1.823120\n",
      "epoch:471 loss:3.809769 gene_loss:-1.819173\n",
      "epoch:472 loss:3.811818 gene_loss:-1.822104\n",
      "epoch:473 loss:3.812095 gene_loss:-1.819956\n",
      "epoch:474 loss:3.809741 gene_loss:-1.817893\n",
      "epoch:475 loss:3.811217 gene_loss:-1.812344\n",
      "epoch:476 loss:3.809687 gene_loss:-1.799551\n",
      "epoch:477 loss:3.807434 gene_loss:-1.788546\n",
      "epoch:478 loss:3.806394 gene_loss:-1.778569\n",
      "epoch:479 loss:3.806037 gene_loss:-1.762587\n",
      "epoch:480 loss:3.805653 gene_loss:-1.734902\n",
      "epoch:481 loss:3.804339 gene_loss:-1.717107\n",
      "epoch:482 loss:3.803558 gene_loss:-1.669151\n",
      "epoch:483 loss:3.802883 gene_loss:-1.627128\n",
      "epoch:484 loss:3.800595 gene_loss:-1.523895\n",
      "epoch:485 loss:3.794865 gene_loss:-1.518512\n",
      "epoch:486 loss:3.787725 gene_loss:-1.284325\n",
      "epoch:487 loss:3.780273 gene_loss:-1.165955\n",
      "epoch:488 loss:3.781599 gene_loss:-1.034634\n",
      "epoch:489 loss:3.754461 gene_loss:-0.910167\n",
      "epoch:490 loss:3.751732 gene_loss:-1.240226\n",
      "epoch:491 loss:3.760774 gene_loss:-1.550640\n",
      "epoch:492 loss:3.777834 gene_loss:-1.622526\n",
      "epoch:493 loss:3.775570 gene_loss:-1.629735\n",
      "epoch:494 loss:3.777031 gene_loss:-1.728681\n",
      "epoch:495 loss:3.785859 gene_loss:-1.780126\n",
      "epoch:496 loss:3.788703 gene_loss:-1.783693\n",
      "epoch:497 loss:3.795022 gene_loss:-1.796522\n",
      "epoch:498 loss:3.797004 gene_loss:-1.793561\n",
      "epoch:499 loss:3.791226 gene_loss:-1.810577\n",
      "epoch:500 loss:3.804900 gene_loss:-1.813879\n",
      "epoch:501 loss:3.802234 gene_loss:-1.800804\n",
      "epoch:502 loss:3.802503 gene_loss:-1.815327\n",
      "epoch:503 loss:3.802989 gene_loss:-1.814243\n",
      "epoch:504 loss:3.805563 gene_loss:-1.816433\n",
      "epoch:505 loss:3.798227 gene_loss:-1.791874\n",
      "epoch:506 loss:3.808035 gene_loss:-1.820427\n",
      "epoch:507 loss:3.805812 gene_loss:-1.813775\n",
      "epoch:508 loss:3.806732 gene_loss:-1.819387\n",
      "epoch:509 loss:3.802747 gene_loss:-1.807144\n",
      "epoch:510 loss:3.809577 gene_loss:-1.821294\n",
      "epoch:511 loss:3.809121 gene_loss:-1.817113\n",
      "epoch:512 loss:3.808739 gene_loss:-1.814551\n",
      "epoch:513 loss:3.809267 gene_loss:-1.817915\n",
      "epoch:514 loss:3.808853 gene_loss:-1.819856\n",
      "epoch:515 loss:3.811369 gene_loss:-1.821863\n",
      "epoch:516 loss:3.809692 gene_loss:-1.818658\n",
      "epoch:517 loss:3.809371 gene_loss:-1.820623\n",
      "epoch:518 loss:3.810688 gene_loss:-1.824202\n",
      "epoch:519 loss:3.805976 gene_loss:-1.821663\n",
      "epoch:520 loss:3.810935 gene_loss:-1.819745\n",
      "epoch:521 loss:3.812016 gene_loss:-1.823678\n",
      "epoch:522 loss:3.810328 gene_loss:-1.818213\n",
      "epoch:523 loss:3.812653 gene_loss:-1.824433\n",
      "epoch:524 loss:3.810975 gene_loss:-1.821639\n",
      "epoch:525 loss:3.810722 gene_loss:-1.823881\n",
      "epoch:526 loss:3.812512 gene_loss:-1.821949\n",
      "epoch:527 loss:3.811424 gene_loss:-1.822885\n",
      "epoch:528 loss:3.811187 gene_loss:-1.823695\n",
      "epoch:529 loss:3.812492 gene_loss:-1.817365\n",
      "epoch:530 loss:3.812158 gene_loss:-1.825794\n",
      "epoch:531 loss:3.813364 gene_loss:-1.825317\n",
      "epoch:532 loss:3.812820 gene_loss:-1.825046\n",
      "epoch:533 loss:3.810958 gene_loss:-1.823565\n",
      "epoch:534 loss:3.813350 gene_loss:-1.825909\n",
      "epoch:535 loss:3.810583 gene_loss:-1.823892\n",
      "epoch:536 loss:3.811846 gene_loss:-1.825591\n",
      "epoch:537 loss:3.812145 gene_loss:-1.822560\n",
      "epoch:538 loss:3.811982 gene_loss:-1.820350\n",
      "epoch:539 loss:3.815138 gene_loss:-1.826485\n",
      "epoch:540 loss:3.813183 gene_loss:-1.825067\n",
      "epoch:541 loss:3.812148 gene_loss:-1.824547\n",
      "epoch:542 loss:3.813447 gene_loss:-1.825967\n",
      "epoch:543 loss:3.812850 gene_loss:-1.824490\n",
      "epoch:544 loss:3.813374 gene_loss:-1.826005\n",
      "epoch:545 loss:3.812180 gene_loss:-1.822006\n",
      "epoch:546 loss:3.813667 gene_loss:-1.827034\n",
      "epoch:547 loss:3.814072 gene_loss:-1.826326\n",
      "epoch:548 loss:3.813285 gene_loss:-1.825643\n",
      "epoch:549 loss:3.813217 gene_loss:-1.825552\n",
      "epoch:550 loss:3.814741 gene_loss:-1.825987\n",
      "epoch:551 loss:3.812243 gene_loss:-1.825769\n",
      "epoch:552 loss:3.813474 gene_loss:-1.825787\n",
      "epoch:553 loss:3.811668 gene_loss:-1.826257\n",
      "epoch:554 loss:3.813505 gene_loss:-1.826942\n",
      "epoch:555 loss:3.810184 gene_loss:-1.820738\n",
      "epoch:556 loss:3.816145 gene_loss:-1.827113\n",
      "epoch:557 loss:3.813095 gene_loss:-1.825412\n",
      "epoch:558 loss:3.813697 gene_loss:-1.825474\n",
      "epoch:559 loss:3.814278 gene_loss:-1.825546\n",
      "epoch:560 loss:3.812569 gene_loss:-1.826582\n",
      "epoch:561 loss:3.812717 gene_loss:-1.819921\n",
      "epoch:562 loss:3.815399 gene_loss:-1.826030\n",
      "epoch:563 loss:3.814316 gene_loss:-1.825933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:564 loss:3.813353 gene_loss:-1.825067\n",
      "epoch:565 loss:3.812702 gene_loss:-1.822130\n",
      "epoch:566 loss:3.813928 gene_loss:-1.825066\n",
      "epoch:567 loss:3.813564 gene_loss:-1.824383\n",
      "epoch:568 loss:3.810439 gene_loss:-1.819223\n",
      "epoch:569 loss:3.813519 gene_loss:-1.822903\n",
      "epoch:570 loss:3.810557 gene_loss:-1.816012\n",
      "epoch:571 loss:3.813372 gene_loss:-1.821332\n",
      "epoch:572 loss:3.811710 gene_loss:-1.821246\n",
      "epoch:573 loss:3.806470 gene_loss:-1.813350\n",
      "epoch:574 loss:3.811640 gene_loss:-1.819893\n",
      "epoch:575 loss:3.811033 gene_loss:-1.819490\n",
      "epoch:576 loss:3.811943 gene_loss:-1.819165\n",
      "epoch:577 loss:3.810019 gene_loss:-1.818850\n",
      "epoch:578 loss:3.810801 gene_loss:-1.818923\n",
      "epoch:579 loss:3.810157 gene_loss:-1.817741\n",
      "epoch:580 loss:3.809667 gene_loss:-1.813039\n",
      "epoch:581 loss:3.809642 gene_loss:-1.816327\n",
      "epoch:582 loss:3.808823 gene_loss:-1.814627\n",
      "epoch:583 loss:3.808841 gene_loss:-1.812648\n",
      "epoch:584 loss:3.807222 gene_loss:-1.809823\n",
      "epoch:585 loss:3.808275 gene_loss:-1.806891\n",
      "epoch:586 loss:3.805264 gene_loss:-1.800086\n",
      "epoch:587 loss:3.805039 gene_loss:-1.797503\n",
      "epoch:588 loss:3.803829 gene_loss:-1.785593\n",
      "epoch:589 loss:3.804763 gene_loss:-1.773851\n",
      "epoch:590 loss:3.804848 gene_loss:-1.757322\n",
      "epoch:591 loss:3.803671 gene_loss:-1.735928\n",
      "epoch:592 loss:3.803952 gene_loss:-1.703453\n",
      "epoch:593 loss:3.804009 gene_loss:-1.638015\n",
      "epoch:594 loss:3.803262 gene_loss:-1.525275\n",
      "epoch:595 loss:3.795304 gene_loss:-1.303219\n",
      "epoch:596 loss:3.793679 gene_loss:-0.774135\n",
      "epoch:597 loss:3.769596 gene_loss:1.158614\n",
      "epoch:598 loss:3.675967 gene_loss:1.657964\n",
      "epoch:599 loss:3.710687 gene_loss:2.830637\n",
      "epoch:600 loss:3.739807 gene_loss:3.213281\n",
      "epoch:601 loss:3.750780 gene_loss:3.132472\n",
      "epoch:602 loss:3.746528 gene_loss:2.012450\n",
      "epoch:603 loss:1.610024 gene_loss:2.417960\n",
      "epoch:604 loss:3.668460 gene_loss:1.870211\n",
      "epoch:605 loss:3.524717 gene_loss:0.326664\n",
      "epoch:606 loss:3.619287 gene_loss:-0.472464\n",
      "epoch:607 loss:3.639471 gene_loss:-0.915648\n",
      "epoch:608 loss:3.668617 gene_loss:-1.346834\n",
      "epoch:609 loss:3.637784 gene_loss:-1.502686\n",
      "epoch:610 loss:3.724142 gene_loss:-1.672574\n",
      "epoch:611 loss:3.751594 gene_loss:-1.732370\n",
      "epoch:612 loss:3.767075 gene_loss:-1.750319\n",
      "epoch:613 loss:3.759270 gene_loss:-1.750457\n",
      "epoch:614 loss:3.789061 gene_loss:-1.796769\n",
      "epoch:615 loss:3.786565 gene_loss:-1.786941\n",
      "epoch:616 loss:3.786417 gene_loss:-1.798107\n",
      "epoch:617 loss:3.802271 gene_loss:-1.814303\n",
      "epoch:618 loss:3.799358 gene_loss:-1.800847\n",
      "epoch:619 loss:3.803735 gene_loss:-1.816227\n",
      "epoch:620 loss:3.803843 gene_loss:-1.803422\n",
      "epoch:621 loss:3.802465 gene_loss:-1.818711\n",
      "epoch:622 loss:3.807487 gene_loss:-1.819662\n",
      "epoch:623 loss:3.803421 gene_loss:-1.806716\n",
      "epoch:624 loss:3.805094 gene_loss:-1.819448\n",
      "epoch:625 loss:3.809928 gene_loss:-1.817148\n",
      "epoch:626 loss:3.809831 gene_loss:-1.819846\n",
      "epoch:627 loss:3.809485 gene_loss:-1.817444\n",
      "epoch:628 loss:3.810543 gene_loss:-1.821975\n",
      "epoch:629 loss:3.810847 gene_loss:-1.821173\n",
      "epoch:630 loss:3.811093 gene_loss:-1.820348\n",
      "epoch:631 loss:3.809523 gene_loss:-1.819584\n",
      "epoch:632 loss:3.808724 gene_loss:-1.820346\n",
      "epoch:633 loss:3.814474 gene_loss:-1.823819\n",
      "epoch:634 loss:3.810130 gene_loss:-1.823683\n",
      "epoch:635 loss:3.809352 gene_loss:-1.821053\n",
      "epoch:636 loss:3.812889 gene_loss:-1.824641\n",
      "epoch:637 loss:3.813590 gene_loss:-1.823815\n",
      "epoch:638 loss:3.811444 gene_loss:-1.824917\n",
      "epoch:639 loss:3.814290 gene_loss:-1.822805\n",
      "epoch:640 loss:3.813041 gene_loss:-1.825157\n",
      "epoch:641 loss:3.813850 gene_loss:-1.823173\n",
      "epoch:642 loss:3.811946 gene_loss:-1.823092\n",
      "epoch:643 loss:3.813692 gene_loss:-1.825895\n",
      "epoch:644 loss:3.814205 gene_loss:-1.825766\n",
      "epoch:645 loss:3.814956 gene_loss:-1.825212\n",
      "epoch:646 loss:3.811070 gene_loss:-1.821158\n",
      "epoch:647 loss:3.813087 gene_loss:-1.825865\n",
      "epoch:648 loss:3.813644 gene_loss:-1.827067\n",
      "epoch:649 loss:3.812575 gene_loss:-1.819647\n",
      "epoch:650 loss:3.816043 gene_loss:-1.827208\n",
      "epoch:651 loss:3.815856 gene_loss:-1.826832\n",
      "epoch:652 loss:3.811295 gene_loss:-1.823427\n",
      "epoch:653 loss:3.814771 gene_loss:-1.827219\n",
      "epoch:654 loss:3.813170 gene_loss:-1.825724\n",
      "epoch:655 loss:3.814249 gene_loss:-1.826370\n",
      "epoch:656 loss:3.816082 gene_loss:-1.824622\n",
      "epoch:657 loss:3.813913 gene_loss:-1.828429\n",
      "epoch:658 loss:3.813943 gene_loss:-1.824031\n",
      "epoch:659 loss:3.815482 gene_loss:-1.827745\n",
      "epoch:660 loss:3.815588 gene_loss:-1.824616\n",
      "epoch:661 loss:3.815887 gene_loss:-1.827872\n",
      "epoch:662 loss:3.813590 gene_loss:-1.825758\n",
      "epoch:663 loss:3.815951 gene_loss:-1.826489\n",
      "epoch:664 loss:3.813713 gene_loss:-1.827354\n",
      "epoch:665 loss:3.813962 gene_loss:-1.826885\n",
      "epoch:666 loss:3.816388 gene_loss:-1.828619\n",
      "epoch:667 loss:3.813095 gene_loss:-1.826411\n",
      "epoch:668 loss:3.814668 gene_loss:-1.828211\n",
      "epoch:669 loss:3.813966 gene_loss:-1.826276\n",
      "epoch:670 loss:3.815306 gene_loss:-1.825372\n",
      "epoch:671 loss:3.817314 gene_loss:-1.828794\n",
      "epoch:672 loss:3.815712 gene_loss:-1.826413\n",
      "epoch:673 loss:3.816785 gene_loss:-1.827942\n",
      "epoch:674 loss:3.816828 gene_loss:-1.828824\n",
      "epoch:675 loss:3.815813 gene_loss:-1.827799\n",
      "epoch:676 loss:3.815175 gene_loss:-1.828368\n",
      "epoch:677 loss:3.816311 gene_loss:-1.828028\n",
      "epoch:678 loss:3.815643 gene_loss:-1.827898\n",
      "epoch:679 loss:3.814241 gene_loss:-1.827372\n",
      "epoch:680 loss:3.815988 gene_loss:-1.828661\n",
      "epoch:681 loss:3.816494 gene_loss:-1.827524\n",
      "epoch:682 loss:3.815898 gene_loss:-1.828832\n",
      "epoch:683 loss:3.815803 gene_loss:-1.827332\n",
      "epoch:684 loss:3.816931 gene_loss:-1.828781\n",
      "epoch:685 loss:3.815109 gene_loss:-1.826071\n",
      "epoch:686 loss:3.817464 gene_loss:-1.829107\n",
      "epoch:687 loss:3.815867 gene_loss:-1.827824\n",
      "epoch:688 loss:3.815926 gene_loss:-1.828414\n",
      "epoch:689 loss:3.817426 gene_loss:-1.828456\n",
      "epoch:690 loss:3.815300 gene_loss:-1.825783\n",
      "epoch:691 loss:3.816953 gene_loss:-1.828811\n",
      "epoch:692 loss:3.815309 gene_loss:-1.828661\n",
      "epoch:693 loss:3.815962 gene_loss:-1.825613\n",
      "epoch:694 loss:3.817476 gene_loss:-1.828977\n",
      "epoch:695 loss:3.816876 gene_loss:-1.828982\n",
      "epoch:696 loss:3.815777 gene_loss:-1.827132\n",
      "epoch:697 loss:3.814624 gene_loss:-1.826274\n",
      "epoch:698 loss:3.817010 gene_loss:-1.828465\n",
      "epoch:699 loss:3.816637 gene_loss:-1.826685\n",
      "epoch:700 loss:3.816508 gene_loss:-1.827882\n",
      "epoch:701 loss:3.816228 gene_loss:-1.825799\n",
      "epoch:702 loss:3.816366 gene_loss:-1.826165\n",
      "epoch:703 loss:3.815973 gene_loss:-1.825658\n",
      "epoch:704 loss:3.814325 gene_loss:-1.824974\n",
      "epoch:705 loss:3.814688 gene_loss:-1.820125\n",
      "epoch:706 loss:3.814908 gene_loss:-1.821012\n",
      "epoch:707 loss:3.815328 gene_loss:-1.815214\n",
      "epoch:708 loss:3.814291 gene_loss:-1.795832\n",
      "epoch:709 loss:3.811445 gene_loss:-1.711373\n",
      "epoch:710 loss:3.798604 gene_loss:-1.451819\n",
      "epoch:711 loss:3.779166 gene_loss:0.510919\n",
      "epoch:712 loss:3.730243 gene_loss:1.950245\n",
      "epoch:713 loss:3.763011 gene_loss:3.086830\n",
      "epoch:714 loss:3.769516 gene_loss:3.171062\n",
      "epoch:715 loss:3.773387 gene_loss:1.944574\n",
      "epoch:716 loss:3.541546 gene_loss:0.616444\n",
      "epoch:717 loss:3.690949 gene_loss:0.752964\n",
      "epoch:718 loss:3.703292 gene_loss:0.087531\n",
      "epoch:719 loss:3.653453 gene_loss:-1.210823\n",
      "epoch:720 loss:3.723152 gene_loss:-1.435768\n",
      "epoch:721 loss:3.712082 gene_loss:-1.665707\n",
      "epoch:722 loss:3.762809 gene_loss:-1.698891\n",
      "epoch:723 loss:3.764314 gene_loss:-1.761111\n",
      "epoch:724 loss:3.785152 gene_loss:-1.784892\n",
      "epoch:725 loss:3.778595 gene_loss:-1.717474\n",
      "epoch:726 loss:3.782526 gene_loss:-1.783082\n",
      "epoch:727 loss:3.799125 gene_loss:-1.808372\n",
      "epoch:728 loss:3.800901 gene_loss:-1.811635\n",
      "epoch:729 loss:3.792458 gene_loss:-1.801242\n",
      "epoch:730 loss:3.804296 gene_loss:-1.813198\n",
      "epoch:731 loss:3.808976 gene_loss:-1.809368\n",
      "epoch:732 loss:3.805775 gene_loss:-1.817173\n",
      "epoch:733 loss:3.796522 gene_loss:-1.792833\n",
      "epoch:734 loss:3.807299 gene_loss:-1.820204\n",
      "epoch:735 loss:3.811551 gene_loss:-1.823523\n",
      "epoch:736 loss:3.808421 gene_loss:-1.811856\n",
      "epoch:737 loss:3.809447 gene_loss:-1.822586\n",
      "epoch:738 loss:3.812897 gene_loss:-1.825177\n",
      "epoch:739 loss:3.808552 gene_loss:-1.823074\n",
      "epoch:740 loss:3.810535 gene_loss:-1.824402\n",
      "epoch:741 loss:3.814146 gene_loss:-1.824209\n",
      "epoch:742 loss:3.810008 gene_loss:-1.823381\n",
      "epoch:743 loss:3.810833 gene_loss:-1.820106\n",
      "epoch:744 loss:3.813238 gene_loss:-1.826962\n",
      "epoch:745 loss:3.811583 gene_loss:-1.821045\n",
      "epoch:746 loss:3.813630 gene_loss:-1.824833\n",
      "epoch:747 loss:3.813774 gene_loss:-1.827340\n",
      "epoch:748 loss:3.812876 gene_loss:-1.820281\n",
      "epoch:749 loss:3.815628 gene_loss:-1.828121\n",
      "epoch:750 loss:3.814385 gene_loss:-1.825809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:751 loss:3.813212 gene_loss:-1.825137\n",
      "epoch:752 loss:3.815205 gene_loss:-1.824410\n",
      "epoch:753 loss:3.814738 gene_loss:-1.827228\n",
      "epoch:754 loss:3.814690 gene_loss:-1.827876\n",
      "epoch:755 loss:3.814699 gene_loss:-1.822277\n",
      "epoch:756 loss:3.816057 gene_loss:-1.828752\n",
      "epoch:757 loss:3.816040 gene_loss:-1.826919\n",
      "epoch:758 loss:3.814242 gene_loss:-1.828080\n",
      "epoch:759 loss:3.816775 gene_loss:-1.828597\n",
      "epoch:760 loss:3.813683 gene_loss:-1.822634\n",
      "epoch:761 loss:3.816525 gene_loss:-1.829097\n",
      "epoch:762 loss:3.815694 gene_loss:-1.826672\n",
      "epoch:763 loss:3.816930 gene_loss:-1.828803\n",
      "epoch:764 loss:3.815432 gene_loss:-1.824244\n",
      "epoch:765 loss:3.817860 gene_loss:-1.829590\n",
      "epoch:766 loss:3.816342 gene_loss:-1.829010\n",
      "epoch:767 loss:3.817561 gene_loss:-1.829403\n",
      "epoch:768 loss:3.816169 gene_loss:-1.826402\n",
      "epoch:769 loss:3.816700 gene_loss:-1.830076\n",
      "epoch:770 loss:3.817248 gene_loss:-1.827876\n",
      "epoch:771 loss:3.815344 gene_loss:-1.829096\n",
      "epoch:772 loss:3.817567 gene_loss:-1.828344\n",
      "epoch:773 loss:3.817045 gene_loss:-1.828914\n",
      "epoch:774 loss:3.817213 gene_loss:-1.828712\n",
      "epoch:775 loss:3.816449 gene_loss:-1.829959\n",
      "epoch:776 loss:3.814833 gene_loss:-1.828106\n",
      "epoch:777 loss:3.818637 gene_loss:-1.829822\n",
      "epoch:778 loss:3.816869 gene_loss:-1.826111\n",
      "epoch:779 loss:3.817036 gene_loss:-1.829862\n",
      "epoch:780 loss:3.817870 gene_loss:-1.829309\n",
      "epoch:781 loss:3.814723 gene_loss:-1.827390\n",
      "epoch:782 loss:3.817035 gene_loss:-1.830743\n",
      "epoch:783 loss:3.817864 gene_loss:-1.829655\n",
      "epoch:784 loss:3.816826 gene_loss:-1.828681\n",
      "epoch:785 loss:3.816395 gene_loss:-1.830358\n",
      "epoch:786 loss:3.818355 gene_loss:-1.828338\n",
      "epoch:787 loss:3.818627 gene_loss:-1.830483\n",
      "epoch:788 loss:3.818358 gene_loss:-1.829162\n",
      "epoch:789 loss:3.818111 gene_loss:-1.829719\n",
      "epoch:790 loss:3.817448 gene_loss:-1.830112\n",
      "epoch:791 loss:3.816764 gene_loss:-1.829743\n",
      "epoch:792 loss:3.817406 gene_loss:-1.830436\n",
      "epoch:793 loss:3.817369 gene_loss:-1.826201\n",
      "epoch:794 loss:3.817137 gene_loss:-1.830810\n",
      "epoch:795 loss:3.818537 gene_loss:-1.830194\n",
      "epoch:796 loss:3.817395 gene_loss:-1.828685\n",
      "epoch:797 loss:3.818766 gene_loss:-1.828840\n",
      "epoch:798 loss:3.817378 gene_loss:-1.830741\n",
      "epoch:799 loss:3.819079 gene_loss:-1.829019\n",
      "epoch:800 loss:3.818068 gene_loss:-1.830172\n",
      "epoch:801 loss:3.817196 gene_loss:-1.828681\n",
      "epoch:802 loss:3.817960 gene_loss:-1.830378\n",
      "epoch:803 loss:3.816276 gene_loss:-1.827276\n",
      "epoch:804 loss:3.818221 gene_loss:-1.830233\n",
      "epoch:805 loss:3.818274 gene_loss:-1.828832\n",
      "epoch:806 loss:3.817627 gene_loss:-1.830446\n",
      "epoch:807 loss:3.817243 gene_loss:-1.830235\n",
      "epoch:808 loss:3.817098 gene_loss:-1.828026\n",
      "epoch:809 loss:3.818462 gene_loss:-1.829351\n",
      "epoch:810 loss:3.817658 gene_loss:-1.827258\n",
      "epoch:811 loss:3.817722 gene_loss:-1.829875\n",
      "epoch:812 loss:3.816420 gene_loss:-1.828987\n",
      "epoch:813 loss:3.816747 gene_loss:-1.827541\n",
      "epoch:814 loss:3.815699 gene_loss:-1.828369\n",
      "epoch:815 loss:3.817174 gene_loss:-1.826746\n",
      "epoch:816 loss:3.817120 gene_loss:-1.823278\n",
      "epoch:817 loss:3.815698 gene_loss:-1.823688\n",
      "epoch:818 loss:3.814344 gene_loss:-1.819682\n",
      "epoch:819 loss:3.813163 gene_loss:-1.812709\n",
      "epoch:820 loss:3.811584 gene_loss:-1.811841\n",
      "epoch:821 loss:3.810399 gene_loss:-1.801914\n",
      "epoch:822 loss:3.807275 gene_loss:-1.795198\n",
      "epoch:823 loss:3.806642 gene_loss:-1.778824\n",
      "epoch:824 loss:3.800562 gene_loss:-1.752031\n",
      "epoch:825 loss:3.795003 gene_loss:-1.677774\n",
      "epoch:826 loss:3.774572 gene_loss:-1.408976\n",
      "epoch:827 loss:3.749197 gene_loss:-0.672403\n",
      "epoch:828 loss:3.717081 gene_loss:0.804194\n",
      "epoch:829 loss:3.748915 gene_loss:2.263330\n",
      "epoch:830 loss:3.746953 gene_loss:2.942889\n",
      "epoch:831 loss:3.770038 gene_loss:3.184491\n",
      "epoch:832 loss:3.770432 gene_loss:3.319430\n",
      "epoch:833 loss:3.766442 gene_loss:3.111807\n",
      "epoch:834 loss:3.771129 gene_loss:0.012126\n",
      "epoch:835 loss:3.761780 gene_loss:1.913969\n",
      "epoch:836 loss:3.765266 gene_loss:1.927622\n",
      "epoch:837 loss:3.778054 gene_loss:2.973007\n",
      "epoch:838 loss:3.774234 gene_loss:2.506466\n",
      "epoch:839 loss:3.770628 gene_loss:0.468921\n",
      "epoch:840 loss:3.733597 gene_loss:1.045386\n",
      "epoch:841 loss:3.770663 gene_loss:-0.930554\n",
      "epoch:842 loss:3.776110 gene_loss:0.037693\n",
      "epoch:843 loss:3.751255 gene_loss:-1.568379\n",
      "epoch:844 loss:3.763792 gene_loss:-1.583066\n",
      "epoch:845 loss:3.781869 gene_loss:-1.631046\n",
      "epoch:846 loss:3.774668 gene_loss:-1.674679\n",
      "epoch:847 loss:3.776136 gene_loss:-1.716054\n",
      "epoch:848 loss:3.786026 gene_loss:-1.784556\n",
      "epoch:849 loss:3.795445 gene_loss:-1.789971\n",
      "epoch:850 loss:3.790534 gene_loss:-1.786785\n",
      "epoch:851 loss:3.794682 gene_loss:-1.800663\n",
      "epoch:852 loss:3.799474 gene_loss:-1.802049\n",
      "epoch:853 loss:3.802738 gene_loss:-1.799479\n",
      "epoch:854 loss:3.805525 gene_loss:-1.811395\n",
      "epoch:855 loss:3.803859 gene_loss:-1.799134\n",
      "epoch:856 loss:3.806400 gene_loss:-1.819602\n",
      "epoch:857 loss:3.803835 gene_loss:-1.817105\n",
      "epoch:858 loss:3.808940 gene_loss:-1.820806\n",
      "epoch:859 loss:3.809615 gene_loss:-1.806339\n",
      "epoch:860 loss:3.810714 gene_loss:-1.824343\n",
      "epoch:861 loss:3.812048 gene_loss:-1.817645\n",
      "epoch:862 loss:3.813076 gene_loss:-1.822717\n",
      "epoch:863 loss:3.810999 gene_loss:-1.822779\n",
      "epoch:864 loss:3.808860 gene_loss:-1.824407\n",
      "epoch:865 loss:3.806461 gene_loss:-1.816688\n",
      "epoch:866 loss:3.813878 gene_loss:-1.826577\n",
      "epoch:867 loss:3.811812 gene_loss:-1.823393\n",
      "epoch:868 loss:3.814095 gene_loss:-1.825947\n",
      "epoch:869 loss:3.810074 gene_loss:-1.822474\n",
      "epoch:870 loss:3.814833 gene_loss:-1.827067\n",
      "epoch:871 loss:3.810969 gene_loss:-1.817976\n",
      "epoch:872 loss:3.815145 gene_loss:-1.828211\n",
      "epoch:873 loss:3.814761 gene_loss:-1.825221\n",
      "epoch:874 loss:3.813514 gene_loss:-1.826024\n",
      "epoch:875 loss:3.815137 gene_loss:-1.825011\n",
      "epoch:876 loss:3.816173 gene_loss:-1.827827\n",
      "epoch:877 loss:3.814276 gene_loss:-1.825647\n",
      "epoch:878 loss:3.813695 gene_loss:-1.828520\n",
      "epoch:879 loss:3.816323 gene_loss:-1.825618\n",
      "epoch:880 loss:3.815653 gene_loss:-1.827569\n",
      "epoch:881 loss:3.816672 gene_loss:-1.826468\n",
      "epoch:882 loss:3.815797 gene_loss:-1.826560\n",
      "epoch:883 loss:3.814714 gene_loss:-1.828794\n",
      "epoch:884 loss:3.816389 gene_loss:-1.827843\n",
      "epoch:885 loss:3.815438 gene_loss:-1.828561\n",
      "epoch:886 loss:3.812397 gene_loss:-1.824918\n",
      "epoch:887 loss:3.817448 gene_loss:-1.828577\n",
      "epoch:888 loss:3.816505 gene_loss:-1.828172\n",
      "epoch:889 loss:3.815370 gene_loss:-1.827188\n",
      "epoch:890 loss:3.817271 gene_loss:-1.829965\n",
      "epoch:891 loss:3.816892 gene_loss:-1.829274\n",
      "epoch:892 loss:3.815581 gene_loss:-1.825463\n",
      "epoch:893 loss:3.816994 gene_loss:-1.829189\n",
      "epoch:894 loss:3.815039 gene_loss:-1.828447\n",
      "epoch:895 loss:3.817130 gene_loss:-1.829160\n",
      "epoch:896 loss:3.816531 gene_loss:-1.826454\n",
      "epoch:897 loss:3.818175 gene_loss:-1.830474\n",
      "epoch:898 loss:3.817031 gene_loss:-1.828519\n",
      "epoch:899 loss:3.818121 gene_loss:-1.829642\n",
      "epoch:900 loss:3.816736 gene_loss:-1.829677\n",
      "epoch:901 loss:3.817284 gene_loss:-1.828853\n",
      "epoch:902 loss:3.817118 gene_loss:-1.830266\n",
      "epoch:903 loss:3.817703 gene_loss:-1.827253\n",
      "epoch:904 loss:3.815354 gene_loss:-1.829070\n",
      "epoch:905 loss:3.818200 gene_loss:-1.830666\n",
      "epoch:906 loss:3.815930 gene_loss:-1.828962\n",
      "epoch:907 loss:3.818335 gene_loss:-1.830446\n",
      "epoch:908 loss:3.818286 gene_loss:-1.829686\n",
      "epoch:909 loss:3.817126 gene_loss:-1.829594\n",
      "epoch:910 loss:3.818245 gene_loss:-1.830273\n",
      "epoch:911 loss:3.818139 gene_loss:-1.829365\n",
      "epoch:912 loss:3.818167 gene_loss:-1.831034\n",
      "epoch:913 loss:3.817044 gene_loss:-1.828488\n",
      "epoch:914 loss:3.817305 gene_loss:-1.831205\n",
      "epoch:915 loss:3.818088 gene_loss:-1.829134\n",
      "epoch:916 loss:3.817976 gene_loss:-1.830654\n",
      "epoch:917 loss:3.818779 gene_loss:-1.830876\n",
      "epoch:918 loss:3.817886 gene_loss:-1.829118\n",
      "epoch:919 loss:3.818028 gene_loss:-1.830996\n",
      "epoch:920 loss:3.817934 gene_loss:-1.830255\n",
      "epoch:921 loss:3.818431 gene_loss:-1.830961\n",
      "epoch:922 loss:3.817791 gene_loss:-1.830451\n",
      "epoch:923 loss:3.817298 gene_loss:-1.830156\n",
      "epoch:924 loss:3.818445 gene_loss:-1.830971\n",
      "epoch:925 loss:3.818943 gene_loss:-1.829574\n",
      "epoch:926 loss:3.818831 gene_loss:-1.831206\n",
      "epoch:927 loss:3.819132 gene_loss:-1.830571\n",
      "epoch:928 loss:3.818329 gene_loss:-1.831242\n",
      "epoch:929 loss:3.817442 gene_loss:-1.830926\n",
      "epoch:930 loss:3.819530 gene_loss:-1.830823\n",
      "epoch:931 loss:3.817229 gene_loss:-1.828236\n",
      "epoch:932 loss:3.819308 gene_loss:-1.831271\n",
      "epoch:933 loss:3.818785 gene_loss:-1.829924\n",
      "epoch:934 loss:3.817712 gene_loss:-1.831175\n",
      "epoch:935 loss:3.817866 gene_loss:-1.830581\n",
      "epoch:936 loss:3.816522 gene_loss:-1.830465\n",
      "epoch:937 loss:3.819534 gene_loss:-1.830805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:938 loss:3.819560 gene_loss:-1.830842\n",
      "epoch:939 loss:3.816247 gene_loss:-1.828218\n",
      "epoch:940 loss:3.818858 gene_loss:-1.830251\n",
      "epoch:941 loss:3.817608 gene_loss:-1.829159\n",
      "epoch:942 loss:3.815171 gene_loss:-1.826324\n",
      "epoch:943 loss:3.817466 gene_loss:-1.825917\n",
      "epoch:944 loss:3.816676 gene_loss:-1.821595\n",
      "epoch:945 loss:3.813273 gene_loss:-1.818489\n",
      "epoch:946 loss:3.814590 gene_loss:-1.817526\n",
      "epoch:947 loss:3.813235 gene_loss:-1.814858\n",
      "epoch:948 loss:3.812843 gene_loss:-1.812361\n",
      "epoch:949 loss:3.811794 gene_loss:-1.810277\n",
      "epoch:950 loss:3.811245 gene_loss:-1.808395\n",
      "epoch:951 loss:3.809380 gene_loss:-1.804917\n",
      "epoch:952 loss:3.809881 gene_loss:-1.805589\n",
      "epoch:953 loss:3.809750 gene_loss:-1.807615\n",
      "epoch:954 loss:3.809262 gene_loss:-1.809164\n",
      "epoch:955 loss:3.810415 gene_loss:-1.808777\n",
      "epoch:956 loss:3.811758 gene_loss:-1.807642\n",
      "epoch:957 loss:3.812791 gene_loss:-1.808295\n",
      "epoch:958 loss:3.810825 gene_loss:-1.810253\n",
      "epoch:959 loss:3.814791 gene_loss:-1.810650\n",
      "epoch:960 loss:3.813832 gene_loss:-1.813017\n",
      "epoch:961 loss:3.813767 gene_loss:-1.813776\n",
      "epoch:962 loss:3.814006 gene_loss:-1.814152\n",
      "epoch:963 loss:3.813771 gene_loss:-1.816399\n",
      "epoch:964 loss:3.814678 gene_loss:-1.816323\n",
      "epoch:965 loss:3.814487 gene_loss:-1.817503\n",
      "epoch:966 loss:3.814283 gene_loss:-1.819377\n",
      "epoch:967 loss:3.814681 gene_loss:-1.820088\n",
      "epoch:968 loss:3.815181 gene_loss:-1.821168\n",
      "epoch:969 loss:3.815478 gene_loss:-1.819756\n",
      "epoch:970 loss:3.815623 gene_loss:-1.822048\n",
      "epoch:971 loss:3.814080 gene_loss:-1.822631\n",
      "epoch:972 loss:3.812408 gene_loss:-1.819819\n",
      "epoch:973 loss:3.814857 gene_loss:-1.821692\n",
      "epoch:974 loss:3.812841 gene_loss:-1.820776\n",
      "epoch:975 loss:3.815707 gene_loss:-1.822469\n",
      "epoch:976 loss:3.815069 gene_loss:-1.821537\n",
      "epoch:977 loss:3.814823 gene_loss:-1.823818\n",
      "epoch:978 loss:3.811380 gene_loss:-1.818650\n",
      "epoch:979 loss:3.815377 gene_loss:-1.823551\n",
      "epoch:980 loss:3.812720 gene_loss:-1.821465\n",
      "epoch:981 loss:3.813685 gene_loss:-1.822554\n",
      "epoch:982 loss:3.813210 gene_loss:-1.822239\n",
      "epoch:983 loss:3.809821 gene_loss:-1.813363\n",
      "epoch:984 loss:3.810043 gene_loss:-1.815483\n",
      "epoch:985 loss:3.810967 gene_loss:-1.811009\n",
      "epoch:986 loss:3.808061 gene_loss:-1.807478\n",
      "epoch:987 loss:3.808624 gene_loss:-1.797317\n",
      "epoch:988 loss:3.805574 gene_loss:-1.775975\n",
      "epoch:989 loss:3.801962 gene_loss:-1.737973\n",
      "epoch:990 loss:3.799887 gene_loss:-1.548968\n",
      "epoch:991 loss:3.787097 gene_loss:0.094357\n",
      "epoch:992 loss:3.763150 gene_loss:3.496309\n",
      "epoch:993 loss:3.775165 gene_loss:3.578075\n",
      "epoch:994 loss:3.777221 gene_loss:3.526574\n",
      "epoch:995 loss:3.786961 gene_loss:3.309574\n",
      "epoch:996 loss:3.768787 gene_loss:3.434149\n",
      "epoch:997 loss:3.767726 gene_loss:3.263492\n",
      "epoch:998 loss:3.713704 gene_loss:2.822365\n",
      "epoch:999 loss:2.766496 gene_loss:1.903567\n"
     ]
    }
   ],
   "source": [
    "real_labels = -np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.ones(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(1000):\n",
    "    #============================\n",
    "    #训练一次G就要训练N_CRITIC次D（Discriminator）\n",
    "    #===\n",
    "    critic_i.trainable = True\n",
    "    for layer in critic_i.layers:\n",
    "        layer.trainable = True\n",
    "    #===\n",
    "    for _ in range(N_CRITIC):\n",
    "        \n",
    "        noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "\n",
    "        real_image = load_image()\n",
    "        #训练判别器\n",
    "        fake_image = generator_i.predict(noise)\n",
    "\n",
    "        real_loss = critic_i.train_on_batch(real_image , real_labels)\n",
    "        fake_loss = critic_i.train_on_batch(fake_image , fake_labels)\n",
    "\n",
    "        loss = np.add(real_loss , fake_loss)/2\n",
    "        \n",
    "        #addin\n",
    "        #WGAN的变化 对权重参数进行裁剪\n",
    "        for layer in critic_i.layers:\n",
    "            weights = layer.get_weights()\n",
    "            weights = [np.clip(w , - CLIP_VALUE , CLIP_VALUE) for w in weights]\n",
    "            layer.set_weights(weights)\n",
    "    #============================\n",
    "    #===\n",
    "    critic_i.trainable = False\n",
    "    for layer in critic_i.layers:\n",
    "        layer.trainable = False\n",
    "    #===\n",
    "    \n",
    "    #训练生成器\n",
    "    noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    generator_loss = combined_model_i.train_on_batch(noise2 , real_labels)\n",
    "\n",
    "    print('epoch:%d loss:%f gene_loss:%f' % (i , 1-loss[0] , 1-generator_loss))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        write_image(i)\n",
    "    \n",
    "write_image(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeli = Sequential()\n",
    "\n",
    "modeli.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "modeli.add(Reshape((7, 7, 128)))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(Activation(\"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modeli.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
