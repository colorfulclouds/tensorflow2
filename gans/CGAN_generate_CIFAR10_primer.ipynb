{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.layers import Multiply\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.initializers import truncated_normal , random_normal , constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CIFAR10 dataset\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNEL = 3\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = '../dataset/train/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n",
    "\n",
    "#=========\n",
    "#=========\n",
    "#add new FLAG(s)\n",
    "CLASS_NUM = 10 #mnist=10 CIFAR10=10 CIFAR100=100 CIFAR1000=1000\n",
    "\n",
    "LABEL2INDEX = {'frog':0 , 'truck':1 , 'deer':2 , 'automobile':3 , 'bird':4 , 'horse':5 , 'ship':6 , 'cat':7 , 'dog':8 , 'airplane':9}\n",
    "INDEX2LABEL = {value:key for key , value in LABEL2INDEX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'frog',\n",
       " 1: 'truck',\n",
       " 2: 'deer',\n",
       " 3: 'automobile',\n",
       " 4: 'bird',\n",
       " 5: 'horse',\n",
       " 6: 'ship',\n",
       " 7: 'cat',\n",
       " 8: 'dog',\n",
       " 9: 'airplane'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX2LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('../dataset/trainLabels.csv')\n",
    "#每张图片对应的类别标号\n",
    "train_labels = train_labels['label'].map(LABEL2INDEX).get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "        labels.append(train_labels[(load_index + i) % IMAGES_COUNT])\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1 , np.array(labels)\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    labels = np.random.randint(low=0 , high=CLASS_NUM , size=(ROW*COL , 1))\n",
    "    \n",
    "    generated_image = generator_i.predict([noise , labels])\n",
    "    \n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    fig.subplots_adjust(hspace=0.9 , wspace=0.9)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            axes[i][j].set_title(INDEX2LABEL[labels[i*ROW+j][0]])\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('CIFAR10_cgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(output_size):\n",
    "    return Conv2D(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def dense(output_size):\n",
    "    return Dense(output_size , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def deconv2d(output_size):\n",
    "    return Conv2DTranspose(output_size , kernel_size=(5,5) , strides=(2,2) , padding='same' , kernel_initializer=random_normal(stddev=0.02) , bias_initializer=constant(0.0))\n",
    "\n",
    "def batch_norm():\n",
    "    return BatchNormalization(momentum=0.9 , epsilon=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "    \n",
    "    #CIFAR10 图像使用 32*32*3\n",
    "    model.add(Dense(2 * 2 * 64*8, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "    model.add(Reshape((2, 2, 64*8)))\n",
    "    \n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #4\n",
    "    model.add(deconv2d(64*4))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #8\n",
    "    model.add(deconv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #16\n",
    "    model.add(deconv2d(64*1))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #32\n",
    "    model.add(deconv2d(3))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    label = Input(shape=(1,) , dtype='int32')\n",
    "    \n",
    "    _ = Embedding(input_dim=CLASS_NUM , output_dim=LATENT_DIM)(label)\n",
    "    embedding_label = Flatten()(_)\n",
    "    \n",
    "    noise_embedding_label = Multiply()([noise , embedding_label]) #(None , LATENT_DIM)\n",
    "    \n",
    "    image = model(noise_embedding_label)\n",
    "    \n",
    "    return Model([noise , label] , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='discriminator')\n",
    "    \n",
    "    model.add(Conv2D(filters=64 , kernel_size=(5,5) , strides=(2,2) , padding='same' , input_shape=(WIDTH , HEIGHT , CHANNEL) , kernel_initializer=truncated_normal(stddev=0.02) , bias_initializer=constant(0.0) , name='conv1'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(conv2d(64*2))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(conv2d(64*4))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(conv2d(64*8))\n",
    "    model.add(batch_norm())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    #=====\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    #=====\n",
    "    model.add(Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    flatten_feature = model(image)\n",
    "    \n",
    "    label = Input(shape=(1,))\n",
    "    embedding_label = Embedding(input_dim=CLASS_NUM , output_dim=WIDTH*HEIGHT*CHANNEL)(label)\n",
    "    flatten_embedding_label = Flatten()(embedding_label)\n",
    "    \n",
    "    input_ = Multiply()([flatten_feature , flatten_embedding_label])\n",
    "    input_reshape = Reshape(target_shape=(WIDTH , HEIGHT , CHANNEL))(input_)\n",
    "    \n",
    "    #FC层 多加了一层\n",
    "    #_ = Dense(128)(flatten_feature)\n",
    "    #_ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    validity = model(input_reshape)\n",
    "        \n",
    "    return Model([image , label] , validity , name='discriminator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_model(generator_i , discriminator_i):\n",
    "    #生成器和判别器组合成整体\n",
    "    z = Input(shape=(LATENT_DIM , ) , name='z')\n",
    "    label = Input(shape=(1,) , dtype='int32')\n",
    "    \n",
    "    image = generator_i([z , label])\n",
    "    \n",
    "    #print(image.shape)\n",
    "    \n",
    "    discriminator_i.trainable = False\n",
    "    validity = discriminator_i([image , label])\n",
    "    \n",
    "    return Model([z , label] , validity , name='combined_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_i = discriminator()\n",
    "discriminator_i.compile(optimizer=adam , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator()\n",
    "\n",
    "combined_model_i = combined_model(generator_i , discriminator_i)\n",
    "\n",
    "\n",
    "combined_model_i.compile(optimizer=adam , loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 batch:52 loss:0.718152 accu:0.000000 gene_loss:[validity:0.718192]\n",
      "epoch:1 batch:52 loss:0.718136 accu:0.000000 gene_loss:[validity:0.718185]\n",
      "epoch:2 batch:52 loss:0.718129 accu:0.000000 gene_loss:[validity:0.718162]\n",
      "epoch:3 batch:52 loss:0.718112 accu:0.000000 gene_loss:[validity:0.718156]\n",
      "epoch:4 batch:52 loss:0.718102 accu:0.000000 gene_loss:[validity:0.718136]\n",
      "epoch:5 batch:52 loss:0.718086 accu:0.000000 gene_loss:[validity:0.718126]\n",
      "epoch:6 batch:52 loss:0.718077 accu:0.000000 gene_loss:[validity:0.718110]\n",
      "epoch:7 batch:52 loss:0.718060 accu:0.000000 gene_loss:[validity:0.718102]\n",
      "epoch:8 batch:52 loss:0.718050 accu:0.000000 gene_loss:[validity:0.718082]\n",
      "epoch:9 batch:52 loss:0.718037 accu:0.000000 gene_loss:[validity:0.718061]\n",
      "epoch:10 batch:52 loss:0.718022 accu:0.000000 gene_loss:[validity:0.718044]\n",
      "epoch:11 batch:52 loss:0.718008 accu:0.000000 gene_loss:[validity:0.718033]\n",
      "epoch:12 batch:52 loss:0.717996 accu:0.000000 gene_loss:[validity:0.718020]\n",
      "epoch:13 batch:52 loss:0.717983 accu:0.000000 gene_loss:[validity:0.718011]\n",
      "epoch:14 batch:52 loss:0.717973 accu:0.000000 gene_loss:[validity:0.718002]\n",
      "epoch:15 batch:52 loss:0.717962 accu:0.000000 gene_loss:[validity:0.717997]\n",
      "epoch:16 batch:52 loss:0.717951 accu:0.000000 gene_loss:[validity:0.717986]\n",
      "epoch:17 batch:52 loss:0.717942 accu:0.000000 gene_loss:[validity:0.717980]\n",
      "epoch:18 batch:52 loss:0.717945 accu:0.000000 gene_loss:[validity:0.717946]\n",
      "epoch:19 batch:52 loss:0.717932 accu:0.000000 gene_loss:[validity:0.717940]\n",
      "epoch:20 batch:52 loss:0.717921 accu:0.000000 gene_loss:[validity:0.717944]\n",
      "epoch:21 batch:52 loss:0.717915 accu:0.000000 gene_loss:[validity:0.717950]\n",
      "epoch:22 batch:52 loss:0.717909 accu:0.000000 gene_loss:[validity:0.717958]\n",
      "epoch:23 batch:52 loss:0.717907 accu:0.000000 gene_loss:[validity:0.717963]\n",
      "epoch:24 batch:52 loss:0.717905 accu:0.000000 gene_loss:[validity:0.717979]\n",
      "epoch:25 batch:52 loss:0.717908 accu:0.000000 gene_loss:[validity:0.717982]\n",
      "epoch:26 batch:52 loss:0.717907 accu:0.000000 gene_loss:[validity:0.717984]\n",
      "epoch:27 batch:52 loss:0.717908 accu:0.000000 gene_loss:[validity:0.717970]\n",
      "epoch:28 batch:52 loss:0.717901 accu:0.000000 gene_loss:[validity:0.717963]\n",
      "epoch:29 batch:52 loss:0.717893 accu:0.000000 gene_loss:[validity:0.717944]\n",
      "epoch:30 batch:52 loss:0.717880 accu:0.000000 gene_loss:[validity:0.717925]\n",
      "epoch:31 batch:52 loss:0.717868 accu:0.000000 gene_loss:[validity:0.717898]\n",
      "epoch:32 batch:52 loss:0.717853 accu:0.000000 gene_loss:[validity:0.717881]\n",
      "epoch:33 batch:52 loss:0.717840 accu:0.000000 gene_loss:[validity:0.717860]\n",
      "epoch:34 batch:52 loss:0.717824 accu:0.000000 gene_loss:[validity:0.717846]\n",
      "epoch:35 batch:52 loss:0.717811 accu:0.000000 gene_loss:[validity:0.717831]\n",
      "epoch:36 batch:52 loss:0.717799 accu:0.000000 gene_loss:[validity:0.717821]\n",
      "epoch:37 batch:52 loss:0.717788 accu:0.000000 gene_loss:[validity:0.717811]\n",
      "epoch:38 batch:52 loss:0.717777 accu:0.000000 gene_loss:[validity:0.717804]\n",
      "epoch:39 batch:52 loss:0.717781 accu:0.000000 gene_loss:[validity:0.717764]\n",
      "epoch:40 batch:52 loss:0.717762 accu:0.000000 gene_loss:[validity:0.717756]\n",
      "epoch:41 batch:52 loss:0.717751 accu:0.000000 gene_loss:[validity:0.717757]\n",
      "epoch:42 batch:52 loss:0.717744 accu:0.000000 gene_loss:[validity:0.717759]\n",
      "epoch:43 batch:52 loss:0.717739 accu:0.000000 gene_loss:[validity:0.717759]\n",
      "epoch:44 batch:52 loss:0.717733 accu:0.000000 gene_loss:[validity:0.717758]\n",
      "epoch:45 batch:52 loss:0.717749 accu:0.000000 gene_loss:[validity:0.717691]\n",
      "epoch:46 batch:52 loss:0.717727 accu:0.000000 gene_loss:[validity:0.717682]\n",
      "epoch:47 batch:52 loss:0.717714 accu:0.000000 gene_loss:[validity:0.717693]\n",
      "epoch:48 batch:52 loss:0.717708 accu:0.000000 gene_loss:[validity:0.717705]\n",
      "epoch:49 batch:52 loss:0.717702 accu:0.000000 gene_loss:[validity:0.717714]\n",
      "epoch:50 batch:52 loss:0.717698 accu:0.000000 gene_loss:[validity:0.717719]\n",
      "epoch:51 batch:52 loss:0.717693 accu:0.000000 gene_loss:[validity:0.717725]\n",
      "epoch:52 batch:52 loss:0.717690 accu:0.000000 gene_loss:[validity:0.717725]\n",
      "epoch:53 batch:52 loss:0.717686 accu:0.000000 gene_loss:[validity:0.717726]\n",
      "epoch:54 batch:52 loss:0.717752 accu:0.000000 gene_loss:[validity:0.717503]\n",
      "epoch:55 batch:52 loss:0.717731 accu:0.000000 gene_loss:[validity:0.717455]\n",
      "epoch:56 batch:52 loss:0.717696 accu:0.000000 gene_loss:[validity:0.717549]\n",
      "epoch:57 batch:52 loss:0.717691 accu:0.000000 gene_loss:[validity:0.717639]\n",
      "epoch:58 batch:52 loss:0.717697 accu:0.000000 gene_loss:[validity:0.717691]\n",
      "epoch:59 batch:52 loss:0.717698 accu:0.000000 gene_loss:[validity:0.717729]\n",
      "epoch:60 batch:52 loss:0.717715 accu:0.000000 gene_loss:[validity:0.717703]\n",
      "epoch:61 batch:52 loss:0.717709 accu:0.000000 gene_loss:[validity:0.717700]\n",
      "epoch:62 batch:52 loss:0.717701 accu:0.000000 gene_loss:[validity:0.717704]\n",
      "epoch:63 batch:52 loss:0.717691 accu:0.000000 gene_loss:[validity:0.717717]\n",
      "epoch:64 batch:52 loss:0.717687 accu:0.000000 gene_loss:[validity:0.717715]\n",
      "epoch:65 batch:52 loss:0.717677 accu:0.000000 gene_loss:[validity:0.717720]\n",
      "epoch:66 batch:52 loss:0.717671 accu:0.000000 gene_loss:[validity:0.717711]\n",
      "epoch:67 batch:52 loss:0.717660 accu:0.000000 gene_loss:[validity:0.717710]\n",
      "epoch:68 batch:52 loss:0.717653 accu:0.000000 gene_loss:[validity:0.717697]\n",
      "epoch:69 batch:52 loss:0.717642 accu:0.000000 gene_loss:[validity:0.717691]\n",
      "epoch:70 batch:52 loss:0.717636 accu:0.000000 gene_loss:[validity:0.717769]\n",
      "epoch:71 batch:52 loss:0.717638 accu:0.000000 gene_loss:[validity:0.717882]\n",
      "epoch:72 batch:52 loss:0.717605 accu:0.000000 gene_loss:[validity:0.717966]\n",
      "epoch:73 batch:52 loss:0.717641 accu:0.000000 gene_loss:[validity:0.717923]\n",
      "epoch:74 batch:52 loss:0.717647 accu:0.000000 gene_loss:[validity:0.717849]\n",
      "epoch:75 batch:52 loss:0.717638 accu:0.000000 gene_loss:[validity:0.717781]\n",
      "epoch:76 batch:52 loss:0.717626 accu:0.000000 gene_loss:[validity:0.717728]\n",
      "epoch:77 batch:52 loss:0.717615 accu:0.000000 gene_loss:[validity:0.717689]\n",
      "epoch:78 batch:52 loss:0.717604 accu:0.000000 gene_loss:[validity:0.717657]\n",
      "epoch:79 batch:52 loss:0.717596 accu:0.000000 gene_loss:[validity:0.717866]\n",
      "epoch:80 batch:52 loss:0.717516 accu:0.000000 gene_loss:[validity:0.718183]\n",
      "epoch:81 batch:52 loss:0.717634 accu:0.000000 gene_loss:[validity:0.718130]\n",
      "epoch:82 batch:52 loss:0.717659 accu:0.000000 gene_loss:[validity:0.717997]\n",
      "epoch:83 batch:52 loss:0.717653 accu:0.000000 gene_loss:[validity:0.717868]\n",
      "epoch:84 batch:52 loss:0.717639 accu:0.000000 gene_loss:[validity:0.717771]\n",
      "epoch:85 batch:52 loss:0.717634 accu:0.000000 gene_loss:[validity:0.717669]\n",
      "epoch:86 batch:52 loss:0.717617 accu:0.000000 gene_loss:[validity:0.717614]\n",
      "epoch:87 batch:52 loss:0.717603 accu:0.000000 gene_loss:[validity:0.717631]\n",
      "epoch:88 batch:52 loss:0.717583 accu:0.000000 gene_loss:[validity:0.717641]\n",
      "epoch:89 batch:52 loss:0.717587 accu:0.000000 gene_loss:[validity:0.717623]\n",
      "epoch:90 batch:52 loss:0.717591 accu:0.000000 gene_loss:[validity:0.717574]\n",
      "epoch:91 batch:52 loss:0.717605 accu:0.000000 gene_loss:[validity:0.717479]\n",
      "epoch:92 batch:52 loss:0.717580 accu:0.000000 gene_loss:[validity:0.717464]\n",
      "epoch:93 batch:52 loss:0.717566 accu:0.000000 gene_loss:[validity:0.717480]\n",
      "epoch:94 batch:52 loss:0.717563 accu:0.000000 gene_loss:[validity:0.717648]\n",
      "epoch:95 batch:52 loss:0.717563 accu:0.000000 gene_loss:[validity:0.717732]\n",
      "epoch:96 batch:52 loss:0.717597 accu:0.000000 gene_loss:[validity:0.717730]\n",
      "epoch:97 batch:52 loss:0.717614 accu:0.000000 gene_loss:[validity:0.717680]\n",
      "epoch:98 batch:52 loss:0.717612 accu:0.000000 gene_loss:[validity:0.717639]\n",
      "epoch:99 batch:52 loss:0.717607 accu:0.000000 gene_loss:[validity:0.717602]\n",
      "epoch:100 batch:52 loss:0.717600 accu:0.000000 gene_loss:[validity:0.717582]\n",
      "epoch:101 batch:52 loss:0.717594 accu:0.000000 gene_loss:[validity:0.717606]\n",
      "epoch:102 batch:52 loss:0.717578 accu:0.000000 gene_loss:[validity:0.717620]\n",
      "epoch:103 batch:52 loss:0.717584 accu:0.000000 gene_loss:[validity:0.717601]\n",
      "epoch:104 batch:52 loss:0.717581 accu:0.000000 gene_loss:[validity:0.717579]\n",
      "epoch:105 batch:52 loss:0.717577 accu:0.000000 gene_loss:[validity:0.717556]\n",
      "epoch:106 batch:52 loss:0.717566 accu:0.000000 gene_loss:[validity:0.717543]\n",
      "epoch:107 batch:52 loss:0.717597 accu:0.000000 gene_loss:[validity:0.717417]\n",
      "epoch:108 batch:52 loss:0.717555 accu:0.000000 gene_loss:[validity:0.717391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:109 batch:52 loss:0.717532 accu:0.000000 gene_loss:[validity:0.717386]\n",
      "epoch:110 batch:52 loss:0.717515 accu:0.000000 gene_loss:[validity:0.717390]\n",
      "epoch:111 batch:52 loss:0.717502 accu:0.000000 gene_loss:[validity:0.717392]\n",
      "epoch:112 batch:52 loss:0.717489 accu:0.000000 gene_loss:[validity:0.717391]\n",
      "epoch:113 batch:52 loss:0.717476 accu:0.000000 gene_loss:[validity:0.717393]\n",
      "epoch:114 batch:52 loss:0.717467 accu:0.000000 gene_loss:[validity:0.717394]\n",
      "epoch:115 batch:52 loss:0.717458 accu:0.000000 gene_loss:[validity:0.717389]\n",
      "epoch:116 batch:52 loss:0.717479 accu:0.000000 gene_loss:[validity:0.717355]\n",
      "epoch:117 batch:52 loss:0.717465 accu:0.000000 gene_loss:[validity:0.717363]\n",
      "epoch:118 batch:52 loss:0.717452 accu:0.000000 gene_loss:[validity:0.717384]\n",
      "epoch:119 batch:52 loss:0.717442 accu:0.000000 gene_loss:[validity:0.717400]\n",
      "epoch:120 batch:52 loss:0.717435 accu:0.000000 gene_loss:[validity:0.717409]\n",
      "epoch:121 batch:52 loss:0.717428 accu:0.000000 gene_loss:[validity:0.717415]\n",
      "epoch:122 batch:52 loss:0.717423 accu:0.000000 gene_loss:[validity:0.717417]\n",
      "epoch:123 batch:52 loss:0.717418 accu:0.000000 gene_loss:[validity:0.717413]\n",
      "epoch:124 batch:52 loss:0.717414 accu:0.000000 gene_loss:[validity:0.717407]\n",
      "epoch:125 batch:52 loss:0.717407 accu:0.000000 gene_loss:[validity:0.717409]\n",
      "epoch:126 batch:52 loss:0.717402 accu:0.000000 gene_loss:[validity:0.717397]\n",
      "epoch:127 batch:52 loss:0.717395 accu:0.000000 gene_loss:[validity:0.717393]\n",
      "epoch:128 batch:52 loss:0.717392 accu:0.000000 gene_loss:[validity:0.717594]\n",
      "epoch:129 batch:52 loss:0.717343 accu:0.000000 gene_loss:[validity:0.717704]\n",
      "epoch:130 batch:52 loss:0.717398 accu:0.000000 gene_loss:[validity:0.717637]\n",
      "epoch:131 batch:52 loss:0.717399 accu:0.000000 gene_loss:[validity:0.717557]\n",
      "epoch:132 batch:52 loss:0.717388 accu:0.000000 gene_loss:[validity:0.717490]\n",
      "epoch:133 batch:52 loss:0.717376 accu:0.000000 gene_loss:[validity:0.717442]\n",
      "epoch:134 batch:52 loss:0.717366 accu:0.000000 gene_loss:[validity:0.717407]\n",
      "epoch:135 batch:52 loss:0.717357 accu:0.000000 gene_loss:[validity:0.717383]\n",
      "epoch:136 batch:52 loss:0.717349 accu:0.000000 gene_loss:[validity:0.717364]\n",
      "epoch:137 batch:52 loss:0.717340 accu:0.000000 gene_loss:[validity:0.717351]\n",
      "epoch:138 batch:52 loss:0.717334 accu:0.000000 gene_loss:[validity:0.717341]\n",
      "epoch:139 batch:52 loss:0.717336 accu:0.000000 gene_loss:[validity:0.717323]\n",
      "epoch:140 batch:52 loss:0.717327 accu:0.000000 gene_loss:[validity:0.717315]\n",
      "epoch:141 batch:52 loss:0.717320 accu:0.000000 gene_loss:[validity:0.717319]\n",
      "epoch:142 batch:52 loss:0.717381 accu:0.000000 gene_loss:[validity:0.717157]\n",
      "epoch:143 batch:52 loss:0.718268 accu:0.000000 gene_loss:[validity:0.716302]\n",
      "epoch:144 batch:52 loss:0.717542 accu:0.000000 gene_loss:[validity:0.716513]\n",
      "epoch:145 batch:52 loss:0.717575 accu:0.000000 gene_loss:[validity:0.716353]\n",
      "epoch:146 batch:52 loss:0.717365 accu:0.000000 gene_loss:[validity:0.716510]\n",
      "epoch:147 batch:52 loss:0.717268 accu:0.000000 gene_loss:[validity:0.716787]\n",
      "epoch:148 batch:52 loss:0.717273 accu:0.000000 gene_loss:[validity:0.716986]\n",
      "epoch:149 batch:52 loss:0.717289 accu:0.000000 gene_loss:[validity:0.717108]\n",
      "epoch:150 batch:52 loss:0.717296 accu:0.000000 gene_loss:[validity:0.717184]\n",
      "epoch:151 batch:52 loss:0.717298 accu:0.000000 gene_loss:[validity:0.717283]\n",
      "epoch:152 batch:52 loss:0.717406 accu:0.000000 gene_loss:[validity:0.717036]\n",
      "epoch:153 batch:52 loss:0.717353 accu:0.000000 gene_loss:[validity:0.717013]\n",
      "epoch:154 batch:52 loss:0.717323 accu:0.000000 gene_loss:[validity:0.717080]\n",
      "epoch:155 batch:52 loss:0.717315 accu:0.000000 gene_loss:[validity:0.717158]\n",
      "epoch:156 batch:52 loss:0.717318 accu:0.000000 gene_loss:[validity:0.717219]\n",
      "epoch:157 batch:52 loss:0.717319 accu:0.000000 gene_loss:[validity:0.717268]\n",
      "epoch:158 batch:52 loss:0.717322 accu:0.000000 gene_loss:[validity:0.717299]\n",
      "epoch:159 batch:52 loss:0.717325 accu:0.000000 gene_loss:[validity:0.717318]\n",
      "epoch:160 batch:52 loss:0.717326 accu:0.000000 gene_loss:[validity:0.717331]\n",
      "epoch:161 batch:52 loss:0.717326 accu:0.000000 gene_loss:[validity:0.717340]\n",
      "epoch:162 batch:52 loss:0.717325 accu:0.000000 gene_loss:[validity:0.717345]\n",
      "epoch:163 batch:52 loss:0.717322 accu:0.000000 gene_loss:[validity:0.717346]\n",
      "epoch:164 batch:52 loss:0.717327 accu:0.000000 gene_loss:[validity:0.717324]\n",
      "epoch:165 batch:52 loss:0.717317 accu:0.000000 gene_loss:[validity:0.717319]\n",
      "epoch:166 batch:52 loss:0.717311 accu:0.000000 gene_loss:[validity:0.717316]\n",
      "epoch:167 batch:52 loss:0.717305 accu:0.000000 gene_loss:[validity:0.717311]\n",
      "epoch:168 batch:52 loss:0.717299 accu:0.000000 gene_loss:[validity:0.717307]\n",
      "epoch:169 batch:52 loss:0.717293 accu:0.000000 gene_loss:[validity:0.717296]\n",
      "epoch:170 batch:52 loss:0.717284 accu:0.000000 gene_loss:[validity:0.717291]\n",
      "epoch:171 batch:52 loss:0.717275 accu:0.000000 gene_loss:[validity:0.717302]\n",
      "epoch:172 batch:52 loss:0.717354 accu:0.000000 gene_loss:[validity:0.717013]\n",
      "epoch:173 batch:52 loss:0.717142 accu:0.000000 gene_loss:[validity:0.718373]\n",
      "epoch:174 batch:52 loss:0.717413 accu:0.000000 gene_loss:[validity:0.718475]\n",
      "epoch:175 batch:52 loss:0.717469 accu:0.000000 gene_loss:[validity:0.718269]\n",
      "epoch:176 batch:52 loss:0.717455 accu:0.000000 gene_loss:[validity:0.718026]\n",
      "epoch:177 batch:52 loss:0.717425 accu:0.000000 gene_loss:[validity:0.717819]\n",
      "epoch:178 batch:52 loss:0.717395 accu:0.000000 gene_loss:[validity:0.717662]\n",
      "epoch:179 batch:52 loss:0.717368 accu:0.000000 gene_loss:[validity:0.717546]\n",
      "epoch:180 batch:52 loss:0.717347 accu:0.000000 gene_loss:[validity:0.717467]\n",
      "epoch:181 batch:52 loss:0.717519 accu:0.000000 gene_loss:[validity:0.717588]\n",
      "epoch:182 batch:52 loss:0.717265 accu:0.000000 gene_loss:[validity:0.717765]\n",
      "epoch:183 batch:52 loss:0.717331 accu:0.000000 gene_loss:[validity:0.717727]\n",
      "epoch:184 batch:52 loss:0.717349 accu:0.000000 gene_loss:[validity:0.717621]\n",
      "epoch:185 batch:52 loss:0.717342 accu:0.000000 gene_loss:[validity:0.717520]\n",
      "epoch:186 batch:52 loss:0.717329 accu:0.000000 gene_loss:[validity:0.717431]\n",
      "epoch:187 batch:52 loss:0.717313 accu:0.000000 gene_loss:[validity:0.717365]\n",
      "epoch:188 batch:52 loss:0.717298 accu:0.000000 gene_loss:[validity:0.717312]\n",
      "epoch:189 batch:52 loss:0.717281 accu:0.000000 gene_loss:[validity:0.717273]\n",
      "epoch:190 batch:52 loss:0.717267 accu:0.000000 gene_loss:[validity:0.717243]\n",
      "epoch:191 batch:52 loss:0.717253 accu:0.000000 gene_loss:[validity:0.717223]\n",
      "epoch:192 batch:52 loss:0.717242 accu:0.000000 gene_loss:[validity:0.717209]\n",
      "epoch:193 batch:52 loss:0.717234 accu:0.000000 gene_loss:[validity:0.717192]\n",
      "epoch:194 batch:52 loss:0.717225 accu:0.000000 gene_loss:[validity:0.717179]\n",
      "epoch:195 batch:52 loss:0.717225 accu:0.000000 gene_loss:[validity:0.717149]\n",
      "epoch:196 batch:52 loss:0.717215 accu:0.000000 gene_loss:[validity:0.717144]\n",
      "epoch:197 batch:52 loss:0.717204 accu:0.000000 gene_loss:[validity:0.717151]\n",
      "epoch:198 batch:52 loss:0.717197 accu:0.000000 gene_loss:[validity:0.717156]\n",
      "epoch:199 batch:52 loss:0.717193 accu:0.000000 gene_loss:[validity:0.717159]\n",
      "epoch:200 batch:52 loss:0.717187 accu:0.000000 gene_loss:[validity:0.717163]\n",
      "epoch:201 batch:52 loss:0.717180 accu:0.000000 gene_loss:[validity:0.717162]\n",
      "epoch:202 batch:52 loss:0.717176 accu:0.000000 gene_loss:[validity:0.717170]\n",
      "epoch:203 batch:52 loss:0.717168 accu:0.000000 gene_loss:[validity:0.717173]\n",
      "epoch:204 batch:52 loss:0.717165 accu:0.000000 gene_loss:[validity:0.717170]\n",
      "epoch:205 batch:52 loss:0.717167 accu:0.000000 gene_loss:[validity:0.717147]\n",
      "epoch:206 batch:52 loss:0.717157 accu:0.000000 gene_loss:[validity:0.717141]\n",
      "epoch:207 batch:52 loss:0.717151 accu:0.000000 gene_loss:[validity:0.717138]\n",
      "epoch:208 batch:52 loss:0.717148 accu:0.000000 gene_loss:[validity:0.717137]\n",
      "epoch:209 batch:52 loss:0.717145 accu:0.000000 gene_loss:[validity:0.717137]\n",
      "epoch:210 batch:52 loss:0.717141 accu:0.000000 gene_loss:[validity:0.717138]\n",
      "epoch:211 batch:52 loss:0.717138 accu:0.000000 gene_loss:[validity:0.717141]\n",
      "epoch:212 batch:52 loss:0.717136 accu:0.000000 gene_loss:[validity:0.717140]\n",
      "epoch:213 batch:52 loss:0.717135 accu:0.000000 gene_loss:[validity:0.717140]\n",
      "epoch:214 batch:52 loss:0.717133 accu:0.000000 gene_loss:[validity:0.717140]\n",
      "epoch:215 batch:52 loss:0.717132 accu:0.000000 gene_loss:[validity:0.717141]\n",
      "epoch:216 batch:52 loss:0.717131 accu:0.000000 gene_loss:[validity:0.717142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:217 batch:52 loss:0.717130 accu:0.000000 gene_loss:[validity:0.717143]\n",
      "epoch:218 batch:52 loss:0.717131 accu:0.000000 gene_loss:[validity:0.717144]\n",
      "epoch:219 batch:52 loss:0.717130 accu:0.000000 gene_loss:[validity:0.717145]\n",
      "epoch:220 batch:52 loss:0.717131 accu:0.000000 gene_loss:[validity:0.717147]\n",
      "epoch:221 batch:52 loss:0.717130 accu:0.000000 gene_loss:[validity:0.717145]\n",
      "epoch:222 batch:52 loss:0.717130 accu:0.000000 gene_loss:[validity:0.717145]\n",
      "epoch:223 batch:52 loss:0.717130 accu:0.000000 gene_loss:[validity:0.717150]\n",
      "epoch:224 batch:52 loss:0.717130 accu:0.000000 gene_loss:[validity:0.717184]\n",
      "epoch:225 batch:52 loss:0.717135 accu:0.000000 gene_loss:[validity:0.717195]\n",
      "epoch:226 batch:52 loss:0.717140 accu:0.000000 gene_loss:[validity:0.717183]\n",
      "epoch:227 batch:52 loss:0.717137 accu:0.000000 gene_loss:[validity:0.717165]\n",
      "epoch:228 batch:52 loss:0.717151 accu:0.000000 gene_loss:[validity:0.717100]\n",
      "epoch:229 batch:52 loss:0.717132 accu:0.000000 gene_loss:[validity:0.717082]\n",
      "epoch:230 batch:52 loss:0.717124 accu:0.000000 gene_loss:[validity:0.717069]\n",
      "epoch:231 batch:52 loss:0.717121 accu:0.000000 gene_loss:[validity:0.717032]\n",
      "epoch:232 batch:52 loss:0.717107 accu:0.000000 gene_loss:[validity:0.717024]\n",
      "epoch:233 batch:52 loss:0.717094 accu:0.000000 gene_loss:[validity:0.717023]\n",
      "epoch:234 batch:52 loss:0.717083 accu:0.000000 gene_loss:[validity:0.717025]\n",
      "epoch:235 batch:52 loss:0.717073 accu:0.000000 gene_loss:[validity:0.717020]\n",
      "epoch:236 batch:52 loss:0.717060 accu:0.000000 gene_loss:[validity:0.717017]\n",
      "epoch:237 batch:52 loss:0.717050 accu:0.000000 gene_loss:[validity:0.717010]\n",
      "epoch:238 batch:52 loss:0.717039 accu:0.000000 gene_loss:[validity:0.717003]\n",
      "epoch:239 batch:52 loss:0.717029 accu:0.000000 gene_loss:[validity:0.716995]\n",
      "epoch:240 batch:52 loss:0.717020 accu:0.000000 gene_loss:[validity:0.717032]\n",
      "epoch:241 batch:52 loss:0.717005 accu:0.000000 gene_loss:[validity:0.717046]\n",
      "epoch:242 batch:52 loss:0.717162 accu:0.000000 gene_loss:[validity:0.717224]\n",
      "epoch:243 batch:52 loss:0.716941 accu:0.000000 gene_loss:[validity:0.717679]\n",
      "epoch:244 batch:52 loss:0.716999 accu:0.000000 gene_loss:[validity:0.717865]\n",
      "epoch:245 batch:52 loss:0.717103 accu:0.000000 gene_loss:[validity:0.717734]\n",
      "epoch:246 batch:52 loss:0.717126 accu:0.000000 gene_loss:[validity:0.717530]\n",
      "epoch:247 batch:52 loss:0.717114 accu:0.000000 gene_loss:[validity:0.717349]\n",
      "epoch:248 batch:52 loss:0.717095 accu:0.000000 gene_loss:[validity:0.717210]\n",
      "epoch:249 batch:52 loss:0.717076 accu:0.000000 gene_loss:[validity:0.717111]\n",
      "epoch:250 batch:52 loss:0.717060 accu:0.000000 gene_loss:[validity:0.717039]\n",
      "epoch:251 batch:52 loss:0.717047 accu:0.000000 gene_loss:[validity:0.716992]\n",
      "epoch:252 batch:52 loss:0.717037 accu:0.000000 gene_loss:[validity:0.716950]\n",
      "epoch:253 batch:52 loss:0.717022 accu:0.000000 gene_loss:[validity:0.716928]\n",
      "epoch:254 batch:52 loss:0.717014 accu:0.000000 gene_loss:[validity:0.716913]\n",
      "epoch:255 batch:52 loss:0.717005 accu:0.000000 gene_loss:[validity:0.716905]\n",
      "epoch:256 batch:52 loss:0.716998 accu:0.000000 gene_loss:[validity:0.716900]\n",
      "epoch:257 batch:52 loss:0.716991 accu:0.000000 gene_loss:[validity:0.716917]\n",
      "epoch:258 batch:52 loss:0.716985 accu:0.000000 gene_loss:[validity:0.716891]\n",
      "epoch:259 batch:52 loss:0.716979 accu:0.000000 gene_loss:[validity:0.716884]\n",
      "epoch:260 batch:52 loss:0.716973 accu:0.000000 gene_loss:[validity:0.716882]\n",
      "epoch:261 batch:52 loss:0.716967 accu:0.000000 gene_loss:[validity:0.716882]\n",
      "epoch:262 batch:52 loss:0.716961 accu:0.000000 gene_loss:[validity:0.716881]\n",
      "epoch:263 batch:52 loss:0.716957 accu:0.000000 gene_loss:[validity:0.716884]\n",
      "epoch:264 batch:52 loss:0.716953 accu:0.000000 gene_loss:[validity:0.717170]\n",
      "epoch:265 batch:52 loss:0.716874 accu:0.000000 gene_loss:[validity:0.717483]\n",
      "epoch:266 batch:52 loss:0.716987 accu:0.000000 gene_loss:[validity:0.717445]\n",
      "epoch:267 batch:52 loss:0.717016 accu:0.000000 gene_loss:[validity:0.717317]\n",
      "epoch:268 batch:52 loss:0.717014 accu:0.000000 gene_loss:[validity:0.717194]\n",
      "epoch:269 batch:52 loss:0.717005 accu:0.000000 gene_loss:[validity:0.717102]\n",
      "epoch:270 batch:52 loss:0.716997 accu:0.000000 gene_loss:[validity:0.717029]\n",
      "epoch:271 batch:52 loss:0.716986 accu:0.000000 gene_loss:[validity:0.716984]\n",
      "epoch:272 batch:52 loss:0.716981 accu:0.000000 gene_loss:[validity:0.716957]\n",
      "epoch:273 batch:52 loss:0.716973 accu:0.000000 gene_loss:[validity:0.716942]\n",
      "epoch:274 batch:52 loss:0.716973 accu:0.000000 gene_loss:[validity:0.716923]\n",
      "epoch:275 batch:52 loss:0.716969 accu:0.000000 gene_loss:[validity:0.716917]\n",
      "epoch:276 batch:52 loss:0.716967 accu:0.000000 gene_loss:[validity:0.716911]\n",
      "epoch:277 batch:52 loss:0.716964 accu:0.000000 gene_loss:[validity:0.716912]\n",
      "epoch:278 batch:52 loss:0.716962 accu:0.000000 gene_loss:[validity:0.716914]\n",
      "epoch:279 batch:52 loss:0.716962 accu:0.000000 gene_loss:[validity:0.716915]\n",
      "epoch:280 batch:52 loss:0.716964 accu:0.000000 gene_loss:[validity:0.717089]\n",
      "epoch:281 batch:52 loss:0.716931 accu:0.000000 gene_loss:[validity:0.718322]\n",
      "epoch:282 batch:52 loss:0.717243 accu:0.000000 gene_loss:[validity:0.718424]\n",
      "epoch:283 batch:52 loss:0.717314 accu:0.000000 gene_loss:[validity:0.718430]\n",
      "epoch:284 batch:52 loss:0.717479 accu:0.000000 gene_loss:[validity:0.717929]\n",
      "epoch:285 batch:52 loss:0.717384 accu:0.000000 gene_loss:[validity:0.717605]\n",
      "epoch:286 batch:52 loss:0.717295 accu:0.000000 gene_loss:[validity:0.717440]\n",
      "epoch:287 batch:52 loss:0.717251 accu:0.000000 gene_loss:[validity:0.717317]\n",
      "epoch:288 batch:52 loss:0.717212 accu:0.000000 gene_loss:[validity:0.717226]\n",
      "epoch:289 batch:52 loss:0.717179 accu:0.000000 gene_loss:[validity:0.717155]\n",
      "epoch:290 batch:52 loss:0.717147 accu:0.000000 gene_loss:[validity:0.717095]\n",
      "epoch:291 batch:52 loss:0.717120 accu:0.000000 gene_loss:[validity:0.717049]\n",
      "epoch:292 batch:52 loss:0.717096 accu:0.000000 gene_loss:[validity:0.717010]\n",
      "epoch:293 batch:52 loss:0.717073 accu:0.000000 gene_loss:[validity:0.716983]\n",
      "epoch:294 batch:52 loss:0.717051 accu:0.000000 gene_loss:[validity:0.716961]\n",
      "epoch:295 batch:52 loss:0.717035 accu:0.000000 gene_loss:[validity:0.716942]\n",
      "epoch:296 batch:52 loss:0.717018 accu:0.000000 gene_loss:[validity:0.716929]\n",
      "epoch:297 batch:52 loss:0.717005 accu:0.000000 gene_loss:[validity:0.716916]\n",
      "epoch:298 batch:52 loss:0.716992 accu:0.000000 gene_loss:[validity:0.716904]\n",
      "epoch:299 batch:52 loss:0.716980 accu:0.000000 gene_loss:[validity:0.716892]\n",
      "epoch:300 batch:52 loss:0.716965 accu:0.000000 gene_loss:[validity:0.716884]\n",
      "epoch:301 batch:52 loss:0.716953 accu:0.000000 gene_loss:[validity:0.716874]\n",
      "epoch:302 batch:52 loss:0.716943 accu:0.000000 gene_loss:[validity:0.716868]\n",
      "epoch:303 batch:52 loss:0.716996 accu:0.000000 gene_loss:[validity:0.716651]\n",
      "epoch:304 batch:52 loss:0.716950 accu:0.000000 gene_loss:[validity:0.716596]\n",
      "epoch:305 batch:52 loss:0.716912 accu:0.000000 gene_loss:[validity:0.716629]\n",
      "epoch:306 batch:52 loss:0.716896 accu:0.000000 gene_loss:[validity:0.716680]\n",
      "epoch:307 batch:52 loss:0.716887 accu:0.000000 gene_loss:[validity:0.716724]\n",
      "epoch:308 batch:52 loss:0.716883 accu:0.000000 gene_loss:[validity:0.716763]\n",
      "epoch:309 batch:52 loss:0.716921 accu:0.000000 gene_loss:[validity:0.716753]\n",
      "epoch:310 batch:52 loss:0.716913 accu:0.000000 gene_loss:[validity:0.716752]\n",
      "epoch:311 batch:52 loss:0.716899 accu:0.000000 gene_loss:[validity:0.716780]\n",
      "epoch:312 batch:52 loss:0.716894 accu:0.000000 gene_loss:[validity:0.716796]\n",
      "epoch:313 batch:52 loss:0.716888 accu:0.000000 gene_loss:[validity:0.716807]\n",
      "epoch:314 batch:52 loss:0.716881 accu:0.000000 gene_loss:[validity:0.716815]\n",
      "epoch:315 batch:52 loss:0.716875 accu:0.000000 gene_loss:[validity:0.716819]\n",
      "epoch:316 batch:52 loss:0.716868 accu:0.000000 gene_loss:[validity:0.716822]\n",
      "epoch:317 batch:52 loss:0.716862 accu:0.000000 gene_loss:[validity:0.716821]\n",
      "epoch:318 batch:52 loss:0.716857 accu:0.000000 gene_loss:[validity:0.716817]\n",
      "epoch:319 batch:52 loss:0.716850 accu:0.000000 gene_loss:[validity:0.716815]\n",
      "epoch:320 batch:52 loss:0.716845 accu:0.000000 gene_loss:[validity:0.716812]\n",
      "epoch:321 batch:52 loss:0.716841 accu:0.000000 gene_loss:[validity:0.716811]\n",
      "epoch:322 batch:52 loss:0.716835 accu:0.000000 gene_loss:[validity:0.716814]\n",
      "epoch:323 batch:52 loss:0.716830 accu:0.000000 gene_loss:[validity:0.716808]\n",
      "epoch:324 batch:52 loss:0.716827 accu:0.000000 gene_loss:[validity:0.716803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:325 batch:52 loss:0.716821 accu:0.000000 gene_loss:[validity:0.716802]\n",
      "epoch:326 batch:52 loss:0.716817 accu:0.000000 gene_loss:[validity:0.716798]\n",
      "epoch:327 batch:52 loss:0.716812 accu:0.000000 gene_loss:[validity:0.716795]\n",
      "epoch:328 batch:52 loss:0.716809 accu:0.000000 gene_loss:[validity:0.716792]\n",
      "epoch:329 batch:52 loss:0.716804 accu:0.000000 gene_loss:[validity:0.716789]\n",
      "epoch:330 batch:52 loss:0.716800 accu:0.000000 gene_loss:[validity:0.716788]\n",
      "epoch:331 batch:52 loss:0.716796 accu:0.000000 gene_loss:[validity:0.716791]\n",
      "epoch:332 batch:52 loss:0.716793 accu:0.000000 gene_loss:[validity:0.716789]\n",
      "epoch:333 batch:52 loss:0.716792 accu:0.000000 gene_loss:[validity:0.716789]\n",
      "epoch:334 batch:52 loss:0.716789 accu:0.000000 gene_loss:[validity:0.716790]\n",
      "epoch:335 batch:52 loss:0.716787 accu:0.000000 gene_loss:[validity:0.716795]\n",
      "epoch:336 batch:52 loss:0.716790 accu:0.000000 gene_loss:[validity:0.716801]\n",
      "epoch:337 batch:52 loss:0.716791 accu:0.000000 gene_loss:[validity:0.716811]\n",
      "epoch:338 batch:52 loss:0.716796 accu:0.000000 gene_loss:[validity:0.716820]\n",
      "epoch:339 batch:52 loss:0.716803 accu:0.000000 gene_loss:[validity:0.716839]\n",
      "epoch:340 batch:52 loss:0.716816 accu:0.000000 gene_loss:[validity:0.716854]\n",
      "epoch:341 batch:52 loss:0.716827 accu:0.000000 gene_loss:[validity:0.716877]\n",
      "epoch:342 batch:52 loss:0.716842 accu:0.000000 gene_loss:[validity:0.716898]\n",
      "epoch:343 batch:52 loss:0.716856 accu:0.000000 gene_loss:[validity:0.716913]\n",
      "epoch:344 batch:52 loss:0.716870 accu:0.000000 gene_loss:[validity:0.716915]\n",
      "epoch:345 batch:52 loss:0.716876 accu:0.000000 gene_loss:[validity:0.716904]\n",
      "epoch:346 batch:52 loss:0.716872 accu:0.000000 gene_loss:[validity:0.716879]\n",
      "epoch:347 batch:52 loss:0.716863 accu:0.000000 gene_loss:[validity:0.716843]\n",
      "epoch:348 batch:52 loss:0.716843 accu:0.000000 gene_loss:[validity:0.716803]\n",
      "epoch:349 batch:52 loss:0.716821 accu:0.000000 gene_loss:[validity:0.716765]\n",
      "epoch:350 batch:52 loss:0.716797 accu:0.000000 gene_loss:[validity:0.716733]\n",
      "epoch:351 batch:52 loss:0.716773 accu:0.000000 gene_loss:[validity:0.716708]\n",
      "epoch:352 batch:52 loss:0.716752 accu:0.000000 gene_loss:[validity:0.716689]\n",
      "epoch:353 batch:52 loss:0.716734 accu:0.000000 gene_loss:[validity:0.716677]\n",
      "epoch:354 batch:52 loss:0.716719 accu:0.000000 gene_loss:[validity:0.716672]\n",
      "epoch:355 batch:52 loss:0.716708 accu:0.000000 gene_loss:[validity:0.716674]\n",
      "epoch:356 batch:52 loss:0.716702 accu:0.000000 gene_loss:[validity:0.716682]\n",
      "epoch:357 batch:52 loss:0.716699 accu:0.000000 gene_loss:[validity:0.716689]\n",
      "epoch:358 batch:52 loss:0.716698 accu:0.000000 gene_loss:[validity:0.716700]\n",
      "epoch:359 batch:52 loss:0.716701 accu:0.000000 gene_loss:[validity:0.716696]\n",
      "epoch:360 batch:52 loss:0.716697 accu:0.000000 gene_loss:[validity:0.716719]\n",
      "epoch:361 batch:52 loss:0.716687 accu:0.000000 gene_loss:[validity:0.716713]\n",
      "epoch:362 batch:52 loss:0.716680 accu:0.000000 gene_loss:[validity:0.716691]\n",
      "epoch:363 batch:52 loss:0.716669 accu:0.000000 gene_loss:[validity:0.716671]\n",
      "epoch:364 batch:52 loss:0.716659 accu:0.000000 gene_loss:[validity:0.716655]\n",
      "epoch:365 batch:52 loss:0.716652 accu:0.000000 gene_loss:[validity:0.716650]\n",
      "epoch:366 batch:52 loss:0.716648 accu:0.000000 gene_loss:[validity:0.716649]\n",
      "epoch:367 batch:52 loss:0.716649 accu:0.000000 gene_loss:[validity:0.716642]\n",
      "epoch:368 batch:52 loss:0.716647 accu:0.000000 gene_loss:[validity:0.716642]\n",
      "epoch:369 batch:52 loss:0.716644 accu:0.000000 gene_loss:[validity:0.716634]\n",
      "epoch:370 batch:52 loss:0.716638 accu:0.000000 gene_loss:[validity:0.716631]\n",
      "epoch:371 batch:52 loss:0.716633 accu:0.000000 gene_loss:[validity:0.716622]\n",
      "epoch:372 batch:52 loss:0.716625 accu:0.000000 gene_loss:[validity:0.716627]\n",
      "epoch:373 batch:52 loss:0.716676 accu:0.000000 gene_loss:[validity:0.716530]\n",
      "epoch:374 batch:52 loss:0.716665 accu:0.000000 gene_loss:[validity:0.716550]\n",
      "epoch:375 batch:52 loss:0.716621 accu:0.000000 gene_loss:[validity:0.720334]\n",
      "epoch:376 batch:52 loss:0.717672 accu:0.000000 gene_loss:[validity:0.720348]\n",
      "epoch:377 batch:52 loss:0.717760 accu:0.000000 gene_loss:[validity:0.719605]\n",
      "epoch:378 batch:52 loss:0.717647 accu:0.000000 gene_loss:[validity:0.718858]\n",
      "epoch:379 batch:52 loss:0.717506 accu:0.000000 gene_loss:[validity:0.718264]\n",
      "epoch:380 batch:52 loss:0.717384 accu:0.000000 gene_loss:[validity:0.717836]\n",
      "epoch:381 batch:52 loss:0.717286 accu:0.000000 gene_loss:[validity:0.717537]\n",
      "epoch:382 batch:52 loss:0.717209 accu:0.000000 gene_loss:[validity:0.717385]\n",
      "epoch:383 batch:52 loss:0.717151 accu:0.000000 gene_loss:[validity:0.717257]\n",
      "epoch:384 batch:52 loss:0.717128 accu:0.000000 gene_loss:[validity:0.717149]\n",
      "epoch:385 batch:52 loss:0.717088 accu:0.000000 gene_loss:[validity:0.717072]\n",
      "epoch:386 batch:52 loss:0.717103 accu:0.000000 gene_loss:[validity:0.716908]\n",
      "epoch:387 batch:52 loss:0.717072 accu:0.000000 gene_loss:[validity:0.716888]\n",
      "epoch:388 batch:52 loss:0.717026 accu:0.000000 gene_loss:[validity:0.716905]\n",
      "epoch:389 batch:52 loss:0.717005 accu:0.000000 gene_loss:[validity:0.716916]\n",
      "epoch:390 batch:52 loss:0.716987 accu:0.000000 gene_loss:[validity:0.716917]\n",
      "epoch:391 batch:52 loss:0.716969 accu:0.000000 gene_loss:[validity:0.716915]\n",
      "epoch:392 batch:52 loss:0.716954 accu:0.000000 gene_loss:[validity:0.716907]\n",
      "epoch:393 batch:52 loss:0.716938 accu:0.000000 gene_loss:[validity:0.716902]\n",
      "epoch:394 batch:52 loss:0.716927 accu:0.000000 gene_loss:[validity:0.716890]\n",
      "epoch:395 batch:52 loss:0.716915 accu:0.000000 gene_loss:[validity:0.716923]\n",
      "epoch:396 batch:52 loss:0.716898 accu:0.000000 gene_loss:[validity:0.716935]\n",
      "epoch:397 batch:52 loss:0.716898 accu:0.000000 gene_loss:[validity:0.716919]\n",
      "epoch:398 batch:52 loss:0.716954 accu:0.000000 gene_loss:[validity:0.716674]\n",
      "epoch:399 batch:52 loss:0.717137 accu:0.000000 gene_loss:[validity:0.716331]\n",
      "epoch:400 batch:52 loss:0.716951 accu:0.000000 gene_loss:[validity:0.716326]\n",
      "epoch:401 batch:52 loss:0.716868 accu:0.000000 gene_loss:[validity:0.716454]\n",
      "epoch:402 batch:52 loss:0.716849 accu:0.000000 gene_loss:[validity:0.716572]\n",
      "epoch:403 batch:52 loss:0.716847 accu:0.000000 gene_loss:[validity:0.716655]\n",
      "epoch:404 batch:52 loss:0.716845 accu:0.000000 gene_loss:[validity:0.717047]\n",
      "epoch:405 batch:52 loss:0.716935 accu:0.000000 gene_loss:[validity:0.717109]\n",
      "epoch:406 batch:52 loss:0.716903 accu:0.000000 gene_loss:[validity:0.721872]\n",
      "epoch:407 batch:52 loss:0.718658 accu:0.000000 gene_loss:[validity:0.720644]\n",
      "epoch:408 batch:52 loss:0.717874 accu:0.000000 gene_loss:[validity:0.720320]\n",
      "epoch:409 batch:52 loss:0.717481 accu:0.000000 gene_loss:[validity:0.720093]\n",
      "epoch:410 batch:52 loss:0.717258 accu:0.000000 gene_loss:[validity:0.719675]\n",
      "epoch:411 batch:52 loss:0.717167 accu:0.000000 gene_loss:[validity:0.719050]\n",
      "epoch:412 batch:52 loss:0.717083 accu:0.000000 gene_loss:[validity:0.718480]\n",
      "epoch:413 batch:52 loss:0.717024 accu:0.000000 gene_loss:[validity:0.718010]\n",
      "epoch:414 batch:52 loss:0.716973 accu:0.000000 gene_loss:[validity:0.717834]\n",
      "epoch:415 batch:52 loss:0.716891 accu:0.000000 gene_loss:[validity:0.718012]\n",
      "epoch:416 batch:52 loss:0.717004 accu:0.000000 gene_loss:[validity:0.717717]\n",
      "epoch:417 batch:52 loss:0.716964 accu:0.000000 gene_loss:[validity:0.717417]\n",
      "epoch:418 batch:52 loss:0.716914 accu:0.000000 gene_loss:[validity:0.717187]\n",
      "epoch:419 batch:52 loss:0.716871 accu:0.000000 gene_loss:[validity:0.717022]\n",
      "epoch:420 batch:52 loss:0.716833 accu:0.000000 gene_loss:[validity:0.716905]\n",
      "epoch:421 batch:52 loss:0.716805 accu:0.000000 gene_loss:[validity:0.716826]\n",
      "epoch:422 batch:52 loss:0.716781 accu:0.000000 gene_loss:[validity:0.716770]\n",
      "epoch:423 batch:52 loss:0.716762 accu:0.000000 gene_loss:[validity:0.716735]\n",
      "epoch:424 batch:52 loss:0.716743 accu:0.000000 gene_loss:[validity:0.716851]\n",
      "epoch:425 batch:52 loss:0.716693 accu:0.000000 gene_loss:[validity:0.716973]\n",
      "epoch:426 batch:52 loss:0.716737 accu:0.000000 gene_loss:[validity:0.716920]\n",
      "epoch:427 batch:52 loss:0.716733 accu:0.000000 gene_loss:[validity:0.716850]\n",
      "epoch:428 batch:52 loss:0.716721 accu:0.000000 gene_loss:[validity:0.716789]\n",
      "epoch:429 batch:52 loss:0.716707 accu:0.000000 gene_loss:[validity:0.716744]\n",
      "epoch:430 batch:52 loss:0.716695 accu:0.000000 gene_loss:[validity:0.716706]\n",
      "epoch:431 batch:52 loss:0.716684 accu:0.000000 gene_loss:[validity:0.716680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:432 batch:52 loss:0.716676 accu:0.000000 gene_loss:[validity:0.716661]\n",
      "epoch:433 batch:52 loss:0.716667 accu:0.000000 gene_loss:[validity:0.716649]\n",
      "epoch:434 batch:52 loss:0.716663 accu:0.000000 gene_loss:[validity:0.716637]\n",
      "epoch:435 batch:52 loss:0.716657 accu:0.000000 gene_loss:[validity:0.716630]\n",
      "epoch:436 batch:52 loss:0.718338 accu:0.000000 gene_loss:[validity:0.716913]\n",
      "epoch:437 batch:52 loss:0.718298 accu:0.000000 gene_loss:[validity:0.716349]\n",
      "epoch:438 batch:52 loss:0.718045 accu:0.000000 gene_loss:[validity:0.715770]\n",
      "epoch:439 batch:52 loss:0.717639 accu:0.000000 gene_loss:[validity:0.715825]\n",
      "epoch:440 batch:52 loss:0.717484 accu:0.000000 gene_loss:[validity:0.715994]\n",
      "epoch:441 batch:52 loss:0.717405 accu:0.000000 gene_loss:[validity:0.716131]\n",
      "epoch:442 batch:52 loss:0.717348 accu:0.000000 gene_loss:[validity:0.716576]\n",
      "epoch:443 batch:52 loss:0.717254 accu:0.000000 gene_loss:[validity:0.716851]\n",
      "epoch:444 batch:52 loss:0.717299 accu:0.000000 gene_loss:[validity:0.716853]\n",
      "epoch:445 batch:52 loss:0.717280 accu:0.000000 gene_loss:[validity:0.716797]\n",
      "epoch:446 batch:52 loss:0.717237 accu:0.000000 gene_loss:[validity:0.716731]\n",
      "epoch:447 batch:52 loss:0.717195 accu:0.000000 gene_loss:[validity:0.716684]\n",
      "epoch:448 batch:52 loss:0.717148 accu:0.000000 gene_loss:[validity:0.716648]\n",
      "epoch:449 batch:52 loss:0.717106 accu:0.000000 gene_loss:[validity:0.716635]\n",
      "epoch:450 batch:52 loss:0.717064 accu:0.000000 gene_loss:[validity:0.716600]\n",
      "epoch:451 batch:52 loss:0.717065 accu:0.000000 gene_loss:[validity:0.716555]\n",
      "epoch:452 batch:52 loss:0.717024 accu:0.000000 gene_loss:[validity:0.716813]\n",
      "epoch:453 batch:52 loss:0.716961 accu:0.000000 gene_loss:[validity:0.716981]\n",
      "epoch:454 batch:52 loss:0.716998 accu:0.000000 gene_loss:[validity:0.716945]\n",
      "epoch:455 batch:52 loss:0.717908 accu:0.000000 gene_loss:[validity:0.715724]\n",
      "epoch:456 batch:52 loss:0.717433 accu:0.000000 gene_loss:[validity:0.715557]\n",
      "epoch:457 batch:52 loss:0.717250 accu:0.000000 gene_loss:[validity:0.715703]\n",
      "epoch:458 batch:52 loss:0.717164 accu:0.000000 gene_loss:[validity:0.715920]\n",
      "epoch:459 batch:52 loss:0.717109 accu:0.000000 gene_loss:[validity:0.716116]\n",
      "epoch:460 batch:52 loss:0.717077 accu:0.000000 gene_loss:[validity:0.716257]\n",
      "epoch:461 batch:52 loss:0.717047 accu:0.000000 gene_loss:[validity:0.716365]\n",
      "epoch:462 batch:52 loss:0.717019 accu:0.000000 gene_loss:[validity:0.716441]\n",
      "epoch:463 batch:52 loss:0.716993 accu:0.000000 gene_loss:[validity:0.716502]\n",
      "epoch:464 batch:52 loss:0.716968 accu:0.000000 gene_loss:[validity:0.716543]\n",
      "epoch:465 batch:52 loss:0.716944 accu:0.000000 gene_loss:[validity:0.716578]\n",
      "epoch:466 batch:52 loss:0.716923 accu:0.000000 gene_loss:[validity:0.716598]\n",
      "epoch:467 batch:52 loss:0.716909 accu:0.000000 gene_loss:[validity:0.716804]\n",
      "epoch:468 batch:52 loss:0.716860 accu:0.000000 gene_loss:[validity:0.716930]\n",
      "epoch:469 batch:52 loss:0.716897 accu:0.000000 gene_loss:[validity:0.716909]\n",
      "epoch:470 batch:52 loss:0.716899 accu:0.000000 gene_loss:[validity:0.716882]\n",
      "epoch:471 batch:52 loss:0.718968 accu:0.000000 gene_loss:[validity:0.714154]\n",
      "epoch:472 batch:52 loss:0.717617 accu:0.000000 gene_loss:[validity:0.714246]\n",
      "epoch:473 batch:52 loss:0.717250 accu:0.000000 gene_loss:[validity:0.714877]\n",
      "epoch:474 batch:52 loss:0.717141 accu:0.000000 gene_loss:[validity:0.715460]\n",
      "epoch:475 batch:52 loss:0.717105 accu:0.000000 gene_loss:[validity:0.715920]\n",
      "epoch:476 batch:52 loss:0.717090 accu:0.000000 gene_loss:[validity:0.716246]\n",
      "epoch:477 batch:52 loss:0.717081 accu:0.000000 gene_loss:[validity:0.716472]\n",
      "epoch:478 batch:52 loss:0.717078 accu:0.000000 gene_loss:[validity:0.716627]\n",
      "epoch:479 batch:52 loss:0.717070 accu:0.000000 gene_loss:[validity:0.716728]\n",
      "epoch:480 batch:52 loss:0.717063 accu:0.000000 gene_loss:[validity:0.719502]\n",
      "epoch:481 batch:52 loss:0.717708 accu:0.000000 gene_loss:[validity:0.718623]\n",
      "epoch:482 batch:52 loss:0.717489 accu:0.000000 gene_loss:[validity:0.718399]\n",
      "epoch:483 batch:52 loss:0.717280 accu:0.000000 gene_loss:[validity:0.718291]\n",
      "epoch:484 batch:52 loss:0.717169 accu:0.000000 gene_loss:[validity:0.718109]\n",
      "epoch:485 batch:52 loss:0.717117 accu:0.000000 gene_loss:[validity:0.717807]\n",
      "epoch:486 batch:52 loss:0.717050 accu:0.000000 gene_loss:[validity:0.717542]\n",
      "epoch:487 batch:52 loss:0.716996 accu:0.000000 gene_loss:[validity:0.717467]\n",
      "epoch:488 batch:52 loss:0.716936 accu:0.000000 gene_loss:[validity:0.717445]\n",
      "epoch:489 batch:52 loss:0.716933 accu:0.000000 gene_loss:[validity:0.717296]\n",
      "epoch:490 batch:52 loss:0.716924 accu:0.000000 gene_loss:[validity:0.717139]\n",
      "epoch:491 batch:52 loss:0.716898 accu:0.000000 gene_loss:[validity:0.717011]\n",
      "epoch:492 batch:52 loss:0.716871 accu:0.000000 gene_loss:[validity:0.716911]\n",
      "epoch:493 batch:52 loss:0.716845 accu:0.000000 gene_loss:[validity:0.716834]\n",
      "epoch:494 batch:52 loss:0.716824 accu:0.000000 gene_loss:[validity:0.716780]\n",
      "epoch:495 batch:52 loss:0.716804 accu:0.000000 gene_loss:[validity:0.716738]\n",
      "epoch:496 batch:52 loss:0.716788 accu:0.000000 gene_loss:[validity:0.716708]\n",
      "epoch:497 batch:52 loss:0.716773 accu:0.000000 gene_loss:[validity:0.716690]\n",
      "epoch:498 batch:52 loss:0.716761 accu:0.000000 gene_loss:[validity:0.716673]\n",
      "epoch:499 batch:52 loss:0.716749 accu:0.000000 gene_loss:[validity:0.717071]\n",
      "epoch:500 batch:52 loss:0.716812 accu:0.000000 gene_loss:[validity:0.716994]\n",
      "epoch:501 batch:52 loss:0.718467 accu:0.000000 gene_loss:[validity:0.714983]\n",
      "epoch:502 batch:52 loss:0.717582 accu:0.000000 gene_loss:[validity:0.714885]\n",
      "epoch:503 batch:52 loss:0.717284 accu:0.000000 gene_loss:[validity:0.715254]\n",
      "epoch:504 batch:52 loss:0.717167 accu:0.000000 gene_loss:[validity:0.715661]\n",
      "epoch:505 batch:52 loss:0.717113 accu:0.000000 gene_loss:[validity:0.716012]\n",
      "epoch:506 batch:52 loss:0.717079 accu:0.000000 gene_loss:[validity:0.716270]\n",
      "epoch:507 batch:52 loss:0.717055 accu:0.000000 gene_loss:[validity:0.716460]\n",
      "epoch:508 batch:52 loss:0.717030 accu:0.000000 gene_loss:[validity:0.716588]\n",
      "epoch:509 batch:52 loss:0.717008 accu:0.000000 gene_loss:[validity:0.716674]\n",
      "epoch:510 batch:52 loss:0.716981 accu:0.000000 gene_loss:[validity:0.716724]\n",
      "epoch:511 batch:52 loss:0.716956 accu:0.000000 gene_loss:[validity:0.716754]\n",
      "epoch:512 batch:52 loss:0.716927 accu:0.000000 gene_loss:[validity:0.716777]\n",
      "epoch:513 batch:52 loss:0.716896 accu:0.000000 gene_loss:[validity:0.716787]\n",
      "epoch:514 batch:52 loss:0.716873 accu:0.000000 gene_loss:[validity:0.716775]\n",
      "epoch:515 batch:52 loss:0.716850 accu:0.000000 gene_loss:[validity:0.716762]\n",
      "epoch:516 batch:52 loss:0.716826 accu:0.000000 gene_loss:[validity:0.716746]\n",
      "epoch:517 batch:52 loss:0.716818 accu:0.000000 gene_loss:[validity:0.716691]\n",
      "epoch:518 batch:52 loss:0.716787 accu:0.000000 gene_loss:[validity:0.716772]\n",
      "epoch:519 batch:52 loss:0.716754 accu:0.000000 gene_loss:[validity:0.716847]\n",
      "epoch:520 batch:52 loss:0.716770 accu:0.000000 gene_loss:[validity:0.716828]\n",
      "epoch:521 batch:52 loss:0.716765 accu:0.000000 gene_loss:[validity:0.716790]\n",
      "epoch:522 batch:52 loss:0.716755 accu:0.000000 gene_loss:[validity:0.716747]\n",
      "epoch:523 batch:52 loss:0.716734 accu:0.000000 gene_loss:[validity:0.716714]\n",
      "epoch:524 batch:52 loss:0.716714 accu:0.000000 gene_loss:[validity:0.716700]\n",
      "epoch:525 batch:52 loss:0.716707 accu:0.000000 gene_loss:[validity:0.716663]\n",
      "epoch:526 batch:52 loss:0.716687 accu:0.000000 gene_loss:[validity:0.716645]\n",
      "epoch:527 batch:52 loss:0.716672 accu:0.000000 gene_loss:[validity:0.716634]\n",
      "epoch:528 batch:52 loss:0.716690 accu:0.000000 gene_loss:[validity:0.716549]\n",
      "epoch:529 batch:52 loss:0.716636 accu:0.000000 gene_loss:[validity:0.716893]\n",
      "epoch:530 batch:52 loss:0.716813 accu:0.000000 gene_loss:[validity:0.716643]\n",
      "epoch:531 batch:52 loss:0.716712 accu:0.000000 gene_loss:[validity:0.716595]\n",
      "epoch:532 batch:52 loss:0.716815 accu:0.000000 gene_loss:[validity:0.716257]\n",
      "epoch:533 batch:52 loss:0.716817 accu:0.000000 gene_loss:[validity:0.716166]\n",
      "epoch:534 batch:52 loss:0.717210 accu:0.000000 gene_loss:[validity:0.715597]\n",
      "epoch:535 batch:52 loss:0.716770 accu:0.000000 gene_loss:[validity:0.715688]\n",
      "epoch:536 batch:52 loss:0.716672 accu:0.000000 gene_loss:[validity:0.715904]\n",
      "epoch:537 batch:52 loss:0.716635 accu:0.000000 gene_loss:[validity:0.716130]\n",
      "epoch:538 batch:52 loss:0.716638 accu:0.000000 gene_loss:[validity:0.716283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:539 batch:52 loss:0.716631 accu:0.000000 gene_loss:[validity:0.716400]\n",
      "epoch:540 batch:52 loss:0.716632 accu:0.000000 gene_loss:[validity:0.716467]\n",
      "epoch:541 batch:52 loss:0.716625 accu:0.000000 gene_loss:[validity:0.716519]\n",
      "epoch:542 batch:52 loss:0.716619 accu:0.000000 gene_loss:[validity:0.716556]\n",
      "epoch:543 batch:52 loss:0.716611 accu:0.000000 gene_loss:[validity:0.716580]\n",
      "epoch:544 batch:52 loss:0.716601 accu:0.000000 gene_loss:[validity:0.716590]\n",
      "epoch:545 batch:52 loss:0.716592 accu:0.000000 gene_loss:[validity:0.716792]\n",
      "epoch:546 batch:52 loss:0.716739 accu:0.000000 gene_loss:[validity:0.716419]\n",
      "epoch:547 batch:52 loss:0.716746 accu:0.000000 gene_loss:[validity:0.716269]\n",
      "epoch:548 batch:52 loss:0.716671 accu:0.000000 gene_loss:[validity:0.716321]\n",
      "epoch:549 batch:52 loss:0.716642 accu:0.000000 gene_loss:[validity:0.716416]\n",
      "epoch:550 batch:52 loss:0.716629 accu:0.000000 gene_loss:[validity:0.716493]\n",
      "epoch:551 batch:52 loss:0.716618 accu:0.000000 gene_loss:[validity:0.716554]\n",
      "epoch:552 batch:52 loss:0.716617 accu:0.000000 gene_loss:[validity:0.716605]\n",
      "epoch:553 batch:52 loss:0.716597 accu:0.000000 gene_loss:[validity:0.716653]\n",
      "epoch:554 batch:52 loss:0.716594 accu:0.000000 gene_loss:[validity:0.716665]\n",
      "epoch:555 batch:52 loss:0.716586 accu:0.000000 gene_loss:[validity:0.716669]\n",
      "epoch:556 batch:52 loss:0.716577 accu:0.007812 gene_loss:[validity:0.716662]\n",
      "epoch:557 batch:52 loss:0.716566 accu:0.000000 gene_loss:[validity:0.716711]\n",
      "epoch:558 batch:52 loss:0.716546 accu:0.000000 gene_loss:[validity:0.716744]\n",
      "epoch:559 batch:52 loss:0.716553 accu:0.000000 gene_loss:[validity:0.716731]\n",
      "epoch:560 batch:52 loss:0.716547 accu:0.000000 gene_loss:[validity:0.716699]\n",
      "epoch:561 batch:52 loss:0.716538 accu:0.000000 gene_loss:[validity:0.716671]\n",
      "epoch:562 batch:52 loss:0.716530 accu:0.000000 gene_loss:[validity:0.716643]\n",
      "epoch:563 batch:52 loss:0.716519 accu:0.000000 gene_loss:[validity:0.716627]\n",
      "epoch:564 batch:52 loss:0.716509 accu:0.000000 gene_loss:[validity:0.716609]\n",
      "epoch:565 batch:52 loss:0.716501 accu:0.000000 gene_loss:[validity:0.716598]\n",
      "epoch:566 batch:52 loss:0.716493 accu:0.000000 gene_loss:[validity:0.716585]\n",
      "epoch:567 batch:52 loss:0.716487 accu:0.000000 gene_loss:[validity:0.716580]\n",
      "epoch:568 batch:52 loss:0.716481 accu:0.000000 gene_loss:[validity:0.716593]\n",
      "epoch:569 batch:52 loss:0.716474 accu:0.000000 gene_loss:[validity:0.716595]\n",
      "epoch:570 batch:52 loss:0.716473 accu:0.000000 gene_loss:[validity:0.716588]\n",
      "epoch:571 batch:52 loss:0.716471 accu:0.000000 gene_loss:[validity:0.716580]\n",
      "epoch:572 batch:52 loss:0.716468 accu:0.000000 gene_loss:[validity:0.716573]\n",
      "epoch:573 batch:52 loss:0.716467 accu:0.000000 gene_loss:[validity:0.716567]\n",
      "epoch:574 batch:52 loss:0.716464 accu:0.000000 gene_loss:[validity:0.716563]\n",
      "epoch:575 batch:52 loss:0.716463 accu:0.000000 gene_loss:[validity:0.716575]\n",
      "epoch:576 batch:52 loss:0.716460 accu:0.000000 gene_loss:[validity:0.716576]\n",
      "epoch:577 batch:52 loss:0.716462 accu:0.000000 gene_loss:[validity:0.716569]\n",
      "epoch:578 batch:52 loss:0.716461 accu:0.000000 gene_loss:[validity:0.716562]\n",
      "epoch:579 batch:52 loss:0.716460 accu:0.000000 gene_loss:[validity:0.716554]\n",
      "epoch:580 batch:52 loss:0.716457 accu:0.000000 gene_loss:[validity:0.716544]\n",
      "epoch:581 batch:52 loss:0.716456 accu:0.000000 gene_loss:[validity:0.716541]\n",
      "epoch:582 batch:52 loss:0.716452 accu:0.000000 gene_loss:[validity:0.716528]\n",
      "epoch:583 batch:52 loss:0.716450 accu:0.000000 gene_loss:[validity:0.716522]\n",
      "epoch:584 batch:52 loss:0.716439 accu:0.000000 gene_loss:[validity:0.716574]\n",
      "epoch:585 batch:52 loss:0.716446 accu:0.000000 gene_loss:[validity:0.716587]\n",
      "epoch:586 batch:52 loss:0.716455 accu:0.000000 gene_loss:[validity:0.716564]\n",
      "epoch:587 batch:52 loss:0.716451 accu:0.000000 gene_loss:[validity:0.716537]\n",
      "epoch:588 batch:52 loss:0.716443 accu:0.000000 gene_loss:[validity:0.716506]\n",
      "epoch:589 batch:52 loss:0.716434 accu:0.000000 gene_loss:[validity:0.716484]\n",
      "epoch:590 batch:52 loss:0.716431 accu:0.000000 gene_loss:[validity:0.723897]\n",
      "epoch:591 batch:52 loss:0.717746 accu:0.000000 gene_loss:[validity:0.723574]\n",
      "epoch:592 batch:52 loss:0.717664 accu:0.000000 gene_loss:[validity:0.722194]\n",
      "epoch:593 batch:52 loss:0.717475 accu:0.000000 gene_loss:[validity:0.721151]\n",
      "epoch:594 batch:52 loss:0.717476 accu:0.000000 gene_loss:[validity:0.719819]\n",
      "epoch:595 batch:52 loss:0.717253 accu:0.000000 gene_loss:[validity:0.718905]\n",
      "epoch:596 batch:52 loss:0.717096 accu:0.000000 gene_loss:[validity:0.718289]\n",
      "epoch:597 batch:52 loss:0.716997 accu:0.000000 gene_loss:[validity:0.717835]\n",
      "epoch:598 batch:52 loss:0.716915 accu:0.000000 gene_loss:[validity:0.717512]\n",
      "epoch:599 batch:52 loss:0.716849 accu:0.000000 gene_loss:[validity:0.717279]\n",
      "epoch:600 batch:52 loss:0.716794 accu:0.000000 gene_loss:[validity:0.717098]\n",
      "epoch:601 batch:52 loss:0.716745 accu:0.000000 gene_loss:[validity:0.716966]\n",
      "epoch:602 batch:52 loss:0.716702 accu:0.000000 gene_loss:[validity:0.716860]\n",
      "epoch:603 batch:52 loss:0.716663 accu:0.000000 gene_loss:[validity:0.716781]\n",
      "epoch:604 batch:52 loss:0.716626 accu:0.000000 gene_loss:[validity:0.716716]\n",
      "epoch:605 batch:52 loss:0.716598 accu:0.000000 gene_loss:[validity:0.716670]\n",
      "epoch:606 batch:52 loss:0.716568 accu:0.000000 gene_loss:[validity:0.716631]\n",
      "epoch:607 batch:52 loss:0.716543 accu:0.000000 gene_loss:[validity:0.716604]\n",
      "epoch:608 batch:52 loss:0.716527 accu:0.000000 gene_loss:[validity:0.716552]\n",
      "epoch:609 batch:52 loss:0.716505 accu:0.000000 gene_loss:[validity:0.716526]\n",
      "epoch:610 batch:52 loss:0.716480 accu:0.000000 gene_loss:[validity:0.716511]\n",
      "epoch:611 batch:52 loss:0.716462 accu:0.000000 gene_loss:[validity:0.716502]\n",
      "epoch:612 batch:52 loss:0.716443 accu:0.000000 gene_loss:[validity:0.716490]\n",
      "epoch:613 batch:52 loss:0.716428 accu:0.000000 gene_loss:[validity:0.716478]\n",
      "epoch:614 batch:52 loss:0.716412 accu:0.000000 gene_loss:[validity:0.716720]\n",
      "epoch:615 batch:52 loss:0.716334 accu:0.000000 gene_loss:[validity:0.716995]\n",
      "epoch:616 batch:52 loss:0.716471 accu:0.000000 gene_loss:[validity:0.716912]\n",
      "epoch:617 batch:52 loss:0.716481 accu:0.000000 gene_loss:[validity:0.716801]\n",
      "epoch:618 batch:52 loss:0.716456 accu:0.000000 gene_loss:[validity:0.716711]\n",
      "epoch:619 batch:52 loss:0.716430 accu:0.000000 gene_loss:[validity:0.716639]\n",
      "epoch:620 batch:52 loss:0.716409 accu:0.000000 gene_loss:[validity:0.716567]\n",
      "epoch:621 batch:52 loss:0.716391 accu:0.000000 gene_loss:[validity:0.716513]\n",
      "epoch:622 batch:52 loss:0.716375 accu:0.000000 gene_loss:[validity:0.716466]\n",
      "epoch:623 batch:52 loss:0.716364 accu:0.000000 gene_loss:[validity:0.716485]\n",
      "epoch:624 batch:52 loss:0.716348 accu:0.000000 gene_loss:[validity:0.716485]\n",
      "epoch:625 batch:52 loss:0.716349 accu:0.000000 gene_loss:[validity:0.716457]\n",
      "epoch:626 batch:52 loss:0.716339 accu:0.000000 gene_loss:[validity:0.716529]\n",
      "epoch:627 batch:52 loss:0.716358 accu:0.000000 gene_loss:[validity:0.716481]\n",
      "epoch:628 batch:52 loss:0.716345 accu:0.000000 gene_loss:[validity:0.716415]\n",
      "epoch:629 batch:52 loss:0.716333 accu:0.000000 gene_loss:[validity:0.716377]\n",
      "epoch:630 batch:52 loss:0.716367 accu:0.000000 gene_loss:[validity:0.716270]\n",
      "epoch:631 batch:52 loss:0.716365 accu:0.000000 gene_loss:[validity:0.716299]\n",
      "epoch:632 batch:52 loss:0.716367 accu:0.000000 gene_loss:[validity:0.716325]\n",
      "epoch:633 batch:52 loss:0.716368 accu:0.000000 gene_loss:[validity:0.716348]\n",
      "epoch:634 batch:52 loss:0.716364 accu:0.000000 gene_loss:[validity:0.716374]\n",
      "epoch:635 batch:52 loss:0.716364 accu:0.000000 gene_loss:[validity:0.716400]\n",
      "epoch:636 batch:52 loss:0.716367 accu:0.000000 gene_loss:[validity:0.716425]\n",
      "epoch:637 batch:52 loss:0.716377 accu:0.000000 gene_loss:[validity:0.716445]\n",
      "epoch:638 batch:52 loss:0.716385 accu:0.000000 gene_loss:[validity:0.716466]\n",
      "epoch:639 batch:52 loss:0.716408 accu:0.000000 gene_loss:[validity:0.716476]\n",
      "epoch:640 batch:52 loss:0.716403 accu:0.000000 gene_loss:[validity:0.716485]\n",
      "epoch:641 batch:52 loss:0.716410 accu:0.000000 gene_loss:[validity:0.716479]\n",
      "epoch:642 batch:52 loss:0.716409 accu:0.000000 gene_loss:[validity:0.716464]\n",
      "epoch:643 batch:52 loss:0.716405 accu:0.000000 gene_loss:[validity:0.716437]\n",
      "epoch:644 batch:52 loss:0.716395 accu:0.000000 gene_loss:[validity:0.716405]\n",
      "epoch:645 batch:52 loss:0.716377 accu:0.000000 gene_loss:[validity:0.716381]\n",
      "epoch:646 batch:52 loss:0.716361 accu:0.000000 gene_loss:[validity:0.716344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:647 batch:52 loss:0.716344 accu:0.000000 gene_loss:[validity:0.716330]\n",
      "epoch:648 batch:52 loss:0.716327 accu:0.000000 gene_loss:[validity:0.716300]\n",
      "epoch:649 batch:52 loss:0.720768 accu:0.000000 gene_loss:[validity:0.711636]\n",
      "epoch:650 batch:52 loss:0.717925 accu:0.000000 gene_loss:[validity:0.712065]\n",
      "epoch:651 batch:52 loss:0.717164 accu:0.000000 gene_loss:[validity:0.713250]\n",
      "epoch:652 batch:52 loss:0.716939 accu:0.000000 gene_loss:[validity:0.714229]\n",
      "epoch:653 batch:52 loss:0.716851 accu:0.000000 gene_loss:[validity:0.714960]\n",
      "epoch:654 batch:52 loss:0.716788 accu:0.000000 gene_loss:[validity:0.715509]\n",
      "epoch:655 batch:52 loss:0.716757 accu:0.000000 gene_loss:[validity:0.715879]\n",
      "epoch:656 batch:52 loss:0.716730 accu:0.000000 gene_loss:[validity:0.716121]\n",
      "epoch:657 batch:52 loss:0.716707 accu:0.000000 gene_loss:[validity:0.716273]\n",
      "epoch:658 batch:52 loss:0.716686 accu:0.000000 gene_loss:[validity:0.716369]\n",
      "epoch:659 batch:52 loss:0.716659 accu:0.000000 gene_loss:[validity:0.716434]\n",
      "epoch:660 batch:52 loss:0.716630 accu:0.000000 gene_loss:[validity:0.716467]\n",
      "epoch:661 batch:52 loss:0.716607 accu:0.000000 gene_loss:[validity:0.716491]\n",
      "epoch:662 batch:52 loss:0.716593 accu:0.000000 gene_loss:[validity:0.716490]\n",
      "epoch:663 batch:52 loss:0.716569 accu:0.000000 gene_loss:[validity:0.716506]\n",
      "epoch:664 batch:52 loss:0.716552 accu:0.000000 gene_loss:[validity:0.716505]\n",
      "epoch:665 batch:52 loss:0.716533 accu:0.000000 gene_loss:[validity:0.716499]\n",
      "epoch:666 batch:52 loss:0.716515 accu:0.000000 gene_loss:[validity:0.716489]\n",
      "epoch:667 batch:52 loss:0.716495 accu:0.000000 gene_loss:[validity:0.716475]\n",
      "epoch:668 batch:52 loss:0.716474 accu:0.000000 gene_loss:[validity:0.716459]\n",
      "epoch:669 batch:52 loss:0.716457 accu:0.000000 gene_loss:[validity:0.716443]\n",
      "epoch:670 batch:52 loss:0.716437 accu:0.000000 gene_loss:[validity:0.716426]\n",
      "epoch:671 batch:52 loss:0.716421 accu:0.000000 gene_loss:[validity:0.716410]\n",
      "epoch:672 batch:52 loss:0.716420 accu:0.000000 gene_loss:[validity:0.716384]\n",
      "epoch:673 batch:52 loss:0.716390 accu:0.000000 gene_loss:[validity:0.716382]\n",
      "epoch:674 batch:52 loss:0.716376 accu:0.000000 gene_loss:[validity:0.716369]\n",
      "epoch:675 batch:52 loss:0.716365 accu:0.000000 gene_loss:[validity:0.716365]\n",
      "epoch:676 batch:52 loss:0.716354 accu:0.000000 gene_loss:[validity:0.716354]\n",
      "epoch:677 batch:52 loss:0.716352 accu:0.000000 gene_loss:[validity:0.716344]\n",
      "epoch:678 batch:52 loss:0.716340 accu:0.000000 gene_loss:[validity:0.716332]\n",
      "epoch:679 batch:52 loss:0.716329 accu:0.000000 gene_loss:[validity:0.716352]\n",
      "epoch:680 batch:52 loss:0.716409 accu:0.000000 gene_loss:[validity:0.716146]\n",
      "epoch:681 batch:52 loss:0.716398 accu:0.000000 gene_loss:[validity:0.716308]\n",
      "epoch:682 batch:52 loss:0.716384 accu:0.000000 gene_loss:[validity:0.716426]\n",
      "epoch:683 batch:52 loss:0.716408 accu:0.000000 gene_loss:[validity:0.716478]\n",
      "epoch:684 batch:52 loss:0.716386 accu:0.000000 gene_loss:[validity:0.716470]\n",
      "epoch:685 batch:52 loss:0.716379 accu:0.000000 gene_loss:[validity:0.716442]\n",
      "epoch:686 batch:52 loss:0.716366 accu:0.000000 gene_loss:[validity:0.716399]\n",
      "epoch:687 batch:52 loss:0.716351 accu:0.000000 gene_loss:[validity:0.716370]\n",
      "epoch:688 batch:52 loss:0.716337 accu:0.000000 gene_loss:[validity:0.716340]\n",
      "epoch:689 batch:52 loss:0.716323 accu:0.000000 gene_loss:[validity:0.716321]\n",
      "epoch:690 batch:52 loss:0.716309 accu:0.000000 gene_loss:[validity:0.716319]\n",
      "epoch:691 batch:52 loss:0.716378 accu:0.000000 gene_loss:[validity:0.716101]\n",
      "epoch:692 batch:52 loss:0.716341 accu:0.000000 gene_loss:[validity:0.716054]\n",
      "epoch:693 batch:52 loss:0.716308 accu:0.000000 gene_loss:[validity:0.716084]\n",
      "epoch:694 batch:52 loss:0.716286 accu:0.000000 gene_loss:[validity:0.716125]\n",
      "epoch:695 batch:52 loss:0.716262 accu:0.000000 gene_loss:[validity:0.716285]\n",
      "epoch:696 batch:52 loss:0.716317 accu:0.000000 gene_loss:[validity:0.716218]\n",
      "epoch:697 batch:52 loss:0.716277 accu:0.000000 gene_loss:[validity:0.716204]\n",
      "epoch:698 batch:52 loss:0.716253 accu:0.000000 gene_loss:[validity:0.716201]\n",
      "epoch:699 batch:52 loss:0.716239 accu:0.000000 gene_loss:[validity:0.716200]\n",
      "epoch:700 batch:52 loss:0.716226 accu:0.000000 gene_loss:[validity:0.716195]\n",
      "epoch:701 batch:52 loss:0.716214 accu:0.000000 gene_loss:[validity:0.716193]\n",
      "epoch:702 batch:52 loss:0.716204 accu:0.000000 gene_loss:[validity:0.716187]\n",
      "epoch:703 batch:52 loss:0.716194 accu:0.000000 gene_loss:[validity:0.716183]\n",
      "epoch:704 batch:52 loss:0.716187 accu:0.000000 gene_loss:[validity:0.716182]\n",
      "epoch:705 batch:52 loss:0.716181 accu:0.000000 gene_loss:[validity:0.716181]\n",
      "epoch:706 batch:52 loss:0.716179 accu:0.000000 gene_loss:[validity:0.716176]\n",
      "epoch:707 batch:52 loss:0.716172 accu:0.000000 gene_loss:[validity:0.716175]\n",
      "epoch:708 batch:52 loss:0.716169 accu:0.000000 gene_loss:[validity:0.716170]\n",
      "epoch:709 batch:52 loss:0.716167 accu:0.000000 gene_loss:[validity:0.716167]\n",
      "epoch:710 batch:52 loss:0.716165 accu:0.000000 gene_loss:[validity:0.716166]\n",
      "epoch:711 batch:52 loss:0.716165 accu:0.000000 gene_loss:[validity:0.716168]\n",
      "epoch:712 batch:52 loss:0.716165 accu:0.000000 gene_loss:[validity:0.716166]\n",
      "epoch:713 batch:52 loss:0.716167 accu:0.000000 gene_loss:[validity:0.716166]\n",
      "epoch:714 batch:52 loss:0.716167 accu:0.000000 gene_loss:[validity:0.716169]\n",
      "epoch:715 batch:52 loss:0.716173 accu:0.000000 gene_loss:[validity:0.716176]\n",
      "epoch:716 batch:52 loss:0.716172 accu:0.000000 gene_loss:[validity:0.716177]\n",
      "epoch:717 batch:52 loss:0.716176 accu:0.000000 gene_loss:[validity:0.716184]\n",
      "epoch:718 batch:52 loss:0.716173 accu:0.000000 gene_loss:[validity:0.716185]\n",
      "epoch:719 batch:52 loss:0.716176 accu:0.000000 gene_loss:[validity:0.716191]\n",
      "epoch:720 batch:52 loss:0.716172 accu:0.000000 gene_loss:[validity:0.716623]\n",
      "epoch:721 batch:52 loss:0.716996 accu:0.000000 gene_loss:[validity:0.714765]\n",
      "epoch:722 batch:52 loss:0.716522 accu:0.000000 gene_loss:[validity:0.714599]\n",
      "epoch:723 batch:52 loss:0.716360 accu:0.000000 gene_loss:[validity:0.714875]\n",
      "epoch:724 batch:52 loss:0.716282 accu:0.000000 gene_loss:[validity:0.715232]\n",
      "epoch:725 batch:52 loss:0.716241 accu:0.000000 gene_loss:[validity:0.715606]\n",
      "epoch:726 batch:52 loss:0.716243 accu:0.000000 gene_loss:[validity:0.715878]\n",
      "epoch:727 batch:52 loss:0.716252 accu:0.000000 gene_loss:[validity:0.716071]\n",
      "epoch:728 batch:52 loss:0.716259 accu:0.000000 gene_loss:[validity:0.716199]\n",
      "epoch:729 batch:52 loss:0.716245 accu:0.000000 gene_loss:[validity:0.718748]\n",
      "epoch:730 batch:52 loss:0.716976 accu:0.000000 gene_loss:[validity:0.718714]\n",
      "epoch:731 batch:52 loss:0.717036 accu:0.000000 gene_loss:[validity:0.718165]\n",
      "epoch:732 batch:52 loss:0.716940 accu:0.000000 gene_loss:[validity:0.717641]\n",
      "epoch:733 batch:52 loss:0.716831 accu:0.000000 gene_loss:[validity:0.717252]\n",
      "epoch:734 batch:52 loss:0.716736 accu:0.000000 gene_loss:[validity:0.716978]\n",
      "epoch:735 batch:52 loss:0.716641 accu:0.000000 gene_loss:[validity:0.717085]\n",
      "epoch:736 batch:52 loss:0.716737 accu:0.000000 gene_loss:[validity:0.716753]\n",
      "epoch:737 batch:52 loss:0.716624 accu:0.000000 gene_loss:[validity:0.716593]\n",
      "epoch:738 batch:52 loss:0.716550 accu:0.000000 gene_loss:[validity:0.716512]\n",
      "epoch:739 batch:52 loss:0.716509 accu:0.000000 gene_loss:[validity:0.716460]\n",
      "epoch:740 batch:52 loss:0.716471 accu:0.000000 gene_loss:[validity:0.716416]\n",
      "epoch:741 batch:52 loss:0.716438 accu:0.000000 gene_loss:[validity:0.716384]\n",
      "epoch:742 batch:52 loss:0.716405 accu:0.000000 gene_loss:[validity:0.716351]\n",
      "epoch:743 batch:52 loss:0.716373 accu:0.000000 gene_loss:[validity:0.716325]\n",
      "epoch:744 batch:52 loss:0.716344 accu:0.000000 gene_loss:[validity:0.716290]\n",
      "epoch:745 batch:52 loss:0.716316 accu:0.000000 gene_loss:[validity:0.716266]\n",
      "epoch:746 batch:52 loss:0.716288 accu:0.000000 gene_loss:[validity:0.716237]\n",
      "epoch:747 batch:52 loss:0.716263 accu:0.000000 gene_loss:[validity:0.716229]\n",
      "epoch:748 batch:52 loss:0.716240 accu:0.000000 gene_loss:[validity:0.716198]\n",
      "epoch:749 batch:52 loss:0.716221 accu:0.000000 gene_loss:[validity:0.716183]\n",
      "epoch:750 batch:52 loss:0.716199 accu:0.000000 gene_loss:[validity:0.716166]\n",
      "epoch:751 batch:52 loss:0.716179 accu:0.000000 gene_loss:[validity:0.716156]\n",
      "epoch:752 batch:52 loss:0.716164 accu:0.000000 gene_loss:[validity:0.716142]\n",
      "epoch:753 batch:52 loss:0.716148 accu:0.000000 gene_loss:[validity:0.716137]\n",
      "epoch:754 batch:52 loss:0.716134 accu:0.000000 gene_loss:[validity:0.716123]\n"
     ]
    }
   ],
   "source": [
    "real_labels = np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(1001):\n",
    "    #for j in range(int(IMAGES_COUNT/BATCH_SIZE)):\n",
    "        \n",
    "    noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    corresponding_fake_label = np.random.randint(low=0 , high=CLASS_NUM , size=(BATCH_SIZE , 1)) #label的取值范围 可能会发生变化\n",
    "\n",
    "    real_image , corresponding_real_label = load_image()\n",
    "\n",
    "    #训练判别器\n",
    "    fake_image = generator_i.predict([noise , corresponding_fake_label])\n",
    "\n",
    "    real_loss = discriminator_i.train_on_batch([real_image , corresponding_real_label] , real_labels)\n",
    "    fake_loss = discriminator_i.train_on_batch([fake_image , corresponding_fake_label] , fake_labels) #应该是real还是fake\n",
    "\n",
    "    loss = np.add(real_loss , fake_loss)/2\n",
    "\n",
    "    #训练生成器\n",
    "    noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    corresponding_fake_label2 = np.random.randint(low=0 , high=CLASS_NUM , size=(BATCH_SIZE , 1))\n",
    "\n",
    "        #下面的损失是一个list 有两个损失 一个是validity一个是与label的softmax\n",
    "    generator_loss = combined_model_i.train_on_batch([noise2 , corresponding_fake_label2] , real_labels)\n",
    "\n",
    "    print('epoch:%d batch:%d loss:%f accu:%f gene_loss:[validity:%f]' % (i , 52 , loss[0] , loss[1] , generator_loss))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        write_image(i)\n",
    "    \n",
    "write_image(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
