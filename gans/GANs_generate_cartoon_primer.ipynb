{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense , Conv2D , MaxPool2D , BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 96\n",
    "HEIGHT = 96\n",
    "CHANNEL = 3\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = 'faces/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(X_train , y_train),(X_test , y_test) = mnist.load_data()\\nX_train = X_train/127.5-1\\nX_train = np.expand_dims(X_train , 3)\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(X_train , y_train),(X_test , y_test) = mnist.load_data()\n",
    "X_train = X_train/127.5-1\n",
    "X_train = np.expand_dims(X_train , 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_mnist():\\n    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\\n    \\ndef write_image_mnist(epoch):\\n    \\n    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\\n    generated_image = generator_i.predict(noise)\\n    generated_image = generated_image*0.5+0.5\\n    \\n    fig , axes = plt.pyplot.subplots(ROW , COL)\\n    \\n    count=0\\n    \\n    for i in range(ROW):\\n        for j in range(COL):\\n            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\\n            axes[i][j].axis('off')\\n            count += 1\\n            \\n    fig.savefig('images/No.%d.png' % epoch)\\n    plt.pyplot.close()\\n\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_mnist():\n",
    "    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\n",
    "    \n",
    "def write_image_mnist(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = generated_image*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('images/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('generated_faces/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    #sample from noise z\n",
    "    model = Sequential(name='generator')\n",
    "    \n",
    "    model.add(Dense(units = 256 , input_dim = LATENT_DIM , name='dense1'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8 , name='batchnorm1'))\n",
    "    model.add(Dense(units = 512 , name='dense2'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8 , name='batchnorm2'))\n",
    "    model.add(Dense(units = 1024 , name='dense3'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8 , name='batchnorm3'))\n",
    "    #model.add(Dense(units = 5000 , activation='relu' , name='dense4'))\n",
    "    #model.add(BatchNormalization(name='batchnorm4'))\n",
    "    #model.add(Dense(units = 10000 , activation='relu' , name='dense5'))\n",
    "    #model.add(BatchNormalization(name='batchnorm5'))\n",
    "    #model.add(Dense(units = 20000 , activation='relu' , name='dense6'))\n",
    "    #model.add(BatchNormalization(name='batchnorm6'))\n",
    "    model.add(Dense(units = WIDTH*HEIGHT*CHANNEL , activation='tanh' , name='dense7'))\n",
    "    model.add(Reshape(target_shape=(WIDTH , HEIGHT , CHANNEL) , name='reshape1'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    noise = Input(shape=(LATENT_DIM , ) , name='input1')\n",
    "    image = model(noise)\n",
    "    \n",
    "    return Model(noise , image , name='generator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='discriminator')\n",
    "    \n",
    "    model.add(Flatten(input_shape = (WIDTH,HEIGHT,CHANNEL) , name='flatten1'))\n",
    "    #model.add(Dense(units=10000 , name='dense8'))\n",
    "    #model.add(LeakyReLU(0.3 , name='leakyrelu1'))\n",
    "    #model.add(Dense(units=5000 , name='dense9'))\n",
    "    #model.add(LeakyReLU(0.3 , name='leakyrelu2'))\n",
    "    #model.add(Dense(units=1024 , name='dense10'))\n",
    "    #model.add(LeakyReLU(0.3 , name='leakyrelu3'))\n",
    "    model.add(Dense(units=512 , name='dense11'))\n",
    "    model.add(LeakyReLU(0.2 , name='leakyrelu4'))\n",
    "    model.add(Dense(units=256 , name='dense12'))\n",
    "    model.add(LeakyReLU(0.2 , name='leakyrelu5'))\n",
    "    #model.add(Dense(units=32 , name='dense13'))\n",
    "    #model.add(LeakyReLU(0.3 , name='leakyrelu6'))\n",
    "    #model.add(Dense(units=4 , name='dense14'))\n",
    "    #model.add(LeakyReLU(0.3 , name='leakyrelu7'))\n",
    "    model.add(Dense(units=1 , activation='sigmoid' , name='dense15'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    image = Input(shape=(WIDTH , HEIGHT , CHANNEL) , name='input1')\n",
    "    validity = model(image)\n",
    "    \n",
    "    return Model(image , validity , name='discriminator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_model(generator_i , discriminator_i):\n",
    "    #生成器和判别器组合成整体\n",
    "    z = Input(shape=(LATENT_DIM , ) , name='z')\n",
    "    \n",
    "    image = generator_i(z)\n",
    "    discriminator_i.trainable = False\n",
    "    validity = discriminator_i(image)\n",
    "    \n",
    "    return Model(z , validity , name='combined_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_i = discriminator()\n",
    "discriminator_i.compile(optimizer=Adam(lr=0.0002 , beta_1=0.5) , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "generator_i = generator()\n",
    "\n",
    "combined_model_i = combined_model(generator_i , discriminator_i)\n",
    "\n",
    "\n",
    "combined_model_i.compile(optimizer=Adam(lr=0.0002 , beta_1=0.5) , loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:0.740570 accu:0.296875 gene_loss:0.702766\n",
      "epoch:1 loss:0.537559 accu:0.542969 gene_loss:0.555938\n",
      "epoch:2 loss:0.675689 accu:0.523438 gene_loss:0.530261\n",
      "epoch:3 loss:0.672871 accu:0.542969 gene_loss:0.534743\n",
      "epoch:4 loss:0.695099 accu:0.546875 gene_loss:0.521769\n",
      "epoch:5 loss:0.628405 accu:0.574219 gene_loss:0.590071\n",
      "epoch:6 loss:0.575869 accu:0.609375 gene_loss:0.690930\n",
      "epoch:7 loss:0.571797 accu:0.640625 gene_loss:0.758851\n",
      "epoch:8 loss:0.533622 accu:0.636719 gene_loss:0.871782\n",
      "epoch:9 loss:0.452216 accu:0.687500 gene_loss:0.987085\n",
      "epoch:10 loss:0.450965 accu:0.710938 gene_loss:1.161847\n",
      "epoch:11 loss:0.347822 accu:0.785156 gene_loss:1.293101\n",
      "epoch:12 loss:0.311907 accu:0.828125 gene_loss:1.445907\n",
      "epoch:13 loss:0.312792 accu:0.808594 gene_loss:1.497506\n",
      "epoch:14 loss:0.247289 accu:0.890625 gene_loss:1.524854\n",
      "epoch:15 loss:0.225497 accu:0.898438 gene_loss:1.610508\n",
      "epoch:16 loss:0.248967 accu:0.917969 gene_loss:1.653491\n",
      "epoch:17 loss:0.350959 accu:0.832031 gene_loss:1.715170\n",
      "epoch:18 loss:0.864843 accu:0.765625 gene_loss:1.758370\n",
      "epoch:19 loss:0.590014 accu:0.816406 gene_loss:1.596723\n",
      "epoch:20 loss:0.283686 accu:0.839844 gene_loss:1.625718\n",
      "epoch:21 loss:0.250717 accu:0.863281 gene_loss:1.730332\n",
      "epoch:22 loss:0.281550 accu:0.898438 gene_loss:1.822147\n",
      "epoch:23 loss:0.245882 accu:0.886719 gene_loss:1.907446\n",
      "epoch:24 loss:0.166537 accu:0.953125 gene_loss:1.919928\n",
      "epoch:25 loss:0.142421 accu:0.964844 gene_loss:1.987817\n",
      "epoch:26 loss:0.159033 accu:0.960938 gene_loss:1.932465\n",
      "epoch:27 loss:0.154906 accu:0.972656 gene_loss:2.041194\n",
      "epoch:28 loss:0.131791 accu:0.984375 gene_loss:2.078148\n",
      "epoch:29 loss:0.117764 accu:0.996094 gene_loss:2.129303\n",
      "epoch:30 loss:0.105733 accu:0.992188 gene_loss:2.076170\n",
      "epoch:31 loss:0.110689 accu:1.000000 gene_loss:2.231359\n",
      "epoch:32 loss:0.140512 accu:0.988281 gene_loss:2.264597\n",
      "epoch:33 loss:0.250369 accu:0.890625 gene_loss:2.289072\n",
      "epoch:34 loss:0.318657 accu:0.867188 gene_loss:2.091295\n",
      "epoch:35 loss:0.347500 accu:0.925781 gene_loss:2.138138\n",
      "epoch:36 loss:0.151069 accu:0.949219 gene_loss:2.151605\n",
      "epoch:37 loss:0.161437 accu:0.960938 gene_loss:2.207284\n",
      "epoch:38 loss:0.145881 accu:0.957031 gene_loss:2.206959\n",
      "epoch:39 loss:0.157196 accu:0.964844 gene_loss:2.266388\n",
      "epoch:40 loss:0.118159 accu:0.992188 gene_loss:2.256573\n",
      "epoch:41 loss:0.103993 accu:0.988281 gene_loss:2.309676\n",
      "epoch:42 loss:0.107321 accu:0.988281 gene_loss:2.273418\n",
      "epoch:43 loss:0.088079 accu:1.000000 gene_loss:2.327412\n",
      "epoch:44 loss:0.102189 accu:0.996094 gene_loss:2.343614\n",
      "epoch:45 loss:0.086365 accu:1.000000 gene_loss:2.379870\n",
      "epoch:46 loss:0.113662 accu:0.992188 gene_loss:2.363482\n",
      "epoch:47 loss:0.716995 accu:0.742188 gene_loss:3.774450\n",
      "epoch:48 loss:1.231302 accu:0.792969 gene_loss:2.778650\n",
      "epoch:49 loss:0.770908 accu:0.765625 gene_loss:2.700695\n",
      "epoch:50 loss:0.436203 accu:0.835938 gene_loss:2.699767\n",
      "epoch:51 loss:0.412875 accu:0.847656 gene_loss:2.555344\n",
      "epoch:52 loss:0.256912 accu:0.898438 gene_loss:2.523808\n",
      "epoch:53 loss:0.385661 accu:0.902344 gene_loss:2.279719\n",
      "epoch:54 loss:0.164618 accu:0.945312 gene_loss:2.248582\n",
      "epoch:55 loss:0.232218 accu:0.917969 gene_loss:2.338080\n",
      "epoch:56 loss:0.149853 accu:0.957031 gene_loss:2.386563\n",
      "epoch:57 loss:0.133829 accu:0.960938 gene_loss:2.169937\n",
      "epoch:58 loss:0.123300 accu:0.972656 gene_loss:2.262783\n",
      "epoch:59 loss:0.124667 accu:0.984375 gene_loss:2.276811\n",
      "epoch:60 loss:0.150175 accu:0.968750 gene_loss:2.177094\n",
      "epoch:61 loss:0.230906 accu:0.957031 gene_loss:2.058997\n",
      "epoch:62 loss:0.134860 accu:0.968750 gene_loss:2.159669\n",
      "epoch:63 loss:0.188469 accu:0.988281 gene_loss:2.214422\n",
      "epoch:64 loss:0.122591 accu:0.984375 gene_loss:2.238246\n",
      "epoch:65 loss:0.244702 accu:0.957031 gene_loss:2.106225\n",
      "epoch:66 loss:0.226350 accu:0.945312 gene_loss:2.254514\n",
      "epoch:67 loss:0.276623 accu:0.902344 gene_loss:2.230715\n",
      "epoch:68 loss:0.413544 accu:0.843750 gene_loss:2.380834\n",
      "epoch:69 loss:0.223967 accu:0.902344 gene_loss:2.429627\n",
      "epoch:70 loss:0.178341 accu:0.925781 gene_loss:2.412287\n",
      "epoch:71 loss:0.200596 accu:0.925781 gene_loss:2.403749\n",
      "epoch:72 loss:0.211737 accu:0.875000 gene_loss:2.504640\n",
      "epoch:73 loss:0.145794 accu:0.945312 gene_loss:2.553648\n",
      "epoch:74 loss:0.154285 accu:0.953125 gene_loss:2.571345\n",
      "epoch:75 loss:0.135932 accu:0.972656 gene_loss:2.292032\n",
      "epoch:76 loss:0.182607 accu:0.925781 gene_loss:2.300291\n",
      "epoch:77 loss:0.139190 accu:0.941406 gene_loss:2.455941\n",
      "epoch:78 loss:0.206964 accu:0.906250 gene_loss:2.536659\n",
      "epoch:79 loss:0.513819 accu:0.792969 gene_loss:4.182251\n",
      "epoch:80 loss:0.847381 accu:0.781250 gene_loss:2.238962\n",
      "epoch:81 loss:0.286542 accu:0.851562 gene_loss:2.229156\n",
      "epoch:82 loss:0.319669 accu:0.910156 gene_loss:2.212899\n",
      "epoch:83 loss:0.345520 accu:0.914062 gene_loss:2.028290\n",
      "epoch:84 loss:0.206122 accu:0.937500 gene_loss:2.120954\n",
      "epoch:85 loss:0.181432 accu:0.929688 gene_loss:2.198410\n",
      "epoch:86 loss:0.184075 accu:0.937500 gene_loss:2.077380\n",
      "epoch:87 loss:0.167928 accu:0.953125 gene_loss:2.200196\n",
      "epoch:88 loss:0.143617 accu:0.960938 gene_loss:2.157156\n",
      "epoch:89 loss:0.207574 accu:0.921875 gene_loss:2.097624\n",
      "epoch:90 loss:0.155385 accu:0.957031 gene_loss:2.128762\n",
      "epoch:91 loss:0.218316 accu:0.953125 gene_loss:2.068949\n",
      "epoch:92 loss:0.220494 accu:0.921875 gene_loss:2.099274\n",
      "epoch:93 loss:0.139428 accu:0.957031 gene_loss:2.215117\n",
      "epoch:94 loss:0.336685 accu:0.867188 gene_loss:2.542424\n",
      "epoch:95 loss:0.903597 accu:0.734375 gene_loss:1.839972\n",
      "epoch:96 loss:0.424392 accu:0.773438 gene_loss:1.942544\n",
      "epoch:97 loss:1.212632 accu:0.644531 gene_loss:3.136277\n",
      "epoch:98 loss:0.687272 accu:0.742188 gene_loss:1.952475\n",
      "epoch:99 loss:0.439569 accu:0.785156 gene_loss:1.992624\n",
      "epoch:100 loss:0.448493 accu:0.789062 gene_loss:1.901632\n",
      "epoch:101 loss:0.390395 accu:0.898438 gene_loss:1.918479\n",
      "epoch:102 loss:0.331213 accu:0.871094 gene_loss:1.930949\n",
      "epoch:103 loss:0.683061 accu:0.621094 gene_loss:1.163505\n",
      "epoch:104 loss:0.443780 accu:0.773438 gene_loss:1.090852\n",
      "epoch:105 loss:0.602041 accu:0.738281 gene_loss:1.357484\n",
      "epoch:106 loss:0.305996 accu:0.851562 gene_loss:1.596164\n",
      "epoch:107 loss:0.287765 accu:0.859375 gene_loss:1.779595\n",
      "epoch:108 loss:0.343329 accu:0.878906 gene_loss:1.564709\n",
      "epoch:109 loss:0.248378 accu:0.882812 gene_loss:1.801777\n",
      "epoch:110 loss:0.525894 accu:0.828125 gene_loss:1.804194\n",
      "epoch:111 loss:0.292594 accu:0.855469 gene_loss:1.819172\n",
      "epoch:112 loss:0.403265 accu:0.863281 gene_loss:1.999189\n",
      "epoch:113 loss:0.230426 accu:0.902344 gene_loss:2.059745\n",
      "epoch:114 loss:1.180997 accu:0.675781 gene_loss:3.337803\n",
      "epoch:115 loss:1.674665 accu:0.640625 gene_loss:1.241700\n",
      "epoch:116 loss:0.765208 accu:0.703125 gene_loss:1.600103\n",
      "epoch:117 loss:0.537964 accu:0.710938 gene_loss:1.467065\n",
      "epoch:118 loss:0.491249 accu:0.800781 gene_loss:1.523440\n",
      "epoch:119 loss:0.615229 accu:0.757812 gene_loss:1.323722\n",
      "epoch:120 loss:0.502871 accu:0.785156 gene_loss:1.256372\n",
      "epoch:121 loss:0.396898 accu:0.796875 gene_loss:1.223244\n",
      "epoch:122 loss:0.329884 accu:0.808594 gene_loss:1.370824\n",
      "epoch:123 loss:0.385638 accu:0.832031 gene_loss:1.369589\n",
      "epoch:124 loss:0.300211 accu:0.867188 gene_loss:1.456269\n",
      "epoch:125 loss:0.266095 accu:0.886719 gene_loss:1.643776\n",
      "epoch:126 loss:0.500479 accu:0.808594 gene_loss:1.103678\n",
      "epoch:127 loss:0.317026 accu:0.824219 gene_loss:1.355107\n",
      "epoch:128 loss:0.660182 accu:0.742188 gene_loss:0.996716\n",
      "epoch:129 loss:0.429358 accu:0.773438 gene_loss:1.358411\n",
      "epoch:130 loss:0.498582 accu:0.773438 gene_loss:1.380846\n",
      "epoch:131 loss:0.862902 accu:0.617188 gene_loss:1.207380\n",
      "epoch:132 loss:0.566188 accu:0.753906 gene_loss:1.274331\n",
      "epoch:133 loss:0.777861 accu:0.664062 gene_loss:0.996703\n",
      "epoch:134 loss:0.507109 accu:0.812500 gene_loss:1.230314\n",
      "epoch:135 loss:0.297448 accu:0.867188 gene_loss:1.564124\n",
      "epoch:136 loss:0.405372 accu:0.871094 gene_loss:1.312101\n",
      "epoch:137 loss:0.652199 accu:0.714844 gene_loss:1.096038\n",
      "epoch:138 loss:0.652401 accu:0.703125 gene_loss:0.981524\n",
      "epoch:139 loss:0.559119 accu:0.765625 gene_loss:1.128573\n",
      "epoch:140 loss:0.388961 accu:0.785156 gene_loss:1.243910\n",
      "epoch:141 loss:0.330306 accu:0.863281 gene_loss:1.723649\n",
      "epoch:142 loss:0.539364 accu:0.839844 gene_loss:1.263428\n",
      "epoch:143 loss:0.627764 accu:0.710938 gene_loss:0.953518\n",
      "epoch:144 loss:0.406314 accu:0.824219 gene_loss:1.355910\n",
      "epoch:145 loss:0.394025 accu:0.820312 gene_loss:1.470912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:146 loss:0.468977 accu:0.777344 gene_loss:1.160611\n",
      "epoch:147 loss:0.534357 accu:0.714844 gene_loss:1.106809\n",
      "epoch:148 loss:0.384121 accu:0.847656 gene_loss:1.480887\n",
      "epoch:149 loss:1.121359 accu:0.613281 gene_loss:1.947453\n",
      "epoch:150 loss:1.055746 accu:0.605469 gene_loss:0.664682\n",
      "epoch:151 loss:0.876746 accu:0.570312 gene_loss:0.834355\n",
      "epoch:152 loss:0.494320 accu:0.773438 gene_loss:1.414364\n",
      "epoch:153 loss:0.804344 accu:0.683594 gene_loss:1.070187\n",
      "epoch:154 loss:1.082852 accu:0.539062 gene_loss:0.479629\n",
      "epoch:155 loss:0.643883 accu:0.628906 gene_loss:0.808145\n",
      "epoch:156 loss:0.337604 accu:0.800781 gene_loss:1.230205\n",
      "epoch:157 loss:0.359933 accu:0.902344 gene_loss:1.478685\n",
      "epoch:158 loss:0.511833 accu:0.738281 gene_loss:0.845462\n",
      "epoch:159 loss:0.536793 accu:0.683594 gene_loss:0.757973\n",
      "epoch:160 loss:0.410480 accu:0.789062 gene_loss:1.036186\n",
      "epoch:161 loss:0.386410 accu:0.796875 gene_loss:1.119257\n",
      "epoch:162 loss:0.307988 accu:0.886719 gene_loss:1.232435\n",
      "epoch:163 loss:1.124411 accu:0.601562 gene_loss:1.040897\n",
      "epoch:164 loss:1.355302 accu:0.500000 gene_loss:0.366900\n",
      "epoch:165 loss:0.763351 accu:0.578125 gene_loss:0.586717\n",
      "epoch:166 loss:0.402438 accu:0.718750 gene_loss:1.386279\n",
      "epoch:167 loss:0.568890 accu:0.824219 gene_loss:1.370868\n",
      "epoch:168 loss:1.118602 accu:0.593750 gene_loss:0.782871\n",
      "epoch:169 loss:0.907700 accu:0.652344 gene_loss:0.607430\n",
      "epoch:170 loss:0.683918 accu:0.703125 gene_loss:0.848682\n",
      "epoch:171 loss:0.432450 accu:0.773438 gene_loss:1.058621\n",
      "epoch:172 loss:0.348842 accu:0.851562 gene_loss:1.203270\n",
      "epoch:173 loss:0.375528 accu:0.835938 gene_loss:1.214555\n",
      "epoch:174 loss:0.374376 accu:0.812500 gene_loss:1.008729\n",
      "epoch:175 loss:0.635237 accu:0.687500 gene_loss:0.916555\n",
      "epoch:176 loss:0.627089 accu:0.691406 gene_loss:0.708802\n",
      "epoch:177 loss:0.440663 accu:0.742188 gene_loss:1.189746\n",
      "epoch:178 loss:0.474206 accu:0.781250 gene_loss:1.177289\n",
      "epoch:179 loss:1.577029 accu:0.492188 gene_loss:1.052293\n",
      "epoch:180 loss:1.492949 accu:0.597656 gene_loss:0.547626\n",
      "epoch:181 loss:1.023664 accu:0.554688 gene_loss:0.525508\n",
      "epoch:182 loss:0.535887 accu:0.675781 gene_loss:0.964729\n",
      "epoch:183 loss:0.685968 accu:0.730469 gene_loss:1.211411\n",
      "epoch:184 loss:0.444815 accu:0.855469 gene_loss:1.242883\n",
      "epoch:185 loss:0.937792 accu:0.578125 gene_loss:0.749002\n",
      "epoch:186 loss:1.072342 accu:0.667969 gene_loss:0.713200\n",
      "epoch:187 loss:1.004803 accu:0.656250 gene_loss:0.669789\n",
      "epoch:188 loss:0.595679 accu:0.722656 gene_loss:0.835824\n",
      "epoch:189 loss:0.455994 accu:0.773438 gene_loss:1.277749\n",
      "epoch:190 loss:0.375440 accu:0.847656 gene_loss:1.341782\n",
      "epoch:191 loss:0.327383 accu:0.890625 gene_loss:1.219397\n",
      "epoch:192 loss:0.558153 accu:0.671875 gene_loss:0.885527\n",
      "epoch:193 loss:0.542792 accu:0.726562 gene_loss:0.917698\n",
      "epoch:194 loss:0.390376 accu:0.804688 gene_loss:1.243540\n",
      "epoch:195 loss:1.065841 accu:0.601562 gene_loss:0.715117\n",
      "epoch:196 loss:0.961064 accu:0.632812 gene_loss:0.556780\n",
      "epoch:197 loss:0.780529 accu:0.636719 gene_loss:0.761900\n",
      "epoch:198 loss:0.407527 accu:0.757812 gene_loss:1.244497\n",
      "epoch:199 loss:0.614047 accu:0.761719 gene_loss:1.255376\n",
      "epoch:200 loss:1.116310 accu:0.519531 gene_loss:0.691769\n",
      "epoch:201 loss:0.778953 accu:0.707031 gene_loss:0.909046\n",
      "epoch:202 loss:0.774947 accu:0.531250 gene_loss:0.588161\n",
      "epoch:203 loss:0.467363 accu:0.652344 gene_loss:1.051148\n",
      "epoch:204 loss:0.302155 accu:0.859375 gene_loss:1.484590\n",
      "epoch:205 loss:0.539146 accu:0.734375 gene_loss:0.911949\n",
      "epoch:206 loss:0.378211 accu:0.789062 gene_loss:1.013757\n",
      "epoch:207 loss:0.524212 accu:0.746094 gene_loss:1.128050\n",
      "epoch:208 loss:1.164730 accu:0.585938 gene_loss:1.017880\n",
      "epoch:209 loss:1.031882 accu:0.597656 gene_loss:0.471912\n",
      "epoch:210 loss:0.673604 accu:0.593750 gene_loss:0.794793\n",
      "epoch:211 loss:0.345044 accu:0.777344 gene_loss:1.521767\n",
      "epoch:212 loss:0.866917 accu:0.632812 gene_loss:0.619573\n",
      "epoch:213 loss:0.630789 accu:0.671875 gene_loss:0.828913\n",
      "epoch:214 loss:0.488692 accu:0.761719 gene_loss:1.016946\n",
      "epoch:215 loss:0.604376 accu:0.699219 gene_loss:0.991721\n",
      "epoch:216 loss:0.546547 accu:0.714844 gene_loss:0.903103\n",
      "epoch:217 loss:0.374586 accu:0.789062 gene_loss:1.228619\n",
      "epoch:218 loss:0.592028 accu:0.738281 gene_loss:0.879221\n",
      "epoch:219 loss:0.524872 accu:0.671875 gene_loss:0.863584\n",
      "epoch:220 loss:0.422178 accu:0.757812 gene_loss:1.179890\n",
      "epoch:221 loss:0.952193 accu:0.574219 gene_loss:0.531278\n",
      "epoch:222 loss:0.668196 accu:0.625000 gene_loss:0.702701\n",
      "epoch:223 loss:0.388284 accu:0.734375 gene_loss:1.244237\n",
      "epoch:224 loss:0.576271 accu:0.675781 gene_loss:0.781748\n",
      "epoch:225 loss:0.444530 accu:0.726562 gene_loss:1.288160\n",
      "epoch:226 loss:0.318611 accu:0.917969 gene_loss:1.459790\n",
      "epoch:227 loss:0.944372 accu:0.640625 gene_loss:0.767316\n",
      "epoch:228 loss:1.010640 accu:0.582031 gene_loss:0.534830\n",
      "epoch:229 loss:0.545349 accu:0.636719 gene_loss:0.869321\n",
      "epoch:230 loss:0.313493 accu:0.859375 gene_loss:1.276712\n",
      "epoch:231 loss:0.477448 accu:0.789062 gene_loss:0.843877\n",
      "epoch:232 loss:0.391795 accu:0.730469 gene_loss:0.927661\n",
      "epoch:233 loss:0.479431 accu:0.796875 gene_loss:1.171708\n",
      "epoch:234 loss:0.902153 accu:0.578125 gene_loss:0.682777\n",
      "epoch:235 loss:0.862939 accu:0.667969 gene_loss:0.719363\n",
      "epoch:236 loss:0.616222 accu:0.621094 gene_loss:0.741936\n",
      "epoch:237 loss:0.325353 accu:0.832031 gene_loss:1.213677\n",
      "epoch:238 loss:0.448122 accu:0.769531 gene_loss:0.955627\n",
      "epoch:239 loss:0.347742 accu:0.828125 gene_loss:1.227214\n",
      "epoch:240 loss:0.511985 accu:0.738281 gene_loss:0.927553\n",
      "epoch:241 loss:0.548644 accu:0.675781 gene_loss:0.804683\n",
      "epoch:242 loss:0.431622 accu:0.750000 gene_loss:1.128550\n",
      "epoch:243 loss:0.327316 accu:0.898438 gene_loss:1.119854\n",
      "epoch:244 loss:0.849643 accu:0.605469 gene_loss:0.692023\n",
      "epoch:245 loss:0.725593 accu:0.679688 gene_loss:0.698342\n",
      "epoch:246 loss:0.630573 accu:0.617188 gene_loss:0.623142\n",
      "epoch:247 loss:0.490115 accu:0.695312 gene_loss:0.855239\n",
      "epoch:248 loss:0.352012 accu:0.863281 gene_loss:1.183993\n",
      "epoch:249 loss:0.497826 accu:0.761719 gene_loss:0.751465\n",
      "epoch:250 loss:0.471306 accu:0.718750 gene_loss:0.865408\n",
      "epoch:251 loss:0.370585 accu:0.804688 gene_loss:1.026652\n",
      "epoch:252 loss:0.541249 accu:0.722656 gene_loss:0.727638\n",
      "epoch:253 loss:0.505872 accu:0.687500 gene_loss:0.952782\n",
      "epoch:254 loss:0.384952 accu:0.835938 gene_loss:1.065364\n",
      "epoch:255 loss:0.383005 accu:0.843750 gene_loss:1.119262\n",
      "epoch:256 loss:0.593841 accu:0.750000 gene_loss:0.891714\n",
      "epoch:257 loss:0.358198 accu:0.812500 gene_loss:1.073573\n",
      "epoch:258 loss:0.459424 accu:0.785156 gene_loss:0.894815\n",
      "epoch:259 loss:0.419557 accu:0.746094 gene_loss:0.911180\n",
      "epoch:260 loss:0.349632 accu:0.851562 gene_loss:1.094800\n",
      "epoch:261 loss:0.665999 accu:0.667969 gene_loss:0.748691\n",
      "epoch:262 loss:0.589122 accu:0.687500 gene_loss:0.859577\n",
      "epoch:263 loss:0.422786 accu:0.796875 gene_loss:1.011483\n",
      "epoch:264 loss:0.334746 accu:0.921875 gene_loss:1.182110\n",
      "epoch:265 loss:0.453026 accu:0.738281 gene_loss:0.934706\n",
      "epoch:266 loss:0.333253 accu:0.828125 gene_loss:1.082773\n",
      "epoch:267 loss:0.288263 accu:0.898438 gene_loss:1.200587\n",
      "epoch:268 loss:0.455476 accu:0.800781 gene_loss:0.860268\n",
      "epoch:269 loss:0.358508 accu:0.808594 gene_loss:1.012930\n",
      "epoch:270 loss:0.359531 accu:0.832031 gene_loss:1.022777\n",
      "epoch:271 loss:0.386274 accu:0.855469 gene_loss:1.047556\n",
      "epoch:272 loss:0.391100 accu:0.828125 gene_loss:1.000822\n",
      "epoch:273 loss:0.334400 accu:0.878906 gene_loss:1.082350\n",
      "epoch:274 loss:0.419226 accu:0.785156 gene_loss:0.927801\n",
      "epoch:275 loss:0.366048 accu:0.812500 gene_loss:1.071026\n",
      "epoch:276 loss:0.365927 accu:0.863281 gene_loss:1.043741\n",
      "epoch:277 loss:0.384658 accu:0.816406 gene_loss:0.975886\n",
      "epoch:278 loss:0.444322 accu:0.746094 gene_loss:0.991655\n",
      "epoch:279 loss:0.332139 accu:0.863281 gene_loss:1.148617\n",
      "epoch:280 loss:0.443603 accu:0.808594 gene_loss:0.939089\n",
      "epoch:281 loss:0.340836 accu:0.843750 gene_loss:1.151330\n",
      "epoch:282 loss:0.632146 accu:0.695312 gene_loss:0.886247\n",
      "epoch:283 loss:0.791539 accu:0.640625 gene_loss:0.691922\n",
      "epoch:284 loss:0.408111 accu:0.753906 gene_loss:0.947783\n",
      "epoch:285 loss:0.321760 accu:0.906250 gene_loss:1.140962\n",
      "epoch:286 loss:0.425381 accu:0.800781 gene_loss:0.956868\n",
      "epoch:287 loss:0.341899 accu:0.828125 gene_loss:1.099984\n",
      "epoch:288 loss:0.421779 accu:0.785156 gene_loss:0.926688\n"
     ]
    }
   ],
   "source": [
    "real_labels = np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(20000):\n",
    "    noise = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "\n",
    "    real_image = load_image()\n",
    "    #real_image = load_mnist()\n",
    "    #训练判别器\n",
    "    fake_image = generator_i.predict(noise)\n",
    "\n",
    "    real_loss = discriminator_i.train_on_batch(real_image , real_labels)\n",
    "    fake_loss = discriminator_i.train_on_batch(fake_image , fake_labels)\n",
    "\n",
    "    loss = np.add(real_loss , fake_loss)/2\n",
    "\n",
    "    #训练生成器\n",
    "    noise2 = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "    generator_loss = combined_model_i.train_on_batch(noise2 , real_labels)\n",
    "    \n",
    "    print('epoch:%d loss:%f accu:%f gene_loss:%f' % (i , loss[0] , loss[1] , generator_loss))\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        write_image(i)\n",
    "        #write_image_mnist(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 28, 28, 1)         1097744   \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 1,631,249\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 536,065\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
