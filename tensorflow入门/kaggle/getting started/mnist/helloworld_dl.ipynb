{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name = 'train.csv' , one_hot = True):\n",
    "    csv_reader = csv.reader(open(file_name))\n",
    "\n",
    "    train_set = []\n",
    "    count = 0\n",
    "    train_labels = []\n",
    "    for row in islice(csv_reader ,1 , None):\n",
    "        row = list(map(np.float32 , row)) #类型转换\n",
    "        train_set.append(row)\n",
    "\n",
    "        temp = [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.]\n",
    "        temp[int(train_set[count][0])] = 1.\n",
    "        train_labels.append(temp)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    for i in range(count):\n",
    "        train_set[i] = train_set[i][1:785]\n",
    "    \n",
    "    train_set = np.matrix(train_set)\n",
    "    train_labels = np.matrix(train_labels)\n",
    "    \n",
    "    return train_set , train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载数据\n",
    "train_set , train_labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch = 0\n",
    "\n",
    "def next_batch(train_set , train_labels , batch_size = 600):\n",
    "    global mini_batch\n",
    "    if mini_batch + batch_size > 40000:\n",
    "        mini_batch = 0\n",
    "\n",
    "    train_set_batch = train_set[mini_batch : mini_batch + batch_size]\n",
    "    train_labels_batch = train_labels[mini_batch : mini_batch + batch_size]\n",
    "    \n",
    "    mini_batch = mini_batch + batch_size\n",
    "    \n",
    "    return train_set_batch , train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32 , shape = [None , 784])\n",
    "y_ = tf.placeholder(tf.float32 , shape = [None , 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.123\n",
      "0.2925\n",
      "0.577\n",
      "0.7345\n",
      "0.7575\n",
      "0.8125\n",
      "0.8375\n",
      "0.8245\n",
      "0.83\n",
      "0.8295\n",
      "0.8365\n",
      "0.821\n",
      "0.8205\n",
      "0.8435\n",
      "0.8305\n",
      "0.8545\n",
      "0.857\n",
      "0.859\n",
      "0.8565\n",
      "0.873\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape , stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1 , shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x , W):\n",
    "    return tf.nn.conv2d(x , W , strides=[1,1,1,1] , padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x , ksize = [1 , 2 , 2 , 1] , strides=[1,2,2,1] , padding='SAME')\n",
    "\n",
    "x_image = tf.reshape(x , [-1 , 28,28,1])\n",
    "#mean , variance = tf.nn.moments(x_image , [0,1,2])\n",
    "#x_image = tf.nn.batch_normalization(x_image , mean=mean , variance=variance , offset=0 , scale=1 , variance_epsilon=1e-10)\n",
    "\n",
    "#==============\n",
    "#第一层卷积\n",
    "#==============\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image , W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_pool1 = tf.nn.lrn(h_pool1 , depth_radius=4 , bias= 1, alpha=0.0001 , beta=0.75)\n",
    "\n",
    "#==============\n",
    "#第二层卷积\n",
    "#==============\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1 , W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "h_pool2 = tf.nn.lrn(h_pool2 , depth_radius=4 , bias= 1, alpha=0.0001 , beta=0.75)\n",
    "#==============\n",
    "#全连接层（密集连接层）\n",
    "#==============\n",
    "\n",
    "W_fc1 = weight_variable([7*7*64 , 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2 , [-1 , 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat , W_fc1) + b_fc1)\n",
    "\n",
    "#==============\n",
    "#dropout\n",
    "#==============\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1 , keep_prob)\n",
    "\n",
    "#==============\n",
    "#output layer softmax\n",
    "#==============\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "logits = tf.matmul(h_fc1_drop , W_fc2 ) + b_fc2\n",
    "\n",
    "#==============\n",
    "#cost function\n",
    "#==============\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits , labels=y_)\n",
    "\n",
    "#==============\n",
    "#train\n",
    "#==============\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "#尝试 0.005 变慢了\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "dev_feed = {x:train_set[40000:] , y_:train_labels[40000:] , keep_prob:1.0}\n",
    "right = tf.equal(tf.argmax(y_ , 1) , tf.argmax(tf.nn.softmax(logits) , 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(right , tf.float32))\n",
    "\n",
    "for i in range(200):\n",
    "    train_set_batch , train_labels_batch = next_batch(train_set , train_labels , batch_size = 100)\n",
    "    sess.run(train_step , feed_dict = {x:train_set_batch , y_:train_labels_batch , keep_prob:0.8})\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(sess.run(accuracy , feed_dict = dev_feed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_set(file_name = 'test.csv'):\n",
    "    csv_reader = csv.reader(open(file_name))\n",
    "    test_set = []\n",
    "    \n",
    "    for row in islice(csv_reader ,1 , None):\n",
    "        row = list(map(np.float32 , row))\n",
    "        test_set.append(row)\n",
    "        \n",
    "    test_set = np.matrix(test_set)\n",
    "\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = load_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch_test = 0\n",
    "\n",
    "def read_next_batch(test_set , batch_size = 2800):\n",
    "    global mini_batch_test\n",
    "    \n",
    "    test_set_batch = test_set[mini_batch_test : mini_batch_test + batch_size]\n",
    "    \n",
    "    mini_batch_test = mini_batch_test + batch_size\n",
    "    \n",
    "    return test_set_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = open('sample_submission.csv' , 'w' , newline='')\n",
    "writer = csv.writer(csv_file)\n",
    "writer.writerow(['ImageId','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_labels = tf.argmax(tf.nn.softmax(logits) , 1)\n",
    "\n",
    "k=1\n",
    "for i in range(10):\n",
    "    mini_test_set = read_next_batch(test_set)\n",
    "    result = sess.run(test_set_labels , feed_dict={x:mini_test_set , keep_prob:1.0})\n",
    "    \n",
    "    for j in range(2800):\n",
    "        w=j+k\n",
    "        writer.writerow([w , result[j]])\n",
    "    k+=2800\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
