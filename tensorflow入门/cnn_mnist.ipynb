{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name = 'train.csv' , one_hot = True):\n",
    "    csv_reader = csv.reader(open(file_name))\n",
    "\n",
    "    train_set = []\n",
    "    count = 0\n",
    "    train_labels = []\n",
    "    for row in islice(csv_reader ,1 , None):\n",
    "        row = list(map(np.float32 , row)) #类型转换\n",
    "        train_set.append(row)\n",
    "\n",
    "        temp = [0.,0.,0.,0.,0.,0.,0.,0.,0.,0.]\n",
    "        temp[int(train_set[count][0])] = 1.\n",
    "        train_labels.append(temp)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    for i in range(count):\n",
    "        train_set[i] = train_set[i][1:785]\n",
    "    \n",
    "    train_set = np.matrix(train_set)\n",
    "    train_labels = np.matrix(train_labels)\n",
    "    \n",
    "    return train_set , train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载数据\n",
    "train_set , train_labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = train_set / 255.0\n",
    "#print(train_set)\n",
    "#print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnn add\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1 , shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x , W):\n",
    "    return tf.nn.conv2d(x , W , strides=[1,1,1,1] , padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x , ksize=[1,2,2,1] , strides=[1,2,2,1] , padding='SAME')\n",
    "\n",
    "#数据重构\n",
    "xs = tf.placeholder(tf.float32 , [None , 784]) #28*28\n",
    "ys = tf.placeholder(tf.float32 , [None , 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#-1就是一次batch的size\n",
    "x_image = tf.reshape(xs , [-1 , 28 , 28 , 1]) #28*28的图片 通道为1（只有黑白色彩） 彩色就3 （RGB）\n",
    "\n",
    "#========\n",
    "#conv1\n",
    "#首先是卷积核\n",
    "#长 宽 通道数量 卷积核个数（feature map个数）\n",
    "#一个feature map使用一个weight和一个bias\n",
    "W_conv1 = weight_variable([5 , 5 , 1 , 32])\n",
    "#每一个feature map共用一个bias\n",
    "#32个 feature map 所以使用32个bias\n",
    "b_conv1 = bias_variable([32])\n",
    "#relu激活\n",
    "\n",
    "#padding还有VALID方式\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image , W_conv1) + b_conv1)#28*28*32 因为padding 为‘SAME’ 所以图像尺寸没有发生变化 输出为32个feature map\n",
    "#max池化\n",
    "h_pool1 = max_pool_2x2(h_conv1)#14*14*32 因为pooling的size是2*2 故尺寸缩小为原来的1/2 还是32个feature map\n",
    "\n",
    "#========\n",
    "#conv2\n",
    "W_conv2 = weight_variable([5,5,32,64])#第二个卷积层 产生64个feature map\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1 , W_conv2) + b_conv2)#14*14*64\n",
    "h_pool2 = max_pool_2x2(h_conv2)#7*7*64\n",
    "\n",
    "#========\n",
    "#fully connect1\n",
    "#将卷积层输出的图像信息变回2维\n",
    "h_pool2_flat = tf.reshape(h_pool2 , [-1 , 7*7*64]) #-1表示不知道多少个样本 python自动计算 列数为7*7*64 一行一个样本\n",
    "W_fc1 = weight_variable([7*7*64 , 1024])#全连接 为1024神经元\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat , W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1 , keep_prob)#防止overfitting\n",
    "\n",
    "#========\n",
    "#fully connect2\n",
    "W_fc2 = weight_variable([1024 , 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "prediction = tf.matmul(h_fc1_drop , W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch = 0\n",
    "\n",
    "def next_batch(train_set , train_labels , batch_size = 50):\n",
    "    global mini_batch\n",
    "    if mini_batch + batch_size > 42000:\n",
    "        mini_batch = 0\n",
    "\n",
    "    train_set_batch = train_set[mini_batch : mini_batch + batch_size]\n",
    "    train_labels_batch = train_labels[mini_batch : mini_batch + batch_size]\n",
    "    \n",
    "    mini_batch = mini_batch + batch_size\n",
    "    \n",
    "    return train_set_batch , train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.3579\n",
      "46.4316\n",
      "7.10662\n",
      "2.63844\n",
      "2.52945\n",
      "2.6148\n",
      "2.36937\n",
      "2.63521\n",
      "2.52319\n",
      "2.35347\n",
      "2.18342\n",
      "1.92692\n",
      "2.10963\n",
      "1.67048\n",
      "1.82099\n",
      "1.48772\n",
      "1.6829\n",
      "1.33412\n",
      "1.16525\n",
      "1.0439\n",
      "1.04156\n",
      "1.19907\n",
      "1.08739\n",
      "0.951698\n",
      "0.808418\n",
      "1.01813\n",
      "0.770298\n",
      "0.787483\n",
      "0.655927\n",
      "0.524133\n",
      "0.548318\n",
      "0.857671\n",
      "0.656665\n",
      "0.489155\n",
      "0.379253\n",
      "0.628915\n",
      "0.863418\n",
      "0.533263\n",
      "0.605751\n",
      "0.670518\n",
      "0.475306\n",
      "0.229157\n",
      "0.455454\n",
      "0.713327\n",
      "0.229772\n",
      "0.549607\n",
      "0.261112\n",
      "0.399788\n",
      "0.393649\n",
      "0.527245\n",
      "0.364692\n",
      "0.269676\n",
      "0.554575\n",
      "0.347891\n",
      "0.378569\n",
      "0.251422\n",
      "0.406348\n",
      "0.309332\n",
      "0.431848\n",
      "0.559792\n",
      "0.623767\n",
      "0.275286\n",
      "0.181219\n",
      "0.338481\n",
      "0.433795\n",
      "0.108607\n",
      "0.159762\n",
      "0.277572\n",
      "0.412243\n",
      "0.377445\n",
      "0.302704\n",
      "0.223652\n",
      "0.2424\n",
      "0.296083\n",
      "0.131012\n",
      "0.119845\n",
      "0.0757381\n",
      "0.363613\n",
      "0.189785\n",
      "0.296205\n",
      "0.248628\n",
      "0.275451\n",
      "0.260419\n",
      "0.0997222\n",
      "0.185169\n",
      "0.187688\n",
      "0.188557\n",
      "0.170859\n",
      "0.320347\n",
      "0.176334\n",
      "0.184412\n",
      "0.432193\n",
      "0.165413\n",
      "0.0491625\n",
      "0.162401\n",
      "0.15386\n",
      "0.0694445\n",
      "0.037214\n",
      "0.270395\n",
      "0.142799\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction , labels=ys))\n",
    "#-tf.reduce_sum(ys * tf.log(prediction + 1e-10))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    train_set_batch , train_labels_batch = next_batch(train_set , train_labels)\n",
    "    sess.run(train_step , feed_dict = {xs:train_set_batch , ys:train_labels_batch , keep_prob:0.8})\n",
    "    print(sess.run(loss , feed_dict = {xs:train_set_batch , ys:train_labels_batch , keep_prob:0.8}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev-set\n",
    "train_set_batch , train_labels_batch = next_batch(train_set , train_labels ,10000)\n",
    "\n",
    "prediction_number = tf.argmax(tf.nn.softmax(prediction) , 1)\n",
    "real_number = tf.argmax(ys , 1)\n",
    "right = tf.equal(prediction_number , real_number)\n",
    "accuracy = tf.reduce_mean(tf.cast(right , tf.float32))\n",
    "\n",
    "print(sess.run(accuracy , feed_dict={xs:train_set_batch , ys:train_labels_batch , keep_prob:1.0} ))\n",
    "\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_set(file_name = 'test.csv'):\n",
    "    csv_reader = csv.reader(open(file_name))\n",
    "    test_set = []\n",
    "    \n",
    "    for row in islice(csv_reader ,1 , None):\n",
    "        row = list(map(np.float32 , row))\n",
    "        test_set.append(row)\n",
    "        \n",
    "    test_set = np.matrix(test_set)\n",
    "\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = load_test_set()\n",
    "test_set = test_set / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set_labels = tf.argmax(prediction , 1)\n",
    "result = sess.run(test_set_labels , feed_dict={x:test_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = open('sample_submission.csv' , 'w' , newline='')\n",
    "writer = csv.writer(csv_file)\n",
    "writer.writerow(['ImageId','Label'])\n",
    "\n",
    "for i in range(28000):\n",
    "    writer.writerow([i+1 , result[i]])\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
