{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential , Model\n",
    "from keras.layers import Dense ,  BatchNormalization , Reshape , Input , Flatten\n",
    "from keras.layers import Conv2D , MaxPool2D , Conv2DTranspose , UpSampling2D , ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import Add , Multiply\n",
    "\n",
    "from keras.initializers import random_normal\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import gc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIDTH = 28\n",
    "HEIGHT = 28\n",
    "CHANNEL = 1\n",
    "\n",
    "LATENT_DIM = 100 #latent variable z sample from normal distribution\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10\n",
    "\n",
    "PATH = 'faces/'\n",
    "\n",
    "#生成多少个图像 长*宽\n",
    "ROW = 5\n",
    "COL = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nload_index = 0\\n\\nimages_name = os.listdir(PATH)\\n\\nIMAGES_COUNT = len(images_name)\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "load_index = 0\n",
    "\n",
    "images_name = os.listdir(PATH)\n",
    "\n",
    "IMAGES_COUNT = len(images_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "(X_train , y_train),(X_test , y_test) = mnist.load_data()\n",
    "X_train = X_train/127.5-1\n",
    "X_train = np.expand_dims(X_train , 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_mnist():\n",
    "    return X_train[np.random.randint(0, X_train.shape[0], BATCH_SIZE)]\n",
    "    \n",
    "def write_image_mnist(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = decoder_i.predict(noise)\n",
    "    generated_image = generated_image*0.5+0.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count,:,:,0] , cmap = 'gray')\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('mnist_aae/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef load_image(batch_size = BATCH_SIZE):\\n    global load_index\\n    \\n    images = []\\n    \\n    for i in range(batch_size):\\n        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\\n    \\n    load_index += batch_size\\n    \\n    return np.array(images)/127.5-1\\n\\ndef write_image(epoch):\\n    \\n    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\\n    generated_image = generator_i.predict(noise)\\n    generated_image = (generated_image+1)*127.5\\n    \\n    fig , axes = plt.pyplot.subplots(ROW , COL)\\n    \\n    count=0\\n    \\n    for i in range(ROW):\\n        for j in range(COL):\\n            axes[i][j].imshow(generated_image[count])\\n            axes[i][j].axis('off')\\n            count += 1\\n            \\n    fig.savefig('generated_faces_dcgan/No.%d.png' % epoch)\\n    plt.pyplot.close()\\n    \\n    #plt.image.imsave('images/'+str(epoch)+'.jpg')\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def load_image(batch_size = BATCH_SIZE):\n",
    "    global load_index\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        images.append(plt.image.imread(PATH + images_name[(load_index + i) % IMAGES_COUNT]))\n",
    "    \n",
    "    load_index += batch_size\n",
    "    \n",
    "    return np.array(images)/127.5-1\n",
    "\n",
    "def write_image(epoch):\n",
    "    \n",
    "    noise = np.random.normal(size = (ROW*COL , LATENT_DIM))\n",
    "    generated_image = generator_i.predict(noise)\n",
    "    generated_image = (generated_image+1)*127.5\n",
    "    \n",
    "    fig , axes = plt.pyplot.subplots(ROW , COL)\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    for i in range(ROW):\n",
    "        for j in range(COL):\n",
    "            axes[i][j].imshow(generated_image[count])\n",
    "            axes[i][j].axis('off')\n",
    "            count += 1\n",
    "            \n",
    "    fig.savefig('generated_faces_dcgan/No.%d.png' % epoch)\n",
    "    plt.pyplot.close()\n",
    "    \n",
    "    #plt.image.imsave('images/'+str(epoch)+'.jpg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    #input a image to discriminate real or fake\n",
    "    model = Sequential(name='discriminator')\n",
    "    \n",
    "    model.add(Dense(512 , input_shape=(LATENT_DIM , )))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    encoder_feature = Input(shape=(LATENT_DIM , ))\n",
    "    validity = model(encoder_feature)\n",
    "    \n",
    "    return Model(encoder_feature , validity , name='discriminator_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp(x):\n",
    "    return K.exp(x)\n",
    "\n",
    "def encoder():\n",
    "    image = Input(shape=(HEIGHT , WIDTH , CHANNEL))\n",
    "    \n",
    "    h = Flatten()(image)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    \n",
    "    mean = Dense(LATENT_DIM)(h)\n",
    "    log_var = Dense(LATENT_DIM)(h)\n",
    "    \n",
    "    a=Activation(exp)(log_var)\n",
    "    b=K.random_normal(shape=K.shape(log_var))\n",
    "    encoder_feature = Add()([mean , Multiply()([a , b])])\n",
    "    #    \n",
    "    return Model(image , a)\n",
    "\n",
    "def decoder():\n",
    "    input_feature = Input(shape=(LATENT_DIM , ))\n",
    "    \n",
    "    h = Dense(512 , input_shape=(LATENT_DIM , ))(input_feature)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(512)(h)\n",
    "    h = LeakyReLU(alpha=0.2)(h)\n",
    "    h = Dense(HEIGHT*WIDTH*CHANNEL)(h)\n",
    "    h = Activation('tanh')(h)\n",
    "    image_hat = Reshape(target_shape=(HEIGHT , WIDTH , CHANNEL))(h)\n",
    "    \n",
    "    return Model(input_feature , image_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr = 0.0002 , beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_i = discriminator()\n",
    "discriminator_i.compile(optimizer=adam , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "\n",
    "encoder_i = encoder()\n",
    "decoder_i = decoder()\n",
    "\n",
    "image = Input(shape=(HEIGHT , WIDTH , CHANNEL))\n",
    "\n",
    "encoder_feature = encoder_i(image)\n",
    "image_hat = decoder_i(encoder_feature)\n",
    "\n",
    "discriminator_i.trainable = False\n",
    "\n",
    "validity = discriminator_i(encoder_feature)\n",
    "\n",
    "#aae为重新构造的组合model\n",
    "aae = Model(image , [image_hat , validity])\n",
    "\n",
    "aae.compile(optimizer=adam , loss=['mse' , 'binary_crossentropy'] , loss_weights = [0.999 , 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:16.019650 accu:0.000000 aae_loss_1:0.015980 aae_loss_2:0.015996\n",
      "epoch:1 loss:14.639118 accu:0.000000 aae_loss_1:0.016262 aae_loss_2:0.016278\n",
      "epoch:2 loss:9.329687 accu:0.136719 aae_loss_1:0.015477 aae_loss_2:0.015492\n",
      "epoch:3 loss:7.971489 accu:0.500000 aae_loss_1:0.015405 aae_loss_2:0.015420\n",
      "epoch:4 loss:7.971227 accu:0.500000 aae_loss_1:0.014602 aae_loss_2:0.014617\n",
      "epoch:5 loss:7.971199 accu:0.500000 aae_loss_1:0.014125 aae_loss_2:0.014139\n",
      "epoch:6 loss:7.971213 accu:0.500000 aae_loss_1:0.013097 aae_loss_2:0.013110\n",
      "epoch:7 loss:7.971198 accu:0.500000 aae_loss_1:0.013589 aae_loss_2:0.013602\n",
      "epoch:8 loss:7.971196 accu:0.500000 aae_loss_1:0.012287 aae_loss_2:0.012300\n",
      "epoch:9 loss:7.971204 accu:0.500000 aae_loss_1:0.013292 aae_loss_2:0.013306\n",
      "epoch:10 loss:7.971231 accu:0.500000 aae_loss_1:0.013069 aae_loss_2:0.013082\n",
      "epoch:11 loss:7.971200 accu:0.500000 aae_loss_1:0.012406 aae_loss_2:0.012418\n",
      "epoch:12 loss:7.971199 accu:0.500000 aae_loss_1:0.012928 aae_loss_2:0.012941\n",
      "epoch:13 loss:7.971203 accu:0.500000 aae_loss_1:0.013175 aae_loss_2:0.013188\n",
      "epoch:14 loss:7.971206 accu:0.500000 aae_loss_1:0.013873 aae_loss_2:0.013887\n",
      "epoch:15 loss:7.971198 accu:0.500000 aae_loss_1:0.013631 aae_loss_2:0.013645\n",
      "epoch:16 loss:7.971199 accu:0.500000 aae_loss_1:0.015111 aae_loss_2:0.015126\n",
      "epoch:17 loss:7.971198 accu:0.500000 aae_loss_1:0.016153 aae_loss_2:0.016170\n",
      "epoch:18 loss:7.971201 accu:0.500000 aae_loss_1:0.016639 aae_loss_2:0.016656\n",
      "epoch:19 loss:7.971202 accu:0.500000 aae_loss_1:0.015917 aae_loss_2:0.015933\n",
      "epoch:20 loss:7.971200 accu:0.500000 aae_loss_1:0.014916 aae_loss_2:0.014931\n",
      "epoch:21 loss:7.971197 accu:0.500000 aae_loss_1:0.014120 aae_loss_2:0.014134\n",
      "epoch:22 loss:7.971203 accu:0.500000 aae_loss_1:0.014854 aae_loss_2:0.014869\n",
      "epoch:23 loss:7.971199 accu:0.500000 aae_loss_1:0.014285 aae_loss_2:0.014300\n",
      "epoch:24 loss:7.971205 accu:0.500000 aae_loss_1:0.015181 aae_loss_2:0.015196\n",
      "epoch:25 loss:7.971205 accu:0.500000 aae_loss_1:0.015857 aae_loss_2:0.015873\n",
      "epoch:26 loss:7.971198 accu:0.500000 aae_loss_1:0.016059 aae_loss_2:0.016075\n",
      "epoch:27 loss:7.971200 accu:0.500000 aae_loss_1:0.014392 aae_loss_2:0.014407\n",
      "epoch:28 loss:7.971200 accu:0.500000 aae_loss_1:0.013809 aae_loss_2:0.013823\n",
      "epoch:29 loss:7.971201 accu:0.500000 aae_loss_1:0.013606 aae_loss_2:0.013620\n",
      "epoch:30 loss:7.971205 accu:0.500000 aae_loss_1:0.013437 aae_loss_2:0.013450\n",
      "epoch:31 loss:7.971199 accu:0.500000 aae_loss_1:0.014840 aae_loss_2:0.014855\n",
      "epoch:32 loss:7.971200 accu:0.500000 aae_loss_1:0.013549 aae_loss_2:0.013562\n",
      "epoch:33 loss:7.971210 accu:0.500000 aae_loss_1:0.013125 aae_loss_2:0.013138\n",
      "epoch:34 loss:7.971198 accu:0.500000 aae_loss_1:0.014286 aae_loss_2:0.014300\n",
      "epoch:35 loss:7.971204 accu:0.500000 aae_loss_1:0.013468 aae_loss_2:0.013481\n",
      "epoch:36 loss:7.971200 accu:0.500000 aae_loss_1:0.013059 aae_loss_2:0.013072\n",
      "epoch:37 loss:7.971198 accu:0.500000 aae_loss_1:0.013251 aae_loss_2:0.013264\n",
      "epoch:38 loss:7.971200 accu:0.500000 aae_loss_1:0.013291 aae_loss_2:0.013304\n",
      "epoch:39 loss:7.971198 accu:0.500000 aae_loss_1:0.012576 aae_loss_2:0.012589\n",
      "epoch:40 loss:7.971197 accu:0.500000 aae_loss_1:0.013804 aae_loss_2:0.013818\n",
      "epoch:41 loss:7.971199 accu:0.500000 aae_loss_1:0.012448 aae_loss_2:0.012460\n",
      "epoch:42 loss:7.971200 accu:0.500000 aae_loss_1:0.014392 aae_loss_2:0.014407\n",
      "epoch:43 loss:7.971196 accu:0.500000 aae_loss_1:0.013102 aae_loss_2:0.013115\n",
      "epoch:44 loss:7.971198 accu:0.500000 aae_loss_1:0.013482 aae_loss_2:0.013496\n",
      "epoch:45 loss:7.971202 accu:0.500000 aae_loss_1:0.013230 aae_loss_2:0.013243\n",
      "epoch:46 loss:7.971197 accu:0.500000 aae_loss_1:0.014940 aae_loss_2:0.014955\n",
      "epoch:47 loss:7.971198 accu:0.500000 aae_loss_1:0.014840 aae_loss_2:0.014855\n",
      "epoch:48 loss:7.971203 accu:0.500000 aae_loss_1:0.014926 aae_loss_2:0.014941\n",
      "epoch:49 loss:7.971200 accu:0.500000 aae_loss_1:0.015682 aae_loss_2:0.015697\n",
      "epoch:50 loss:7.971199 accu:0.500000 aae_loss_1:0.013702 aae_loss_2:0.013716\n",
      "epoch:51 loss:7.971200 accu:0.500000 aae_loss_1:0.013910 aae_loss_2:0.013924\n",
      "epoch:52 loss:7.971201 accu:0.500000 aae_loss_1:0.013628 aae_loss_2:0.013642\n",
      "epoch:53 loss:7.971198 accu:0.500000 aae_loss_1:0.014089 aae_loss_2:0.014104\n",
      "epoch:54 loss:7.971195 accu:0.500000 aae_loss_1:0.014113 aae_loss_2:0.014127\n",
      "epoch:55 loss:7.971198 accu:0.500000 aae_loss_1:0.013744 aae_loss_2:0.013758\n",
      "epoch:56 loss:7.971202 accu:0.500000 aae_loss_1:0.014750 aae_loss_2:0.014765\n",
      "epoch:57 loss:7.971199 accu:0.500000 aae_loss_1:0.015181 aae_loss_2:0.015196\n",
      "epoch:58 loss:7.971199 accu:0.500000 aae_loss_1:0.015852 aae_loss_2:0.015868\n",
      "epoch:59 loss:7.971200 accu:0.500000 aae_loss_1:0.014830 aae_loss_2:0.014845\n",
      "epoch:60 loss:7.971205 accu:0.500000 aae_loss_1:0.014778 aae_loss_2:0.014793\n",
      "epoch:61 loss:7.971202 accu:0.500000 aae_loss_1:0.014642 aae_loss_2:0.014656\n",
      "epoch:62 loss:7.971198 accu:0.500000 aae_loss_1:0.013862 aae_loss_2:0.013876\n",
      "epoch:63 loss:7.971198 accu:0.500000 aae_loss_1:0.014636 aae_loss_2:0.014651\n",
      "epoch:64 loss:7.971199 accu:0.500000 aae_loss_1:0.013265 aae_loss_2:0.013279\n",
      "epoch:65 loss:7.971203 accu:0.500000 aae_loss_1:0.015497 aae_loss_2:0.015513\n",
      "epoch:66 loss:7.971198 accu:0.500000 aae_loss_1:0.013789 aae_loss_2:0.013803\n",
      "epoch:67 loss:7.971200 accu:0.500000 aae_loss_1:0.013025 aae_loss_2:0.013038\n",
      "epoch:68 loss:7.971199 accu:0.500000 aae_loss_1:0.013724 aae_loss_2:0.013738\n",
      "epoch:69 loss:7.971204 accu:0.500000 aae_loss_1:0.013473 aae_loss_2:0.013487\n",
      "epoch:70 loss:7.971200 accu:0.500000 aae_loss_1:0.013846 aae_loss_2:0.013860\n",
      "epoch:71 loss:7.971201 accu:0.500000 aae_loss_1:0.013329 aae_loss_2:0.013342\n",
      "epoch:72 loss:7.971200 accu:0.500000 aae_loss_1:0.013800 aae_loss_2:0.013814\n",
      "epoch:73 loss:7.971200 accu:0.500000 aae_loss_1:0.014963 aae_loss_2:0.014978\n",
      "epoch:74 loss:7.971200 accu:0.500000 aae_loss_1:0.014491 aae_loss_2:0.014505\n",
      "epoch:75 loss:7.971210 accu:0.500000 aae_loss_1:0.014389 aae_loss_2:0.014403\n",
      "epoch:76 loss:7.971199 accu:0.500000 aae_loss_1:0.013801 aae_loss_2:0.013815\n",
      "epoch:77 loss:7.971210 accu:0.500000 aae_loss_1:0.015455 aae_loss_2:0.015471\n",
      "epoch:78 loss:7.971200 accu:0.500000 aae_loss_1:0.015422 aae_loss_2:0.015437\n",
      "epoch:79 loss:7.971199 accu:0.500000 aae_loss_1:0.014912 aae_loss_2:0.014927\n",
      "epoch:80 loss:7.971195 accu:0.500000 aae_loss_1:0.014270 aae_loss_2:0.014284\n",
      "epoch:81 loss:7.971197 accu:0.500000 aae_loss_1:0.015112 aae_loss_2:0.015127\n",
      "epoch:82 loss:7.971208 accu:0.500000 aae_loss_1:0.013851 aae_loss_2:0.013865\n",
      "epoch:83 loss:7.971199 accu:0.500000 aae_loss_1:0.014735 aae_loss_2:0.014749\n",
      "epoch:84 loss:7.971200 accu:0.500000 aae_loss_1:0.013703 aae_loss_2:0.013716\n",
      "epoch:85 loss:7.971200 accu:0.500000 aae_loss_1:0.014995 aae_loss_2:0.015010\n",
      "epoch:86 loss:7.971195 accu:0.500000 aae_loss_1:0.014065 aae_loss_2:0.014079\n",
      "epoch:87 loss:7.971201 accu:0.500000 aae_loss_1:0.013400 aae_loss_2:0.013414\n",
      "epoch:88 loss:7.971198 accu:0.500000 aae_loss_1:0.014100 aae_loss_2:0.014114\n",
      "epoch:89 loss:7.971202 accu:0.500000 aae_loss_1:0.013924 aae_loss_2:0.013938\n",
      "epoch:90 loss:7.971209 accu:0.500000 aae_loss_1:0.013933 aae_loss_2:0.013947\n",
      "epoch:91 loss:7.971203 accu:0.500000 aae_loss_1:0.014067 aae_loss_2:0.014081\n",
      "epoch:92 loss:7.971198 accu:0.500000 aae_loss_1:0.013656 aae_loss_2:0.013670\n",
      "epoch:93 loss:7.971199 accu:0.500000 aae_loss_1:0.012902 aae_loss_2:0.012914\n",
      "epoch:94 loss:7.971195 accu:0.500000 aae_loss_1:0.012194 aae_loss_2:0.012206\n",
      "epoch:95 loss:7.971200 accu:0.500000 aae_loss_1:0.011960 aae_loss_2:0.011972\n",
      "epoch:96 loss:7.971198 accu:0.500000 aae_loss_1:0.012236 aae_loss_2:0.012248\n",
      "epoch:97 loss:7.971201 accu:0.500000 aae_loss_1:0.013033 aae_loss_2:0.013047\n",
      "epoch:98 loss:7.971196 accu:0.500000 aae_loss_1:0.013049 aae_loss_2:0.013062\n",
      "epoch:99 loss:7.971201 accu:0.500000 aae_loss_1:0.014449 aae_loss_2:0.014463\n",
      "epoch:100 loss:7.971198 accu:0.500000 aae_loss_1:0.013353 aae_loss_2:0.013367\n",
      "epoch:101 loss:7.971198 accu:0.500000 aae_loss_1:0.012860 aae_loss_2:0.012872\n",
      "epoch:102 loss:7.971198 accu:0.500000 aae_loss_1:0.014012 aae_loss_2:0.014026\n",
      "epoch:103 loss:7.971206 accu:0.500000 aae_loss_1:0.014898 aae_loss_2:0.014912\n",
      "epoch:104 loss:7.971197 accu:0.500000 aae_loss_1:0.016797 aae_loss_2:0.016814\n",
      "epoch:105 loss:7.971200 accu:0.500000 aae_loss_1:0.018088 aae_loss_2:0.018106\n",
      "epoch:106 loss:7.971200 accu:0.500000 aae_loss_1:0.015390 aae_loss_2:0.015406\n",
      "epoch:107 loss:7.971215 accu:0.500000 aae_loss_1:0.013780 aae_loss_2:0.013794\n",
      "epoch:108 loss:7.971197 accu:0.500000 aae_loss_1:0.013239 aae_loss_2:0.013252\n",
      "epoch:109 loss:7.971202 accu:0.500000 aae_loss_1:0.013742 aae_loss_2:0.013756\n",
      "epoch:110 loss:7.971197 accu:0.500000 aae_loss_1:0.013215 aae_loss_2:0.013228\n",
      "epoch:111 loss:7.971203 accu:0.500000 aae_loss_1:0.014450 aae_loss_2:0.014465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:112 loss:7.971197 accu:0.500000 aae_loss_1:0.015026 aae_loss_2:0.015041\n",
      "epoch:113 loss:7.971199 accu:0.500000 aae_loss_1:0.013573 aae_loss_2:0.013587\n",
      "epoch:114 loss:7.971197 accu:0.500000 aae_loss_1:0.013246 aae_loss_2:0.013259\n",
      "epoch:115 loss:7.971197 accu:0.500000 aae_loss_1:0.012431 aae_loss_2:0.012443\n",
      "epoch:116 loss:7.971199 accu:0.500000 aae_loss_1:0.013431 aae_loss_2:0.013445\n",
      "epoch:117 loss:7.971198 accu:0.500000 aae_loss_1:0.012520 aae_loss_2:0.012532\n",
      "epoch:118 loss:7.971200 accu:0.500000 aae_loss_1:0.012916 aae_loss_2:0.012928\n",
      "epoch:119 loss:7.971200 accu:0.500000 aae_loss_1:0.013881 aae_loss_2:0.013895\n",
      "epoch:120 loss:7.971204 accu:0.500000 aae_loss_1:0.013530 aae_loss_2:0.013544\n",
      "epoch:121 loss:7.971195 accu:0.500000 aae_loss_1:0.015154 aae_loss_2:0.015169\n",
      "epoch:122 loss:7.971199 accu:0.500000 aae_loss_1:0.013674 aae_loss_2:0.013688\n",
      "epoch:123 loss:7.971196 accu:0.500000 aae_loss_1:0.014734 aae_loss_2:0.014749\n",
      "epoch:124 loss:7.971198 accu:0.500000 aae_loss_1:0.013670 aae_loss_2:0.013684\n",
      "epoch:125 loss:7.971199 accu:0.500000 aae_loss_1:0.013268 aae_loss_2:0.013281\n",
      "epoch:126 loss:7.971198 accu:0.500000 aae_loss_1:0.013959 aae_loss_2:0.013973\n",
      "epoch:127 loss:7.971198 accu:0.500000 aae_loss_1:0.013939 aae_loss_2:0.013953\n",
      "epoch:128 loss:7.971200 accu:0.500000 aae_loss_1:0.013285 aae_loss_2:0.013298\n",
      "epoch:129 loss:7.971205 accu:0.500000 aae_loss_1:0.014301 aae_loss_2:0.014315\n",
      "epoch:130 loss:7.971198 accu:0.500000 aae_loss_1:0.012848 aae_loss_2:0.012861\n",
      "epoch:131 loss:7.971197 accu:0.500000 aae_loss_1:0.015194 aae_loss_2:0.015209\n",
      "epoch:132 loss:7.971197 accu:0.500000 aae_loss_1:0.014648 aae_loss_2:0.014663\n",
      "epoch:133 loss:7.971197 accu:0.500000 aae_loss_1:0.013443 aae_loss_2:0.013456\n",
      "epoch:134 loss:7.971202 accu:0.500000 aae_loss_1:0.014064 aae_loss_2:0.014079\n",
      "epoch:135 loss:7.971199 accu:0.500000 aae_loss_1:0.014917 aae_loss_2:0.014932\n",
      "epoch:136 loss:7.971198 accu:0.500000 aae_loss_1:0.014662 aae_loss_2:0.014677\n",
      "epoch:137 loss:7.971197 accu:0.500000 aae_loss_1:0.013949 aae_loss_2:0.013963\n",
      "epoch:138 loss:7.971199 accu:0.500000 aae_loss_1:0.014209 aae_loss_2:0.014223\n",
      "epoch:139 loss:7.971199 accu:0.500000 aae_loss_1:0.014111 aae_loss_2:0.014125\n",
      "epoch:140 loss:7.971198 accu:0.500000 aae_loss_1:0.014823 aae_loss_2:0.014837\n",
      "epoch:141 loss:7.971198 accu:0.500000 aae_loss_1:0.013879 aae_loss_2:0.013893\n",
      "epoch:142 loss:7.971200 accu:0.500000 aae_loss_1:0.014895 aae_loss_2:0.014910\n",
      "epoch:143 loss:7.971200 accu:0.500000 aae_loss_1:0.014436 aae_loss_2:0.014450\n",
      "epoch:144 loss:7.971199 accu:0.500000 aae_loss_1:0.015455 aae_loss_2:0.015470\n",
      "epoch:145 loss:7.971196 accu:0.500000 aae_loss_1:0.015299 aae_loss_2:0.015315\n",
      "epoch:146 loss:7.971200 accu:0.500000 aae_loss_1:0.014301 aae_loss_2:0.014315\n",
      "epoch:147 loss:7.971212 accu:0.500000 aae_loss_1:0.015126 aae_loss_2:0.015141\n",
      "epoch:148 loss:7.971202 accu:0.500000 aae_loss_1:0.015386 aae_loss_2:0.015401\n",
      "epoch:149 loss:7.971205 accu:0.500000 aae_loss_1:0.016036 aae_loss_2:0.016052\n",
      "epoch:150 loss:7.971200 accu:0.500000 aae_loss_1:0.014468 aae_loss_2:0.014482\n",
      "epoch:151 loss:7.971201 accu:0.500000 aae_loss_1:0.015963 aae_loss_2:0.015979\n",
      "epoch:152 loss:7.971197 accu:0.500000 aae_loss_1:0.016920 aae_loss_2:0.016937\n",
      "epoch:153 loss:7.971211 accu:0.500000 aae_loss_1:0.015322 aae_loss_2:0.015338\n",
      "epoch:154 loss:7.971198 accu:0.500000 aae_loss_1:0.013256 aae_loss_2:0.013270\n",
      "epoch:155 loss:7.971204 accu:0.500000 aae_loss_1:0.015327 aae_loss_2:0.015342\n",
      "epoch:156 loss:7.971207 accu:0.500000 aae_loss_1:0.014020 aae_loss_2:0.014034\n",
      "epoch:157 loss:7.971195 accu:0.500000 aae_loss_1:0.014465 aae_loss_2:0.014480\n",
      "epoch:158 loss:7.971198 accu:0.500000 aae_loss_1:0.013079 aae_loss_2:0.013092\n",
      "epoch:159 loss:7.971200 accu:0.500000 aae_loss_1:0.013388 aae_loss_2:0.013401\n",
      "epoch:160 loss:7.971204 accu:0.500000 aae_loss_1:0.012919 aae_loss_2:0.012932\n",
      "epoch:161 loss:7.971200 accu:0.500000 aae_loss_1:0.012870 aae_loss_2:0.012883\n",
      "epoch:162 loss:7.971203 accu:0.500000 aae_loss_1:0.014786 aae_loss_2:0.014801\n",
      "epoch:163 loss:7.971198 accu:0.500000 aae_loss_1:0.015616 aae_loss_2:0.015632\n",
      "epoch:164 loss:7.971201 accu:0.500000 aae_loss_1:0.013494 aae_loss_2:0.013507\n",
      "epoch:165 loss:7.971198 accu:0.500000 aae_loss_1:0.014153 aae_loss_2:0.014167\n",
      "epoch:166 loss:7.971197 accu:0.500000 aae_loss_1:0.014170 aae_loss_2:0.014184\n",
      "epoch:167 loss:7.971198 accu:0.500000 aae_loss_1:0.014669 aae_loss_2:0.014683\n",
      "epoch:168 loss:7.971200 accu:0.500000 aae_loss_1:0.014777 aae_loss_2:0.014791\n",
      "epoch:169 loss:7.971202 accu:0.500000 aae_loss_1:0.014082 aae_loss_2:0.014096\n",
      "epoch:170 loss:7.971197 accu:0.500000 aae_loss_1:0.013386 aae_loss_2:0.013399\n",
      "epoch:171 loss:7.971197 accu:0.500000 aae_loss_1:0.013643 aae_loss_2:0.013656\n",
      "epoch:172 loss:7.971201 accu:0.500000 aae_loss_1:0.014696 aae_loss_2:0.014711\n",
      "epoch:173 loss:7.971197 accu:0.500000 aae_loss_1:0.014073 aae_loss_2:0.014087\n",
      "epoch:174 loss:7.971198 accu:0.500000 aae_loss_1:0.014890 aae_loss_2:0.014905\n",
      "epoch:175 loss:7.971201 accu:0.500000 aae_loss_1:0.015238 aae_loss_2:0.015253\n",
      "epoch:176 loss:7.971201 accu:0.500000 aae_loss_1:0.015174 aae_loss_2:0.015189\n",
      "epoch:177 loss:7.971198 accu:0.500000 aae_loss_1:0.015984 aae_loss_2:0.016000\n",
      "epoch:178 loss:7.971198 accu:0.500000 aae_loss_1:0.015804 aae_loss_2:0.015820\n",
      "epoch:179 loss:7.971206 accu:0.500000 aae_loss_1:0.014932 aae_loss_2:0.014947\n",
      "epoch:180 loss:7.971199 accu:0.500000 aae_loss_1:0.014254 aae_loss_2:0.014268\n",
      "epoch:181 loss:7.971197 accu:0.500000 aae_loss_1:0.013057 aae_loss_2:0.013070\n",
      "epoch:182 loss:7.971202 accu:0.500000 aae_loss_1:0.013973 aae_loss_2:0.013987\n",
      "epoch:183 loss:7.971200 accu:0.500000 aae_loss_1:0.013975 aae_loss_2:0.013989\n",
      "epoch:184 loss:7.971197 accu:0.500000 aae_loss_1:0.013975 aae_loss_2:0.013989\n",
      "epoch:185 loss:7.971204 accu:0.500000 aae_loss_1:0.012965 aae_loss_2:0.012978\n",
      "epoch:186 loss:7.971199 accu:0.500000 aae_loss_1:0.012397 aae_loss_2:0.012410\n",
      "epoch:187 loss:7.971198 accu:0.500000 aae_loss_1:0.013012 aae_loss_2:0.013026\n",
      "epoch:188 loss:7.971200 accu:0.500000 aae_loss_1:0.012246 aae_loss_2:0.012259\n",
      "epoch:189 loss:7.971201 accu:0.500000 aae_loss_1:0.012929 aae_loss_2:0.012942\n",
      "epoch:190 loss:7.971202 accu:0.500000 aae_loss_1:0.013184 aae_loss_2:0.013198\n",
      "epoch:191 loss:7.971200 accu:0.500000 aae_loss_1:0.014301 aae_loss_2:0.014315\n",
      "epoch:192 loss:7.971196 accu:0.500000 aae_loss_1:0.013969 aae_loss_2:0.013983\n",
      "epoch:193 loss:7.971201 accu:0.500000 aae_loss_1:0.013837 aae_loss_2:0.013850\n",
      "epoch:194 loss:7.971199 accu:0.500000 aae_loss_1:0.015021 aae_loss_2:0.015036\n",
      "epoch:195 loss:7.971199 accu:0.500000 aae_loss_1:0.014474 aae_loss_2:0.014488\n",
      "epoch:196 loss:7.971203 accu:0.500000 aae_loss_1:0.014298 aae_loss_2:0.014312\n",
      "epoch:197 loss:7.971199 accu:0.500000 aae_loss_1:0.014234 aae_loss_2:0.014248\n",
      "epoch:198 loss:7.971199 accu:0.500000 aae_loss_1:0.012727 aae_loss_2:0.012740\n",
      "epoch:199 loss:7.971210 accu:0.500000 aae_loss_1:0.014001 aae_loss_2:0.014015\n",
      "epoch:200 loss:7.971196 accu:0.500000 aae_loss_1:0.014742 aae_loss_2:0.014757\n",
      "epoch:201 loss:7.971202 accu:0.500000 aae_loss_1:0.013353 aae_loss_2:0.013366\n",
      "epoch:202 loss:7.971199 accu:0.500000 aae_loss_1:0.014593 aae_loss_2:0.014608\n",
      "epoch:203 loss:7.971205 accu:0.500000 aae_loss_1:0.014016 aae_loss_2:0.014030\n",
      "epoch:204 loss:7.971199 accu:0.500000 aae_loss_1:0.014036 aae_loss_2:0.014050\n",
      "epoch:205 loss:7.971197 accu:0.500000 aae_loss_1:0.014593 aae_loss_2:0.014608\n",
      "epoch:206 loss:7.971201 accu:0.500000 aae_loss_1:0.014016 aae_loss_2:0.014030\n",
      "epoch:207 loss:7.971199 accu:0.500000 aae_loss_1:0.014248 aae_loss_2:0.014262\n",
      "epoch:208 loss:7.971197 accu:0.500000 aae_loss_1:0.015545 aae_loss_2:0.015561\n",
      "epoch:209 loss:7.971200 accu:0.500000 aae_loss_1:0.014301 aae_loss_2:0.014315\n",
      "epoch:210 loss:7.971198 accu:0.500000 aae_loss_1:0.015670 aae_loss_2:0.015685\n",
      "epoch:211 loss:7.971198 accu:0.500000 aae_loss_1:0.015474 aae_loss_2:0.015489\n",
      "epoch:212 loss:7.971200 accu:0.500000 aae_loss_1:0.014374 aae_loss_2:0.014388\n",
      "epoch:213 loss:7.971198 accu:0.500000 aae_loss_1:0.014530 aae_loss_2:0.014545\n",
      "epoch:214 loss:7.971206 accu:0.500000 aae_loss_1:0.015118 aae_loss_2:0.015133\n",
      "epoch:215 loss:7.971200 accu:0.500000 aae_loss_1:0.014579 aae_loss_2:0.014593\n",
      "epoch:216 loss:7.971199 accu:0.500000 aae_loss_1:0.013586 aae_loss_2:0.013600\n",
      "epoch:217 loss:7.971199 accu:0.500000 aae_loss_1:0.013293 aae_loss_2:0.013306\n",
      "epoch:218 loss:7.971200 accu:0.500000 aae_loss_1:0.014122 aae_loss_2:0.014136\n",
      "epoch:219 loss:7.971196 accu:0.500000 aae_loss_1:0.013703 aae_loss_2:0.013717\n",
      "epoch:220 loss:7.971199 accu:0.500000 aae_loss_1:0.013306 aae_loss_2:0.013319\n",
      "epoch:221 loss:7.971196 accu:0.500000 aae_loss_1:0.013274 aae_loss_2:0.013288\n",
      "epoch:222 loss:7.971196 accu:0.500000 aae_loss_1:0.012678 aae_loss_2:0.012691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:223 loss:7.971201 accu:0.500000 aae_loss_1:0.014890 aae_loss_2:0.014905\n",
      "epoch:224 loss:7.971200 accu:0.500000 aae_loss_1:0.014604 aae_loss_2:0.014619\n",
      "epoch:225 loss:7.971198 accu:0.500000 aae_loss_1:0.016705 aae_loss_2:0.016721\n",
      "epoch:226 loss:7.971198 accu:0.500000 aae_loss_1:0.017267 aae_loss_2:0.017284\n",
      "epoch:227 loss:7.971200 accu:0.500000 aae_loss_1:0.015636 aae_loss_2:0.015652\n",
      "epoch:228 loss:7.971201 accu:0.500000 aae_loss_1:0.014738 aae_loss_2:0.014752\n",
      "epoch:229 loss:7.971201 accu:0.500000 aae_loss_1:0.014096 aae_loss_2:0.014110\n",
      "epoch:230 loss:7.971199 accu:0.500000 aae_loss_1:0.013488 aae_loss_2:0.013501\n",
      "epoch:231 loss:7.971197 accu:0.500000 aae_loss_1:0.014178 aae_loss_2:0.014192\n",
      "epoch:232 loss:7.971196 accu:0.500000 aae_loss_1:0.013060 aae_loss_2:0.013073\n",
      "epoch:233 loss:7.971200 accu:0.500000 aae_loss_1:0.013712 aae_loss_2:0.013725\n",
      "epoch:234 loss:7.971199 accu:0.500000 aae_loss_1:0.014215 aae_loss_2:0.014229\n",
      "epoch:235 loss:7.971206 accu:0.500000 aae_loss_1:0.013761 aae_loss_2:0.013775\n",
      "epoch:236 loss:7.971198 accu:0.500000 aae_loss_1:0.014870 aae_loss_2:0.014885\n",
      "epoch:237 loss:7.971197 accu:0.500000 aae_loss_1:0.015224 aae_loss_2:0.015239\n",
      "epoch:238 loss:7.971208 accu:0.500000 aae_loss_1:0.015442 aae_loss_2:0.015458\n",
      "epoch:239 loss:7.971206 accu:0.500000 aae_loss_1:0.015073 aae_loss_2:0.015088\n",
      "epoch:240 loss:7.971200 accu:0.500000 aae_loss_1:0.014447 aae_loss_2:0.014462\n",
      "epoch:241 loss:7.971205 accu:0.500000 aae_loss_1:0.014019 aae_loss_2:0.014033\n",
      "epoch:242 loss:7.971200 accu:0.500000 aae_loss_1:0.012789 aae_loss_2:0.012802\n",
      "epoch:243 loss:7.971201 accu:0.500000 aae_loss_1:0.014112 aae_loss_2:0.014126\n",
      "epoch:244 loss:7.971205 accu:0.500000 aae_loss_1:0.014931 aae_loss_2:0.014946\n",
      "epoch:245 loss:7.971199 accu:0.500000 aae_loss_1:0.012613 aae_loss_2:0.012626\n",
      "epoch:246 loss:7.971199 accu:0.500000 aae_loss_1:0.013315 aae_loss_2:0.013328\n",
      "epoch:247 loss:7.971201 accu:0.500000 aae_loss_1:0.014144 aae_loss_2:0.014158\n",
      "epoch:248 loss:7.971198 accu:0.500000 aae_loss_1:0.014371 aae_loss_2:0.014385\n",
      "epoch:249 loss:7.971202 accu:0.500000 aae_loss_1:0.013738 aae_loss_2:0.013751\n",
      "epoch:250 loss:7.971197 accu:0.500000 aae_loss_1:0.013820 aae_loss_2:0.013834\n",
      "epoch:251 loss:7.971200 accu:0.500000 aae_loss_1:0.013975 aae_loss_2:0.013989\n",
      "epoch:252 loss:7.971198 accu:0.500000 aae_loss_1:0.013549 aae_loss_2:0.013562\n",
      "epoch:253 loss:7.971200 accu:0.500000 aae_loss_1:0.012642 aae_loss_2:0.012654\n",
      "epoch:254 loss:7.971197 accu:0.500000 aae_loss_1:0.013789 aae_loss_2:0.013803\n",
      "epoch:255 loss:7.971198 accu:0.500000 aae_loss_1:0.012458 aae_loss_2:0.012471\n",
      "epoch:256 loss:7.971197 accu:0.500000 aae_loss_1:0.014059 aae_loss_2:0.014073\n",
      "epoch:257 loss:7.971203 accu:0.500000 aae_loss_1:0.013535 aae_loss_2:0.013548\n",
      "epoch:258 loss:7.971195 accu:0.500000 aae_loss_1:0.013757 aae_loss_2:0.013770\n",
      "epoch:259 loss:7.971200 accu:0.500000 aae_loss_1:0.014072 aae_loss_2:0.014086\n",
      "epoch:260 loss:7.971197 accu:0.500000 aae_loss_1:0.013192 aae_loss_2:0.013205\n",
      "epoch:261 loss:7.971201 accu:0.500000 aae_loss_1:0.014780 aae_loss_2:0.014795\n",
      "epoch:262 loss:7.971212 accu:0.500000 aae_loss_1:0.012773 aae_loss_2:0.012786\n",
      "epoch:263 loss:7.971202 accu:0.500000 aae_loss_1:0.013809 aae_loss_2:0.013823\n",
      "epoch:264 loss:7.971199 accu:0.500000 aae_loss_1:0.013393 aae_loss_2:0.013407\n",
      "epoch:265 loss:7.971198 accu:0.500000 aae_loss_1:0.013650 aae_loss_2:0.013663\n",
      "epoch:266 loss:7.971199 accu:0.500000 aae_loss_1:0.012748 aae_loss_2:0.012761\n",
      "epoch:267 loss:7.971205 accu:0.500000 aae_loss_1:0.014204 aae_loss_2:0.014218\n",
      "epoch:268 loss:7.971196 accu:0.500000 aae_loss_1:0.014422 aae_loss_2:0.014436\n",
      "epoch:269 loss:7.971202 accu:0.500000 aae_loss_1:0.015297 aae_loss_2:0.015312\n",
      "epoch:270 loss:7.971204 accu:0.500000 aae_loss_1:0.014991 aae_loss_2:0.015006\n",
      "epoch:271 loss:7.971197 accu:0.500000 aae_loss_1:0.015082 aae_loss_2:0.015097\n",
      "epoch:272 loss:7.971200 accu:0.500000 aae_loss_1:0.014312 aae_loss_2:0.014326\n",
      "epoch:273 loss:7.971200 accu:0.500000 aae_loss_1:0.013758 aae_loss_2:0.013771\n",
      "epoch:274 loss:7.971210 accu:0.500000 aae_loss_1:0.013436 aae_loss_2:0.013449\n",
      "epoch:275 loss:7.971201 accu:0.500000 aae_loss_1:0.014892 aae_loss_2:0.014907\n",
      "epoch:276 loss:7.971208 accu:0.500000 aae_loss_1:0.014058 aae_loss_2:0.014073\n",
      "epoch:277 loss:7.971199 accu:0.500000 aae_loss_1:0.012964 aae_loss_2:0.012977\n",
      "epoch:278 loss:7.971196 accu:0.500000 aae_loss_1:0.013432 aae_loss_2:0.013446\n",
      "epoch:279 loss:7.971197 accu:0.500000 aae_loss_1:0.014173 aae_loss_2:0.014188\n",
      "epoch:280 loss:7.971198 accu:0.500000 aae_loss_1:0.015915 aae_loss_2:0.015931\n",
      "epoch:281 loss:7.971200 accu:0.500000 aae_loss_1:0.013041 aae_loss_2:0.013054\n",
      "epoch:282 loss:7.971200 accu:0.500000 aae_loss_1:0.013922 aae_loss_2:0.013936\n",
      "epoch:283 loss:7.971199 accu:0.500000 aae_loss_1:0.013083 aae_loss_2:0.013096\n",
      "epoch:284 loss:7.971198 accu:0.500000 aae_loss_1:0.013118 aae_loss_2:0.013131\n",
      "epoch:285 loss:7.971197 accu:0.500000 aae_loss_1:0.012472 aae_loss_2:0.012484\n",
      "epoch:286 loss:7.971199 accu:0.500000 aae_loss_1:0.012884 aae_loss_2:0.012897\n",
      "epoch:287 loss:7.971198 accu:0.500000 aae_loss_1:0.012804 aae_loss_2:0.012816\n",
      "epoch:288 loss:7.971203 accu:0.500000 aae_loss_1:0.014230 aae_loss_2:0.014244\n",
      "epoch:289 loss:7.971207 accu:0.500000 aae_loss_1:0.014239 aae_loss_2:0.014253\n",
      "epoch:290 loss:7.971197 accu:0.500000 aae_loss_1:0.014285 aae_loss_2:0.014299\n",
      "epoch:291 loss:7.971198 accu:0.500000 aae_loss_1:0.014041 aae_loss_2:0.014055\n",
      "epoch:292 loss:7.971203 accu:0.500000 aae_loss_1:0.012746 aae_loss_2:0.012759\n",
      "epoch:293 loss:7.971197 accu:0.500000 aae_loss_1:0.012956 aae_loss_2:0.012969\n",
      "epoch:294 loss:7.971201 accu:0.500000 aae_loss_1:0.013945 aae_loss_2:0.013959\n",
      "epoch:295 loss:7.971197 accu:0.500000 aae_loss_1:0.014886 aae_loss_2:0.014901\n",
      "epoch:296 loss:7.971198 accu:0.500000 aae_loss_1:0.014781 aae_loss_2:0.014796\n",
      "epoch:297 loss:7.971198 accu:0.500000 aae_loss_1:0.014997 aae_loss_2:0.015012\n",
      "epoch:298 loss:7.971197 accu:0.500000 aae_loss_1:0.014932 aae_loss_2:0.014947\n",
      "epoch:299 loss:7.971200 accu:0.500000 aae_loss_1:0.014419 aae_loss_2:0.014433\n",
      "epoch:300 loss:7.971196 accu:0.500000 aae_loss_1:0.015831 aae_loss_2:0.015847\n",
      "epoch:301 loss:7.971197 accu:0.500000 aae_loss_1:0.013769 aae_loss_2:0.013783\n",
      "epoch:302 loss:7.971202 accu:0.500000 aae_loss_1:0.014759 aae_loss_2:0.014773\n",
      "epoch:303 loss:7.971198 accu:0.500000 aae_loss_1:0.014560 aae_loss_2:0.014575\n",
      "epoch:304 loss:7.971198 accu:0.500000 aae_loss_1:0.014248 aae_loss_2:0.014262\n",
      "epoch:305 loss:7.971198 accu:0.500000 aae_loss_1:0.015054 aae_loss_2:0.015069\n",
      "epoch:306 loss:7.971199 accu:0.500000 aae_loss_1:0.013782 aae_loss_2:0.013796\n",
      "epoch:307 loss:7.971198 accu:0.500000 aae_loss_1:0.014138 aae_loss_2:0.014152\n",
      "epoch:308 loss:7.971199 accu:0.500000 aae_loss_1:0.012437 aae_loss_2:0.012450\n",
      "epoch:309 loss:7.971197 accu:0.500000 aae_loss_1:0.012644 aae_loss_2:0.012657\n",
      "epoch:310 loss:7.971200 accu:0.500000 aae_loss_1:0.012559 aae_loss_2:0.012572\n",
      "epoch:311 loss:7.971202 accu:0.500000 aae_loss_1:0.013090 aae_loss_2:0.013103\n",
      "epoch:312 loss:7.971198 accu:0.500000 aae_loss_1:0.013091 aae_loss_2:0.013104\n",
      "epoch:313 loss:7.971195 accu:0.500000 aae_loss_1:0.013688 aae_loss_2:0.013702\n",
      "epoch:314 loss:7.971197 accu:0.500000 aae_loss_1:0.014881 aae_loss_2:0.014896\n",
      "epoch:315 loss:7.971200 accu:0.500000 aae_loss_1:0.015660 aae_loss_2:0.015676\n",
      "epoch:316 loss:7.971199 accu:0.500000 aae_loss_1:0.014237 aae_loss_2:0.014251\n",
      "epoch:317 loss:7.971198 accu:0.500000 aae_loss_1:0.013299 aae_loss_2:0.013312\n",
      "epoch:318 loss:7.971200 accu:0.500000 aae_loss_1:0.013842 aae_loss_2:0.013856\n",
      "epoch:319 loss:7.971201 accu:0.500000 aae_loss_1:0.013326 aae_loss_2:0.013339\n",
      "epoch:320 loss:7.971199 accu:0.500000 aae_loss_1:0.013187 aae_loss_2:0.013201\n",
      "epoch:321 loss:7.971204 accu:0.500000 aae_loss_1:0.013175 aae_loss_2:0.013188\n",
      "epoch:322 loss:7.971199 accu:0.500000 aae_loss_1:0.012460 aae_loss_2:0.012473\n",
      "epoch:323 loss:7.971198 accu:0.500000 aae_loss_1:0.013358 aae_loss_2:0.013371\n",
      "epoch:324 loss:7.971200 accu:0.500000 aae_loss_1:0.012576 aae_loss_2:0.012588\n",
      "epoch:325 loss:7.971198 accu:0.500000 aae_loss_1:0.013044 aae_loss_2:0.013057\n",
      "epoch:326 loss:7.971200 accu:0.500000 aae_loss_1:0.012809 aae_loss_2:0.012822\n",
      "epoch:327 loss:7.971202 accu:0.500000 aae_loss_1:0.015202 aae_loss_2:0.015217\n",
      "epoch:328 loss:7.971197 accu:0.500000 aae_loss_1:0.013851 aae_loss_2:0.013865\n",
      "epoch:329 loss:7.971198 accu:0.500000 aae_loss_1:0.014337 aae_loss_2:0.014352\n",
      "epoch:330 loss:7.971198 accu:0.500000 aae_loss_1:0.012057 aae_loss_2:0.012069\n",
      "epoch:331 loss:7.971198 accu:0.500000 aae_loss_1:0.013575 aae_loss_2:0.013588\n",
      "epoch:332 loss:7.971197 accu:0.500000 aae_loss_1:0.012883 aae_loss_2:0.012896\n",
      "epoch:333 loss:7.971197 accu:0.500000 aae_loss_1:0.012787 aae_loss_2:0.012799\n",
      "epoch:334 loss:7.971199 accu:0.500000 aae_loss_1:0.013770 aae_loss_2:0.013783\n",
      "epoch:335 loss:7.971200 accu:0.500000 aae_loss_1:0.013471 aae_loss_2:0.013485\n",
      "epoch:336 loss:7.971200 accu:0.500000 aae_loss_1:0.013195 aae_loss_2:0.013208\n",
      "epoch:337 loss:7.971196 accu:0.500000 aae_loss_1:0.013453 aae_loss_2:0.013466\n",
      "epoch:338 loss:7.971202 accu:0.500000 aae_loss_1:0.012893 aae_loss_2:0.012906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:339 loss:7.971199 accu:0.500000 aae_loss_1:0.014452 aae_loss_2:0.014466\n",
      "epoch:340 loss:7.971197 accu:0.500000 aae_loss_1:0.014784 aae_loss_2:0.014799\n",
      "epoch:341 loss:7.971212 accu:0.500000 aae_loss_1:0.015562 aae_loss_2:0.015577\n",
      "epoch:342 loss:7.971198 accu:0.500000 aae_loss_1:0.016732 aae_loss_2:0.016749\n",
      "epoch:343 loss:7.971198 accu:0.500000 aae_loss_1:0.015579 aae_loss_2:0.015595\n",
      "epoch:344 loss:7.971198 accu:0.500000 aae_loss_1:0.016219 aae_loss_2:0.016235\n",
      "epoch:345 loss:7.971197 accu:0.500000 aae_loss_1:0.015448 aae_loss_2:0.015463\n",
      "epoch:346 loss:7.971200 accu:0.500000 aae_loss_1:0.015617 aae_loss_2:0.015633\n",
      "epoch:347 loss:7.971199 accu:0.500000 aae_loss_1:0.015226 aae_loss_2:0.015241\n",
      "epoch:348 loss:7.971201 accu:0.500000 aae_loss_1:0.013246 aae_loss_2:0.013260\n",
      "epoch:349 loss:7.971196 accu:0.500000 aae_loss_1:0.012904 aae_loss_2:0.012917\n",
      "epoch:350 loss:7.971200 accu:0.500000 aae_loss_1:0.012069 aae_loss_2:0.012081\n",
      "epoch:351 loss:7.971207 accu:0.500000 aae_loss_1:0.012892 aae_loss_2:0.012905\n",
      "epoch:352 loss:7.971214 accu:0.500000 aae_loss_1:0.013602 aae_loss_2:0.013615\n",
      "epoch:353 loss:7.971199 accu:0.500000 aae_loss_1:0.013349 aae_loss_2:0.013362\n",
      "epoch:354 loss:7.971196 accu:0.500000 aae_loss_1:0.013503 aae_loss_2:0.013516\n",
      "epoch:355 loss:7.971196 accu:0.500000 aae_loss_1:0.013315 aae_loss_2:0.013328\n",
      "epoch:356 loss:7.971198 accu:0.500000 aae_loss_1:0.013571 aae_loss_2:0.013584\n",
      "epoch:357 loss:7.971200 accu:0.500000 aae_loss_1:0.014616 aae_loss_2:0.014631\n",
      "epoch:358 loss:7.971198 accu:0.500000 aae_loss_1:0.015338 aae_loss_2:0.015354\n",
      "epoch:359 loss:7.971196 accu:0.500000 aae_loss_1:0.014059 aae_loss_2:0.014073\n",
      "epoch:360 loss:7.971200 accu:0.500000 aae_loss_1:0.015133 aae_loss_2:0.015148\n",
      "epoch:361 loss:7.971199 accu:0.500000 aae_loss_1:0.014787 aae_loss_2:0.014801\n",
      "epoch:362 loss:7.971205 accu:0.500000 aae_loss_1:0.013415 aae_loss_2:0.013428\n",
      "epoch:363 loss:7.971202 accu:0.500000 aae_loss_1:0.013957 aae_loss_2:0.013971\n",
      "epoch:364 loss:7.971196 accu:0.500000 aae_loss_1:0.013792 aae_loss_2:0.013805\n",
      "epoch:365 loss:7.971197 accu:0.500000 aae_loss_1:0.012460 aae_loss_2:0.012473\n",
      "epoch:366 loss:7.971199 accu:0.500000 aae_loss_1:0.013708 aae_loss_2:0.013722\n",
      "epoch:367 loss:7.971199 accu:0.500000 aae_loss_1:0.014874 aae_loss_2:0.014888\n",
      "epoch:368 loss:7.971196 accu:0.500000 aae_loss_1:0.012360 aae_loss_2:0.012372\n",
      "epoch:369 loss:7.971212 accu:0.500000 aae_loss_1:0.011954 aae_loss_2:0.011966\n",
      "epoch:370 loss:7.971196 accu:0.500000 aae_loss_1:0.011994 aae_loss_2:0.012006\n",
      "epoch:371 loss:7.971197 accu:0.500000 aae_loss_1:0.013494 aae_loss_2:0.013507\n",
      "epoch:372 loss:7.971199 accu:0.500000 aae_loss_1:0.014469 aae_loss_2:0.014484\n",
      "epoch:373 loss:7.971198 accu:0.500000 aae_loss_1:0.013883 aae_loss_2:0.013897\n",
      "epoch:374 loss:7.971198 accu:0.500000 aae_loss_1:0.014669 aae_loss_2:0.014684\n",
      "epoch:375 loss:7.971196 accu:0.500000 aae_loss_1:0.014640 aae_loss_2:0.014654\n",
      "epoch:376 loss:7.971196 accu:0.500000 aae_loss_1:0.015389 aae_loss_2:0.015404\n",
      "epoch:377 loss:7.971200 accu:0.500000 aae_loss_1:0.015046 aae_loss_2:0.015061\n",
      "epoch:378 loss:7.971200 accu:0.500000 aae_loss_1:0.015000 aae_loss_2:0.015015\n",
      "epoch:379 loss:7.971196 accu:0.500000 aae_loss_1:0.016025 aae_loss_2:0.016042\n",
      "epoch:380 loss:7.971205 accu:0.500000 aae_loss_1:0.015100 aae_loss_2:0.015115\n",
      "epoch:381 loss:7.971197 accu:0.500000 aae_loss_1:0.015426 aae_loss_2:0.015441\n",
      "epoch:382 loss:7.971200 accu:0.500000 aae_loss_1:0.015547 aae_loss_2:0.015563\n",
      "epoch:383 loss:7.971200 accu:0.500000 aae_loss_1:0.014654 aae_loss_2:0.014669\n",
      "epoch:384 loss:7.971198 accu:0.500000 aae_loss_1:0.013353 aae_loss_2:0.013367\n",
      "epoch:385 loss:7.971200 accu:0.500000 aae_loss_1:0.014462 aae_loss_2:0.014476\n",
      "epoch:386 loss:7.971252 accu:0.500000 aae_loss_1:0.014635 aae_loss_2:0.014650\n",
      "epoch:387 loss:7.971201 accu:0.500000 aae_loss_1:0.014709 aae_loss_2:0.014724\n",
      "epoch:388 loss:7.971203 accu:0.500000 aae_loss_1:0.013119 aae_loss_2:0.013132\n",
      "epoch:389 loss:7.971197 accu:0.500000 aae_loss_1:0.013662 aae_loss_2:0.013675\n",
      "epoch:390 loss:7.971197 accu:0.500000 aae_loss_1:0.012519 aae_loss_2:0.012531\n",
      "epoch:391 loss:7.971198 accu:0.500000 aae_loss_1:0.012949 aae_loss_2:0.012962\n",
      "epoch:392 loss:7.971200 accu:0.500000 aae_loss_1:0.012996 aae_loss_2:0.013009\n",
      "epoch:393 loss:7.971198 accu:0.500000 aae_loss_1:0.013704 aae_loss_2:0.013718\n",
      "epoch:394 loss:7.971202 accu:0.500000 aae_loss_1:0.013680 aae_loss_2:0.013694\n",
      "epoch:395 loss:7.971197 accu:0.500000 aae_loss_1:0.014552 aae_loss_2:0.014566\n",
      "epoch:396 loss:7.971197 accu:0.500000 aae_loss_1:0.013229 aae_loss_2:0.013242\n",
      "epoch:397 loss:7.971201 accu:0.500000 aae_loss_1:0.014302 aae_loss_2:0.014316\n",
      "epoch:398 loss:7.971198 accu:0.500000 aae_loss_1:0.013223 aae_loss_2:0.013236\n",
      "epoch:399 loss:7.971197 accu:0.500000 aae_loss_1:0.014315 aae_loss_2:0.014329\n",
      "epoch:400 loss:7.971196 accu:0.500000 aae_loss_1:0.014758 aae_loss_2:0.014773\n",
      "epoch:401 loss:7.971203 accu:0.500000 aae_loss_1:0.014485 aae_loss_2:0.014499\n",
      "epoch:402 loss:7.971197 accu:0.500000 aae_loss_1:0.014112 aae_loss_2:0.014127\n",
      "epoch:403 loss:7.971199 accu:0.500000 aae_loss_1:0.015291 aae_loss_2:0.015306\n",
      "epoch:404 loss:7.971197 accu:0.500000 aae_loss_1:0.015343 aae_loss_2:0.015358\n",
      "epoch:405 loss:7.971199 accu:0.500000 aae_loss_1:0.014838 aae_loss_2:0.014853\n",
      "epoch:406 loss:7.971201 accu:0.500000 aae_loss_1:0.014629 aae_loss_2:0.014644\n",
      "epoch:407 loss:7.971199 accu:0.500000 aae_loss_1:0.014414 aae_loss_2:0.014428\n",
      "epoch:408 loss:7.971200 accu:0.500000 aae_loss_1:0.013061 aae_loss_2:0.013074\n",
      "epoch:409 loss:7.971199 accu:0.500000 aae_loss_1:0.013222 aae_loss_2:0.013236\n",
      "epoch:410 loss:7.971197 accu:0.500000 aae_loss_1:0.012225 aae_loss_2:0.012238\n",
      "epoch:411 loss:7.971200 accu:0.500000 aae_loss_1:0.013127 aae_loss_2:0.013140\n",
      "epoch:412 loss:7.971199 accu:0.500000 aae_loss_1:0.012898 aae_loss_2:0.012911\n",
      "epoch:413 loss:7.971199 accu:0.500000 aae_loss_1:0.013002 aae_loss_2:0.013015\n",
      "epoch:414 loss:7.971199 accu:0.500000 aae_loss_1:0.013213 aae_loss_2:0.013226\n",
      "epoch:415 loss:7.971201 accu:0.500000 aae_loss_1:0.013594 aae_loss_2:0.013607\n",
      "epoch:416 loss:7.971196 accu:0.500000 aae_loss_1:0.013939 aae_loss_2:0.013953\n",
      "epoch:417 loss:7.971196 accu:0.500000 aae_loss_1:0.012446 aae_loss_2:0.012459\n",
      "epoch:418 loss:7.971198 accu:0.500000 aae_loss_1:0.013721 aae_loss_2:0.013735\n",
      "epoch:419 loss:7.971197 accu:0.500000 aae_loss_1:0.013253 aae_loss_2:0.013266\n",
      "epoch:420 loss:7.971196 accu:0.500000 aae_loss_1:0.014142 aae_loss_2:0.014156\n",
      "epoch:421 loss:7.971197 accu:0.500000 aae_loss_1:0.013407 aae_loss_2:0.013420\n",
      "epoch:422 loss:7.971199 accu:0.500000 aae_loss_1:0.014694 aae_loss_2:0.014708\n",
      "epoch:423 loss:7.971197 accu:0.500000 aae_loss_1:0.014106 aae_loss_2:0.014120\n",
      "epoch:424 loss:7.971200 accu:0.500000 aae_loss_1:0.014638 aae_loss_2:0.014652\n",
      "epoch:425 loss:7.971196 accu:0.500000 aae_loss_1:0.014121 aae_loss_2:0.014135\n",
      "epoch:426 loss:7.971196 accu:0.500000 aae_loss_1:0.014853 aae_loss_2:0.014868\n",
      "epoch:427 loss:7.971197 accu:0.500000 aae_loss_1:0.014844 aae_loss_2:0.014859\n",
      "epoch:428 loss:7.971198 accu:0.500000 aae_loss_1:0.015888 aae_loss_2:0.015904\n",
      "epoch:429 loss:7.971196 accu:0.500000 aae_loss_1:0.016103 aae_loss_2:0.016119\n",
      "epoch:430 loss:7.971197 accu:0.500000 aae_loss_1:0.015118 aae_loss_2:0.015133\n",
      "epoch:431 loss:7.971200 accu:0.500000 aae_loss_1:0.014810 aae_loss_2:0.014824\n",
      "epoch:432 loss:7.971203 accu:0.500000 aae_loss_1:0.012915 aae_loss_2:0.012928\n",
      "epoch:433 loss:7.971216 accu:0.500000 aae_loss_1:0.013460 aae_loss_2:0.013473\n",
      "epoch:434 loss:7.971196 accu:0.500000 aae_loss_1:0.012777 aae_loss_2:0.012789\n",
      "epoch:435 loss:7.971197 accu:0.500000 aae_loss_1:0.012143 aae_loss_2:0.012155\n",
      "epoch:436 loss:7.971199 accu:0.500000 aae_loss_1:0.013629 aae_loss_2:0.013643\n",
      "epoch:437 loss:7.971206 accu:0.500000 aae_loss_1:0.013321 aae_loss_2:0.013334\n",
      "epoch:438 loss:7.971195 accu:0.500000 aae_loss_1:0.013068 aae_loss_2:0.013081\n",
      "epoch:439 loss:7.971197 accu:0.500000 aae_loss_1:0.013287 aae_loss_2:0.013300\n",
      "epoch:440 loss:7.971200 accu:0.500000 aae_loss_1:0.013549 aae_loss_2:0.013562\n",
      "epoch:441 loss:7.971198 accu:0.500000 aae_loss_1:0.013235 aae_loss_2:0.013249\n",
      "epoch:442 loss:7.971198 accu:0.500000 aae_loss_1:0.012797 aae_loss_2:0.012810\n",
      "epoch:443 loss:7.971198 accu:0.500000 aae_loss_1:0.012873 aae_loss_2:0.012886\n",
      "epoch:444 loss:7.971198 accu:0.500000 aae_loss_1:0.013761 aae_loss_2:0.013774\n",
      "epoch:445 loss:7.971196 accu:0.500000 aae_loss_1:0.013537 aae_loss_2:0.013551\n",
      "epoch:446 loss:7.971197 accu:0.500000 aae_loss_1:0.013210 aae_loss_2:0.013224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:447 loss:7.971198 accu:0.500000 aae_loss_1:0.014537 aae_loss_2:0.014552\n",
      "epoch:448 loss:7.971198 accu:0.500000 aae_loss_1:0.013689 aae_loss_2:0.013702\n",
      "epoch:449 loss:7.971199 accu:0.500000 aae_loss_1:0.014215 aae_loss_2:0.014230\n",
      "epoch:450 loss:7.971204 accu:0.500000 aae_loss_1:0.013894 aae_loss_2:0.013907\n",
      "epoch:451 loss:7.971197 accu:0.500000 aae_loss_1:0.014584 aae_loss_2:0.014598\n",
      "epoch:452 loss:7.971199 accu:0.500000 aae_loss_1:0.014967 aae_loss_2:0.014982\n",
      "epoch:453 loss:7.971198 accu:0.500000 aae_loss_1:0.012644 aae_loss_2:0.012657\n",
      "epoch:454 loss:7.971200 accu:0.500000 aae_loss_1:0.013134 aae_loss_2:0.013147\n",
      "epoch:455 loss:7.971198 accu:0.500000 aae_loss_1:0.012933 aae_loss_2:0.012946\n",
      "epoch:456 loss:7.971198 accu:0.500000 aae_loss_1:0.013067 aae_loss_2:0.013080\n",
      "epoch:457 loss:7.971200 accu:0.500000 aae_loss_1:0.013103 aae_loss_2:0.013116\n",
      "epoch:458 loss:7.971196 accu:0.500000 aae_loss_1:0.014807 aae_loss_2:0.014822\n",
      "epoch:459 loss:7.971198 accu:0.500000 aae_loss_1:0.013325 aae_loss_2:0.013338\n",
      "epoch:460 loss:7.971198 accu:0.500000 aae_loss_1:0.013441 aae_loss_2:0.013454\n",
      "epoch:461 loss:7.971196 accu:0.500000 aae_loss_1:0.014771 aae_loss_2:0.014786\n",
      "epoch:462 loss:7.971200 accu:0.500000 aae_loss_1:0.013868 aae_loss_2:0.013882\n",
      "epoch:463 loss:7.971196 accu:0.500000 aae_loss_1:0.014085 aae_loss_2:0.014099\n",
      "epoch:464 loss:7.971198 accu:0.500000 aae_loss_1:0.014879 aae_loss_2:0.014894\n",
      "epoch:465 loss:7.971196 accu:0.500000 aae_loss_1:0.014335 aae_loss_2:0.014350\n",
      "epoch:466 loss:7.971197 accu:0.500000 aae_loss_1:0.014349 aae_loss_2:0.014364\n",
      "epoch:467 loss:7.971196 accu:0.500000 aae_loss_1:0.014077 aae_loss_2:0.014091\n",
      "epoch:468 loss:7.971198 accu:0.500000 aae_loss_1:0.015434 aae_loss_2:0.015449\n",
      "epoch:469 loss:7.971197 accu:0.500000 aae_loss_1:0.015660 aae_loss_2:0.015676\n",
      "epoch:470 loss:7.971201 accu:0.500000 aae_loss_1:0.015333 aae_loss_2:0.015348\n",
      "epoch:471 loss:7.971200 accu:0.500000 aae_loss_1:0.014861 aae_loss_2:0.014876\n",
      "epoch:472 loss:7.971198 accu:0.500000 aae_loss_1:0.014399 aae_loss_2:0.014413\n",
      "epoch:473 loss:7.971196 accu:0.500000 aae_loss_1:0.013472 aae_loss_2:0.013486\n",
      "epoch:474 loss:7.971200 accu:0.500000 aae_loss_1:0.011967 aae_loss_2:0.011979\n",
      "epoch:475 loss:7.971196 accu:0.500000 aae_loss_1:0.012732 aae_loss_2:0.012745\n",
      "epoch:476 loss:7.971196 accu:0.500000 aae_loss_1:0.013524 aae_loss_2:0.013538\n",
      "epoch:477 loss:7.971199 accu:0.500000 aae_loss_1:0.013603 aae_loss_2:0.013617\n",
      "epoch:478 loss:7.971196 accu:0.500000 aae_loss_1:0.013218 aae_loss_2:0.013231\n",
      "epoch:479 loss:7.971198 accu:0.500000 aae_loss_1:0.012765 aae_loss_2:0.012777\n",
      "epoch:480 loss:7.971197 accu:0.500000 aae_loss_1:0.013057 aae_loss_2:0.013070\n",
      "epoch:481 loss:7.971198 accu:0.500000 aae_loss_1:0.013527 aae_loss_2:0.013540\n",
      "epoch:482 loss:7.971197 accu:0.500000 aae_loss_1:0.014657 aae_loss_2:0.014672\n",
      "epoch:483 loss:7.971199 accu:0.500000 aae_loss_1:0.013301 aae_loss_2:0.013314\n",
      "epoch:484 loss:7.971197 accu:0.500000 aae_loss_1:0.013038 aae_loss_2:0.013051\n",
      "epoch:485 loss:7.971197 accu:0.500000 aae_loss_1:0.014465 aae_loss_2:0.014480\n",
      "epoch:486 loss:7.971200 accu:0.500000 aae_loss_1:0.013908 aae_loss_2:0.013922\n",
      "epoch:487 loss:7.971196 accu:0.500000 aae_loss_1:0.013717 aae_loss_2:0.013730\n",
      "epoch:488 loss:7.971199 accu:0.500000 aae_loss_1:0.013892 aae_loss_2:0.013906\n",
      "epoch:489 loss:7.971201 accu:0.500000 aae_loss_1:0.013994 aae_loss_2:0.014008\n",
      "epoch:490 loss:7.971199 accu:0.500000 aae_loss_1:0.013238 aae_loss_2:0.013252\n",
      "epoch:491 loss:7.971202 accu:0.500000 aae_loss_1:0.014070 aae_loss_2:0.014084\n",
      "epoch:492 loss:7.971195 accu:0.500000 aae_loss_1:0.014202 aae_loss_2:0.014217\n",
      "epoch:493 loss:7.971200 accu:0.500000 aae_loss_1:0.013441 aae_loss_2:0.013455\n",
      "epoch:494 loss:7.971197 accu:0.500000 aae_loss_1:0.013502 aae_loss_2:0.013515\n",
      "epoch:495 loss:7.971202 accu:0.500000 aae_loss_1:0.013777 aae_loss_2:0.013791\n",
      "epoch:496 loss:7.971195 accu:0.500000 aae_loss_1:0.013983 aae_loss_2:0.013997\n",
      "epoch:497 loss:7.971201 accu:0.500000 aae_loss_1:0.016034 aae_loss_2:0.016050\n",
      "epoch:498 loss:7.971200 accu:0.500000 aae_loss_1:0.014824 aae_loss_2:0.014839\n",
      "epoch:499 loss:7.971197 accu:0.500000 aae_loss_1:0.013715 aae_loss_2:0.013729\n",
      "epoch:500 loss:7.971204 accu:0.500000 aae_loss_1:0.014837 aae_loss_2:0.014852\n",
      "epoch:501 loss:7.971196 accu:0.500000 aae_loss_1:0.013452 aae_loss_2:0.013465\n",
      "epoch:502 loss:7.971198 accu:0.500000 aae_loss_1:0.012974 aae_loss_2:0.012987\n",
      "epoch:503 loss:7.971197 accu:0.500000 aae_loss_1:0.013912 aae_loss_2:0.013926\n",
      "epoch:504 loss:7.971202 accu:0.500000 aae_loss_1:0.014278 aae_loss_2:0.014292\n",
      "epoch:505 loss:7.971199 accu:0.500000 aae_loss_1:0.013916 aae_loss_2:0.013930\n",
      "epoch:506 loss:7.971203 accu:0.500000 aae_loss_1:0.014279 aae_loss_2:0.014293\n",
      "epoch:507 loss:7.971196 accu:0.500000 aae_loss_1:0.014121 aae_loss_2:0.014135\n",
      "epoch:508 loss:7.971197 accu:0.500000 aae_loss_1:0.014284 aae_loss_2:0.014299\n",
      "epoch:509 loss:7.971201 accu:0.500000 aae_loss_1:0.014468 aae_loss_2:0.014482\n",
      "epoch:510 loss:7.971245 accu:0.500000 aae_loss_1:0.016515 aae_loss_2:0.016531\n",
      "epoch:511 loss:7.971197 accu:0.500000 aae_loss_1:0.016841 aae_loss_2:0.016857\n",
      "epoch:512 loss:7.971199 accu:0.500000 aae_loss_1:0.015258 aae_loss_2:0.015273\n",
      "epoch:513 loss:7.971200 accu:0.500000 aae_loss_1:0.014539 aae_loss_2:0.014554\n",
      "epoch:514 loss:7.971199 accu:0.500000 aae_loss_1:0.014065 aae_loss_2:0.014079\n",
      "epoch:515 loss:7.971195 accu:0.500000 aae_loss_1:0.013670 aae_loss_2:0.013683\n",
      "epoch:516 loss:7.971198 accu:0.500000 aae_loss_1:0.012387 aae_loss_2:0.012400\n",
      "epoch:517 loss:7.971197 accu:0.500000 aae_loss_1:0.014333 aae_loss_2:0.014347\n",
      "epoch:518 loss:7.971196 accu:0.500000 aae_loss_1:0.014453 aae_loss_2:0.014467\n",
      "epoch:519 loss:7.971197 accu:0.500000 aae_loss_1:0.014109 aae_loss_2:0.014123\n",
      "epoch:520 loss:7.971195 accu:0.500000 aae_loss_1:0.014622 aae_loss_2:0.014637\n",
      "epoch:521 loss:7.971197 accu:0.500000 aae_loss_1:0.015690 aae_loss_2:0.015705\n",
      "epoch:522 loss:7.971199 accu:0.500000 aae_loss_1:0.014405 aae_loss_2:0.014419\n",
      "epoch:523 loss:7.971196 accu:0.500000 aae_loss_1:0.014965 aae_loss_2:0.014980\n",
      "epoch:524 loss:7.971207 accu:0.500000 aae_loss_1:0.013989 aae_loss_2:0.014003\n",
      "epoch:525 loss:7.971196 accu:0.500000 aae_loss_1:0.013374 aae_loss_2:0.013387\n",
      "epoch:526 loss:7.971196 accu:0.500000 aae_loss_1:0.013520 aae_loss_2:0.013533\n",
      "epoch:527 loss:7.971197 accu:0.500000 aae_loss_1:0.013777 aae_loss_2:0.013791\n",
      "epoch:528 loss:7.971197 accu:0.500000 aae_loss_1:0.014604 aae_loss_2:0.014618\n",
      "epoch:529 loss:7.971201 accu:0.500000 aae_loss_1:0.014423 aae_loss_2:0.014438\n",
      "epoch:530 loss:7.971199 accu:0.500000 aae_loss_1:0.013089 aae_loss_2:0.013102\n",
      "epoch:531 loss:7.971198 accu:0.500000 aae_loss_1:0.013652 aae_loss_2:0.013665\n",
      "epoch:532 loss:7.971196 accu:0.500000 aae_loss_1:0.013345 aae_loss_2:0.013359\n",
      "epoch:533 loss:7.971198 accu:0.500000 aae_loss_1:0.013345 aae_loss_2:0.013358\n",
      "epoch:534 loss:7.971198 accu:0.500000 aae_loss_1:0.014749 aae_loss_2:0.014764\n",
      "epoch:535 loss:7.971197 accu:0.500000 aae_loss_1:0.014128 aae_loss_2:0.014143\n",
      "epoch:536 loss:7.971195 accu:0.500000 aae_loss_1:0.015562 aae_loss_2:0.015578\n",
      "epoch:537 loss:7.971197 accu:0.500000 aae_loss_1:0.014636 aae_loss_2:0.014650\n",
      "epoch:538 loss:7.971197 accu:0.500000 aae_loss_1:0.013899 aae_loss_2:0.013913\n",
      "epoch:539 loss:7.971198 accu:0.500000 aae_loss_1:0.014892 aae_loss_2:0.014907\n",
      "epoch:540 loss:7.971200 accu:0.500000 aae_loss_1:0.012612 aae_loss_2:0.012624\n",
      "epoch:541 loss:7.971200 accu:0.500000 aae_loss_1:0.013990 aae_loss_2:0.014004\n",
      "epoch:542 loss:7.971199 accu:0.500000 aae_loss_1:0.013797 aae_loss_2:0.013811\n",
      "epoch:543 loss:7.971195 accu:0.500000 aae_loss_1:0.014162 aae_loss_2:0.014177\n",
      "epoch:544 loss:7.971199 accu:0.500000 aae_loss_1:0.015321 aae_loss_2:0.015336\n",
      "epoch:545 loss:7.971203 accu:0.500000 aae_loss_1:0.013847 aae_loss_2:0.013861\n",
      "epoch:546 loss:7.971196 accu:0.500000 aae_loss_1:0.013391 aae_loss_2:0.013404\n",
      "epoch:547 loss:7.971198 accu:0.500000 aae_loss_1:0.014126 aae_loss_2:0.014140\n",
      "epoch:548 loss:7.971196 accu:0.500000 aae_loss_1:0.014004 aae_loss_2:0.014018\n",
      "epoch:549 loss:7.971197 accu:0.500000 aae_loss_1:0.013409 aae_loss_2:0.013422\n",
      "epoch:550 loss:7.971197 accu:0.500000 aae_loss_1:0.013013 aae_loss_2:0.013027\n",
      "epoch:551 loss:7.971196 accu:0.500000 aae_loss_1:0.014225 aae_loss_2:0.014239\n",
      "epoch:552 loss:7.971200 accu:0.500000 aae_loss_1:0.015161 aae_loss_2:0.015176\n",
      "epoch:553 loss:7.971197 accu:0.500000 aae_loss_1:0.015124 aae_loss_2:0.015139\n",
      "epoch:554 loss:7.971196 accu:0.500000 aae_loss_1:0.017207 aae_loss_2:0.017224\n",
      "epoch:555 loss:7.971196 accu:0.500000 aae_loss_1:0.015123 aae_loss_2:0.015138\n",
      "epoch:556 loss:7.971199 accu:0.500000 aae_loss_1:0.015831 aae_loss_2:0.015847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:557 loss:7.971200 accu:0.500000 aae_loss_1:0.014979 aae_loss_2:0.014994\n",
      "epoch:558 loss:7.971196 accu:0.500000 aae_loss_1:0.014030 aae_loss_2:0.014044\n",
      "epoch:559 loss:7.971197 accu:0.500000 aae_loss_1:0.012417 aae_loss_2:0.012429\n",
      "epoch:560 loss:7.971197 accu:0.500000 aae_loss_1:0.012936 aae_loss_2:0.012949\n",
      "epoch:561 loss:7.971199 accu:0.500000 aae_loss_1:0.013323 aae_loss_2:0.013336\n",
      "epoch:562 loss:7.971196 accu:0.500000 aae_loss_1:0.012970 aae_loss_2:0.012983\n",
      "epoch:563 loss:7.971197 accu:0.500000 aae_loss_1:0.015065 aae_loss_2:0.015080\n",
      "epoch:564 loss:7.971199 accu:0.500000 aae_loss_1:0.013734 aae_loss_2:0.013748\n",
      "epoch:565 loss:7.971198 accu:0.500000 aae_loss_1:0.013238 aae_loss_2:0.013251\n",
      "epoch:566 loss:7.971199 accu:0.500000 aae_loss_1:0.014011 aae_loss_2:0.014025\n",
      "epoch:567 loss:7.971199 accu:0.500000 aae_loss_1:0.013761 aae_loss_2:0.013775\n",
      "epoch:568 loss:7.971197 accu:0.500000 aae_loss_1:0.013456 aae_loss_2:0.013470\n",
      "epoch:569 loss:7.971205 accu:0.500000 aae_loss_1:0.011938 aae_loss_2:0.011949\n",
      "epoch:570 loss:7.971195 accu:0.500000 aae_loss_1:0.014215 aae_loss_2:0.014229\n",
      "epoch:571 loss:7.971203 accu:0.500000 aae_loss_1:0.013803 aae_loss_2:0.013817\n",
      "epoch:572 loss:7.971199 accu:0.500000 aae_loss_1:0.013158 aae_loss_2:0.013171\n",
      "epoch:573 loss:7.971195 accu:0.500000 aae_loss_1:0.014387 aae_loss_2:0.014402\n",
      "epoch:574 loss:7.971200 accu:0.500000 aae_loss_1:0.013249 aae_loss_2:0.013262\n",
      "epoch:575 loss:7.971195 accu:0.500000 aae_loss_1:0.013377 aae_loss_2:0.013390\n",
      "epoch:576 loss:7.971199 accu:0.500000 aae_loss_1:0.014018 aae_loss_2:0.014032\n",
      "epoch:577 loss:7.971195 accu:0.500000 aae_loss_1:0.014298 aae_loss_2:0.014313\n",
      "epoch:578 loss:7.971196 accu:0.500000 aae_loss_1:0.012976 aae_loss_2:0.012989\n",
      "epoch:579 loss:7.971199 accu:0.500000 aae_loss_1:0.012698 aae_loss_2:0.012711\n",
      "epoch:580 loss:7.971197 accu:0.500000 aae_loss_1:0.012513 aae_loss_2:0.012525\n",
      "epoch:581 loss:7.971200 accu:0.500000 aae_loss_1:0.012731 aae_loss_2:0.012744\n",
      "epoch:582 loss:7.971198 accu:0.500000 aae_loss_1:0.012144 aae_loss_2:0.012156\n",
      "epoch:583 loss:7.971198 accu:0.500000 aae_loss_1:0.012996 aae_loss_2:0.013009\n",
      "epoch:584 loss:7.971196 accu:0.500000 aae_loss_1:0.013073 aae_loss_2:0.013086\n",
      "epoch:585 loss:7.971198 accu:0.500000 aae_loss_1:0.014360 aae_loss_2:0.014374\n",
      "epoch:586 loss:7.971195 accu:0.500000 aae_loss_1:0.014894 aae_loss_2:0.014909\n",
      "epoch:587 loss:7.971197 accu:0.500000 aae_loss_1:0.015644 aae_loss_2:0.015660\n",
      "epoch:588 loss:7.971198 accu:0.500000 aae_loss_1:0.013998 aae_loss_2:0.014012\n",
      "epoch:589 loss:7.971197 accu:0.500000 aae_loss_1:0.014903 aae_loss_2:0.014917\n",
      "epoch:590 loss:7.971200 accu:0.500000 aae_loss_1:0.014186 aae_loss_2:0.014200\n",
      "epoch:591 loss:7.971197 accu:0.500000 aae_loss_1:0.013826 aae_loss_2:0.013840\n",
      "epoch:592 loss:7.971197 accu:0.500000 aae_loss_1:0.013242 aae_loss_2:0.013255\n",
      "epoch:593 loss:7.971196 accu:0.500000 aae_loss_1:0.013585 aae_loss_2:0.013599\n",
      "epoch:594 loss:7.971198 accu:0.500000 aae_loss_1:0.013719 aae_loss_2:0.013733\n",
      "epoch:595 loss:7.971197 accu:0.500000 aae_loss_1:0.014147 aae_loss_2:0.014161\n",
      "epoch:596 loss:7.971197 accu:0.500000 aae_loss_1:0.014710 aae_loss_2:0.014725\n",
      "epoch:597 loss:7.971196 accu:0.500000 aae_loss_1:0.013999 aae_loss_2:0.014013\n",
      "epoch:598 loss:7.971195 accu:0.500000 aae_loss_1:0.012539 aae_loss_2:0.012552\n",
      "epoch:599 loss:7.971197 accu:0.500000 aae_loss_1:0.012671 aae_loss_2:0.012684\n",
      "epoch:600 loss:7.971197 accu:0.500000 aae_loss_1:0.011863 aae_loss_2:0.011874\n",
      "epoch:601 loss:7.971200 accu:0.500000 aae_loss_1:0.013424 aae_loss_2:0.013438\n",
      "epoch:602 loss:7.971199 accu:0.500000 aae_loss_1:0.012843 aae_loss_2:0.012855\n",
      "epoch:603 loss:7.971197 accu:0.500000 aae_loss_1:0.011409 aae_loss_2:0.011421\n",
      "epoch:604 loss:7.971195 accu:0.500000 aae_loss_1:0.012873 aae_loss_2:0.012886\n",
      "epoch:605 loss:7.971199 accu:0.500000 aae_loss_1:0.012913 aae_loss_2:0.012926\n",
      "epoch:606 loss:7.971198 accu:0.500000 aae_loss_1:0.012817 aae_loss_2:0.012829\n",
      "epoch:607 loss:7.971202 accu:0.500000 aae_loss_1:0.012209 aae_loss_2:0.012221\n",
      "epoch:608 loss:7.971197 accu:0.500000 aae_loss_1:0.014394 aae_loss_2:0.014408\n",
      "epoch:609 loss:7.971199 accu:0.500000 aae_loss_1:0.014250 aae_loss_2:0.014264\n",
      "epoch:610 loss:7.971197 accu:0.500000 aae_loss_1:0.015092 aae_loss_2:0.015107\n",
      "epoch:611 loss:7.971200 accu:0.500000 aae_loss_1:0.015024 aae_loss_2:0.015039\n",
      "epoch:612 loss:7.971197 accu:0.500000 aae_loss_1:0.014914 aae_loss_2:0.014929\n",
      "epoch:613 loss:7.971196 accu:0.500000 aae_loss_1:0.014337 aae_loss_2:0.014351\n",
      "epoch:614 loss:7.971200 accu:0.500000 aae_loss_1:0.013767 aae_loss_2:0.013781\n",
      "epoch:615 loss:7.971199 accu:0.500000 aae_loss_1:0.012819 aae_loss_2:0.012832\n",
      "epoch:616 loss:7.971195 accu:0.500000 aae_loss_1:0.013753 aae_loss_2:0.013767\n",
      "epoch:617 loss:7.971195 accu:0.500000 aae_loss_1:0.014664 aae_loss_2:0.014678\n",
      "epoch:618 loss:7.971195 accu:0.500000 aae_loss_1:0.013288 aae_loss_2:0.013301\n",
      "epoch:619 loss:7.971196 accu:0.500000 aae_loss_1:0.014356 aae_loss_2:0.014370\n",
      "epoch:620 loss:7.971196 accu:0.500000 aae_loss_1:0.014185 aae_loss_2:0.014199\n",
      "epoch:621 loss:7.971198 accu:0.500000 aae_loss_1:0.014858 aae_loss_2:0.014873\n",
      "epoch:622 loss:7.971196 accu:0.500000 aae_loss_1:0.012908 aae_loss_2:0.012921\n",
      "epoch:623 loss:7.971198 accu:0.500000 aae_loss_1:0.013081 aae_loss_2:0.013094\n",
      "epoch:624 loss:7.971209 accu:0.500000 aae_loss_1:0.013139 aae_loss_2:0.013152\n",
      "epoch:625 loss:7.971196 accu:0.500000 aae_loss_1:0.013801 aae_loss_2:0.013815\n",
      "epoch:626 loss:7.971196 accu:0.500000 aae_loss_1:0.013565 aae_loss_2:0.013579\n",
      "epoch:627 loss:7.971197 accu:0.500000 aae_loss_1:0.014145 aae_loss_2:0.014159\n",
      "epoch:628 loss:7.971196 accu:0.500000 aae_loss_1:0.013707 aae_loss_2:0.013720\n",
      "epoch:629 loss:7.971197 accu:0.500000 aae_loss_1:0.013577 aae_loss_2:0.013591\n",
      "epoch:630 loss:7.971196 accu:0.500000 aae_loss_1:0.014202 aae_loss_2:0.014216\n",
      "epoch:631 loss:7.971197 accu:0.500000 aae_loss_1:0.015331 aae_loss_2:0.015346\n",
      "epoch:632 loss:7.971198 accu:0.500000 aae_loss_1:0.014928 aae_loss_2:0.014943\n",
      "epoch:633 loss:7.971199 accu:0.500000 aae_loss_1:0.014258 aae_loss_2:0.014272\n",
      "epoch:634 loss:7.971196 accu:0.500000 aae_loss_1:0.015255 aae_loss_2:0.015271\n",
      "epoch:635 loss:7.971200 accu:0.500000 aae_loss_1:0.014656 aae_loss_2:0.014670\n",
      "epoch:636 loss:7.971195 accu:0.500000 aae_loss_1:0.015662 aae_loss_2:0.015678\n",
      "epoch:637 loss:7.971202 accu:0.500000 aae_loss_1:0.013852 aae_loss_2:0.013866\n",
      "epoch:638 loss:7.971200 accu:0.500000 aae_loss_1:0.013529 aae_loss_2:0.013543\n",
      "epoch:639 loss:7.971197 accu:0.500000 aae_loss_1:0.013027 aae_loss_2:0.013040\n",
      "epoch:640 loss:7.971197 accu:0.500000 aae_loss_1:0.011863 aae_loss_2:0.011875\n",
      "epoch:641 loss:7.971196 accu:0.500000 aae_loss_1:0.012231 aae_loss_2:0.012244\n",
      "epoch:642 loss:7.971198 accu:0.500000 aae_loss_1:0.013079 aae_loss_2:0.013092\n",
      "epoch:643 loss:7.971195 accu:0.500000 aae_loss_1:0.013049 aae_loss_2:0.013062\n",
      "epoch:644 loss:7.971197 accu:0.500000 aae_loss_1:0.012457 aae_loss_2:0.012469\n",
      "epoch:645 loss:7.971195 accu:0.500000 aae_loss_1:0.012219 aae_loss_2:0.012231\n",
      "epoch:646 loss:7.971196 accu:0.500000 aae_loss_1:0.013603 aae_loss_2:0.013617\n",
      "epoch:647 loss:7.971195 accu:0.500000 aae_loss_1:0.013501 aae_loss_2:0.013514\n",
      "epoch:648 loss:7.971196 accu:0.500000 aae_loss_1:0.013361 aae_loss_2:0.013374\n",
      "epoch:649 loss:7.971197 accu:0.500000 aae_loss_1:0.012315 aae_loss_2:0.012327\n",
      "epoch:650 loss:7.971197 accu:0.500000 aae_loss_1:0.014093 aae_loss_2:0.014107\n",
      "epoch:651 loss:7.971198 accu:0.500000 aae_loss_1:0.013891 aae_loss_2:0.013905\n",
      "epoch:652 loss:7.971196 accu:0.500000 aae_loss_1:0.013481 aae_loss_2:0.013494\n",
      "epoch:653 loss:7.971197 accu:0.500000 aae_loss_1:0.012669 aae_loss_2:0.012681\n",
      "epoch:654 loss:7.971195 accu:0.500000 aae_loss_1:0.014677 aae_loss_2:0.014692\n",
      "epoch:655 loss:7.971198 accu:0.500000 aae_loss_1:0.013125 aae_loss_2:0.013138\n",
      "epoch:656 loss:7.971198 accu:0.500000 aae_loss_1:0.014115 aae_loss_2:0.014129\n",
      "epoch:657 loss:7.971196 accu:0.500000 aae_loss_1:0.015159 aae_loss_2:0.015174\n",
      "epoch:658 loss:7.971198 accu:0.500000 aae_loss_1:0.014064 aae_loss_2:0.014078\n",
      "epoch:659 loss:7.971199 accu:0.500000 aae_loss_1:0.015470 aae_loss_2:0.015486\n",
      "epoch:660 loss:7.971196 accu:0.500000 aae_loss_1:0.013987 aae_loss_2:0.014001\n",
      "epoch:661 loss:7.971199 accu:0.500000 aae_loss_1:0.013519 aae_loss_2:0.013533\n",
      "epoch:662 loss:7.971196 accu:0.500000 aae_loss_1:0.014122 aae_loss_2:0.014136\n",
      "epoch:663 loss:7.971195 accu:0.500000 aae_loss_1:0.013271 aae_loss_2:0.013285\n",
      "epoch:664 loss:7.971198 accu:0.500000 aae_loss_1:0.012659 aae_loss_2:0.012672\n",
      "epoch:665 loss:7.971197 accu:0.500000 aae_loss_1:0.012159 aae_loss_2:0.012171\n",
      "epoch:666 loss:7.971197 accu:0.500000 aae_loss_1:0.012832 aae_loss_2:0.012844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:667 loss:7.971200 accu:0.500000 aae_loss_1:0.013768 aae_loss_2:0.013781\n",
      "epoch:668 loss:7.971200 accu:0.500000 aae_loss_1:0.012065 aae_loss_2:0.012077\n",
      "epoch:669 loss:7.971204 accu:0.500000 aae_loss_1:0.013270 aae_loss_2:0.013283\n",
      "epoch:670 loss:7.971199 accu:0.500000 aae_loss_1:0.013553 aae_loss_2:0.013567\n",
      "epoch:671 loss:7.971198 accu:0.500000 aae_loss_1:0.014762 aae_loss_2:0.014776\n",
      "epoch:672 loss:7.971197 accu:0.500000 aae_loss_1:0.015282 aae_loss_2:0.015298\n",
      "epoch:673 loss:7.971198 accu:0.500000 aae_loss_1:0.014636 aae_loss_2:0.014651\n",
      "epoch:674 loss:7.971196 accu:0.500000 aae_loss_1:0.014009 aae_loss_2:0.014023\n",
      "epoch:675 loss:7.971195 accu:0.500000 aae_loss_1:0.014281 aae_loss_2:0.014295\n",
      "epoch:676 loss:7.971195 accu:0.500000 aae_loss_1:0.013548 aae_loss_2:0.013561\n",
      "epoch:677 loss:7.971196 accu:0.500000 aae_loss_1:0.015236 aae_loss_2:0.015251\n",
      "epoch:678 loss:7.971196 accu:0.500000 aae_loss_1:0.015878 aae_loss_2:0.015894\n",
      "epoch:679 loss:7.971198 accu:0.500000 aae_loss_1:0.013937 aae_loss_2:0.013951\n",
      "epoch:680 loss:7.971197 accu:0.500000 aae_loss_1:0.012057 aae_loss_2:0.012069\n",
      "epoch:681 loss:7.971196 accu:0.500000 aae_loss_1:0.012789 aae_loss_2:0.012802\n",
      "epoch:682 loss:7.971196 accu:0.500000 aae_loss_1:0.013464 aae_loss_2:0.013478\n",
      "epoch:683 loss:7.971199 accu:0.500000 aae_loss_1:0.013766 aae_loss_2:0.013780\n",
      "epoch:684 loss:7.971197 accu:0.500000 aae_loss_1:0.014189 aae_loss_2:0.014203\n",
      "epoch:685 loss:7.971195 accu:0.500000 aae_loss_1:0.013992 aae_loss_2:0.014006\n",
      "epoch:686 loss:7.971196 accu:0.500000 aae_loss_1:0.014583 aae_loss_2:0.014597\n",
      "epoch:687 loss:7.971196 accu:0.500000 aae_loss_1:0.014220 aae_loss_2:0.014234\n",
      "epoch:688 loss:7.971198 accu:0.500000 aae_loss_1:0.013582 aae_loss_2:0.013596\n",
      "epoch:689 loss:7.971198 accu:0.500000 aae_loss_1:0.012541 aae_loss_2:0.012554\n",
      "epoch:690 loss:7.971195 accu:0.500000 aae_loss_1:0.012423 aae_loss_2:0.012436\n",
      "epoch:691 loss:7.971196 accu:0.500000 aae_loss_1:0.011926 aae_loss_2:0.011938\n",
      "epoch:692 loss:7.971196 accu:0.500000 aae_loss_1:0.013436 aae_loss_2:0.013450\n",
      "epoch:693 loss:7.971196 accu:0.500000 aae_loss_1:0.014320 aae_loss_2:0.014334\n",
      "epoch:694 loss:7.971198 accu:0.500000 aae_loss_1:0.014062 aae_loss_2:0.014076\n",
      "epoch:695 loss:7.971199 accu:0.500000 aae_loss_1:0.012706 aae_loss_2:0.012719\n",
      "epoch:696 loss:7.971200 accu:0.500000 aae_loss_1:0.013348 aae_loss_2:0.013361\n",
      "epoch:697 loss:7.971196 accu:0.500000 aae_loss_1:0.012946 aae_loss_2:0.012959\n",
      "epoch:698 loss:7.971198 accu:0.500000 aae_loss_1:0.012444 aae_loss_2:0.012456\n",
      "epoch:699 loss:7.971195 accu:0.500000 aae_loss_1:0.013430 aae_loss_2:0.013443\n",
      "epoch:700 loss:7.971197 accu:0.500000 aae_loss_1:0.013862 aae_loss_2:0.013876\n",
      "epoch:701 loss:7.971197 accu:0.500000 aae_loss_1:0.014058 aae_loss_2:0.014072\n",
      "epoch:702 loss:7.971195 accu:0.500000 aae_loss_1:0.014681 aae_loss_2:0.014696\n",
      "epoch:703 loss:7.971195 accu:0.500000 aae_loss_1:0.014509 aae_loss_2:0.014524\n",
      "epoch:704 loss:7.971196 accu:0.500000 aae_loss_1:0.014199 aae_loss_2:0.014214\n",
      "epoch:705 loss:7.971195 accu:0.500000 aae_loss_1:0.015231 aae_loss_2:0.015247\n",
      "epoch:706 loss:7.971196 accu:0.500000 aae_loss_1:0.013737 aae_loss_2:0.013750\n",
      "epoch:707 loss:7.971196 accu:0.500000 aae_loss_1:0.013602 aae_loss_2:0.013616\n",
      "epoch:708 loss:7.971195 accu:0.500000 aae_loss_1:0.014114 aae_loss_2:0.014129\n",
      "epoch:709 loss:7.971196 accu:0.500000 aae_loss_1:0.013580 aae_loss_2:0.013594\n",
      "epoch:710 loss:7.971196 accu:0.500000 aae_loss_1:0.013728 aae_loss_2:0.013741\n",
      "epoch:711 loss:7.971197 accu:0.500000 aae_loss_1:0.014480 aae_loss_2:0.014494\n",
      "epoch:712 loss:7.971198 accu:0.500000 aae_loss_1:0.014319 aae_loss_2:0.014334\n",
      "epoch:713 loss:7.971196 accu:0.500000 aae_loss_1:0.014746 aae_loss_2:0.014760\n",
      "epoch:714 loss:7.971196 accu:0.500000 aae_loss_1:0.015609 aae_loss_2:0.015625\n",
      "epoch:715 loss:7.971196 accu:0.500000 aae_loss_1:0.014595 aae_loss_2:0.014610\n",
      "epoch:716 loss:7.971196 accu:0.500000 aae_loss_1:0.014416 aae_loss_2:0.014430\n",
      "epoch:717 loss:7.971195 accu:0.500000 aae_loss_1:0.014218 aae_loss_2:0.014232\n",
      "epoch:718 loss:7.971197 accu:0.500000 aae_loss_1:0.012875 aae_loss_2:0.012888\n",
      "epoch:719 loss:7.971200 accu:0.500000 aae_loss_1:0.013013 aae_loss_2:0.013026\n",
      "epoch:720 loss:7.971197 accu:0.500000 aae_loss_1:0.013119 aae_loss_2:0.013132\n",
      "epoch:721 loss:7.971195 accu:0.500000 aae_loss_1:0.015446 aae_loss_2:0.015462\n",
      "epoch:722 loss:7.971197 accu:0.500000 aae_loss_1:0.015104 aae_loss_2:0.015119\n",
      "epoch:723 loss:7.971194 accu:0.500000 aae_loss_1:0.014528 aae_loss_2:0.014542\n",
      "epoch:724 loss:7.971197 accu:0.500000 aae_loss_1:0.014540 aae_loss_2:0.014554\n",
      "epoch:725 loss:7.971197 accu:0.500000 aae_loss_1:0.013487 aae_loss_2:0.013501\n",
      "epoch:726 loss:7.971197 accu:0.500000 aae_loss_1:0.014566 aae_loss_2:0.014580\n",
      "epoch:727 loss:7.971197 accu:0.500000 aae_loss_1:0.014188 aae_loss_2:0.014203\n",
      "epoch:728 loss:7.971200 accu:0.500000 aae_loss_1:0.014043 aae_loss_2:0.014057\n",
      "epoch:729 loss:7.971199 accu:0.500000 aae_loss_1:0.013562 aae_loss_2:0.013576\n",
      "epoch:730 loss:7.971198 accu:0.500000 aae_loss_1:0.013887 aae_loss_2:0.013901\n",
      "epoch:731 loss:7.971195 accu:0.500000 aae_loss_1:0.012878 aae_loss_2:0.012891\n",
      "epoch:732 loss:7.971197 accu:0.500000 aae_loss_1:0.011729 aae_loss_2:0.011741\n",
      "epoch:733 loss:7.971197 accu:0.500000 aae_loss_1:0.012750 aae_loss_2:0.012763\n",
      "epoch:734 loss:7.971197 accu:0.500000 aae_loss_1:0.013596 aae_loss_2:0.013610\n",
      "epoch:735 loss:7.971196 accu:0.500000 aae_loss_1:0.014153 aae_loss_2:0.014167\n",
      "epoch:736 loss:7.971197 accu:0.500000 aae_loss_1:0.014019 aae_loss_2:0.014033\n",
      "epoch:737 loss:7.971196 accu:0.500000 aae_loss_1:0.013262 aae_loss_2:0.013275\n",
      "epoch:738 loss:7.971196 accu:0.500000 aae_loss_1:0.014242 aae_loss_2:0.014257\n",
      "epoch:739 loss:7.971196 accu:0.500000 aae_loss_1:0.014169 aae_loss_2:0.014184\n",
      "epoch:740 loss:7.971198 accu:0.500000 aae_loss_1:0.013215 aae_loss_2:0.013228\n",
      "epoch:741 loss:7.971198 accu:0.500000 aae_loss_1:0.012787 aae_loss_2:0.012799\n",
      "epoch:742 loss:7.971198 accu:0.500000 aae_loss_1:0.013778 aae_loss_2:0.013792\n",
      "epoch:743 loss:7.971199 accu:0.500000 aae_loss_1:0.013194 aae_loss_2:0.013207\n",
      "epoch:744 loss:7.971195 accu:0.500000 aae_loss_1:0.011817 aae_loss_2:0.011829\n",
      "epoch:745 loss:7.971196 accu:0.500000 aae_loss_1:0.011700 aae_loss_2:0.011712\n",
      "epoch:746 loss:7.971197 accu:0.500000 aae_loss_1:0.013215 aae_loss_2:0.013228\n",
      "epoch:747 loss:7.971199 accu:0.500000 aae_loss_1:0.014070 aae_loss_2:0.014084\n",
      "epoch:748 loss:7.971197 accu:0.500000 aae_loss_1:0.013389 aae_loss_2:0.013403\n",
      "epoch:749 loss:7.971200 accu:0.500000 aae_loss_1:0.013855 aae_loss_2:0.013868\n",
      "epoch:750 loss:7.971198 accu:0.500000 aae_loss_1:0.014189 aae_loss_2:0.014203\n",
      "epoch:751 loss:7.971196 accu:0.500000 aae_loss_1:0.014757 aae_loss_2:0.014772\n",
      "epoch:752 loss:7.971197 accu:0.500000 aae_loss_1:0.014379 aae_loss_2:0.014394\n",
      "epoch:753 loss:7.971198 accu:0.500000 aae_loss_1:0.013596 aae_loss_2:0.013610\n",
      "epoch:754 loss:7.971197 accu:0.500000 aae_loss_1:0.014503 aae_loss_2:0.014518\n",
      "epoch:755 loss:7.971197 accu:0.500000 aae_loss_1:0.013762 aae_loss_2:0.013775\n",
      "epoch:756 loss:7.971196 accu:0.500000 aae_loss_1:0.013544 aae_loss_2:0.013558\n",
      "epoch:757 loss:7.971195 accu:0.500000 aae_loss_1:0.011835 aae_loss_2:0.011847\n",
      "epoch:758 loss:7.971197 accu:0.500000 aae_loss_1:0.013086 aae_loss_2:0.013099\n",
      "epoch:759 loss:7.971195 accu:0.500000 aae_loss_1:0.013431 aae_loss_2:0.013445\n",
      "epoch:760 loss:7.971196 accu:0.500000 aae_loss_1:0.014059 aae_loss_2:0.014073\n",
      "epoch:761 loss:7.971197 accu:0.500000 aae_loss_1:0.011797 aae_loss_2:0.011809\n",
      "epoch:762 loss:7.971200 accu:0.500000 aae_loss_1:0.014310 aae_loss_2:0.014324\n",
      "epoch:763 loss:7.971195 accu:0.500000 aae_loss_1:0.014297 aae_loss_2:0.014311\n",
      "epoch:764 loss:7.971197 accu:0.500000 aae_loss_1:0.014577 aae_loss_2:0.014592\n",
      "epoch:765 loss:7.971199 accu:0.500000 aae_loss_1:0.015292 aae_loss_2:0.015308\n",
      "epoch:766 loss:7.971196 accu:0.500000 aae_loss_1:0.014074 aae_loss_2:0.014088\n",
      "epoch:767 loss:7.971197 accu:0.500000 aae_loss_1:0.015085 aae_loss_2:0.015100\n",
      "epoch:768 loss:7.971197 accu:0.500000 aae_loss_1:0.015064 aae_loss_2:0.015079\n",
      "epoch:769 loss:7.971196 accu:0.500000 aae_loss_1:0.016111 aae_loss_2:0.016127\n",
      "epoch:770 loss:7.971196 accu:0.500000 aae_loss_1:0.015358 aae_loss_2:0.015373\n",
      "epoch:771 loss:7.971200 accu:0.500000 aae_loss_1:0.013436 aae_loss_2:0.013450\n",
      "epoch:772 loss:7.971197 accu:0.500000 aae_loss_1:0.013255 aae_loss_2:0.013268\n",
      "epoch:773 loss:7.971197 accu:0.500000 aae_loss_1:0.012973 aae_loss_2:0.012986\n",
      "epoch:774 loss:7.971196 accu:0.500000 aae_loss_1:0.013257 aae_loss_2:0.013270\n",
      "epoch:775 loss:7.971198 accu:0.500000 aae_loss_1:0.013839 aae_loss_2:0.013853\n",
      "epoch:776 loss:7.971202 accu:0.500000 aae_loss_1:0.013485 aae_loss_2:0.013498\n",
      "epoch:777 loss:7.971195 accu:0.500000 aae_loss_1:0.013123 aae_loss_2:0.013137\n",
      "epoch:778 loss:7.971196 accu:0.500000 aae_loss_1:0.013006 aae_loss_2:0.013019\n",
      "epoch:779 loss:7.971197 accu:0.500000 aae_loss_1:0.014320 aae_loss_2:0.014334\n",
      "epoch:780 loss:7.971195 accu:0.500000 aae_loss_1:0.016502 aae_loss_2:0.016518\n",
      "epoch:781 loss:7.971196 accu:0.500000 aae_loss_1:0.017103 aae_loss_2:0.017121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:782 loss:7.971196 accu:0.500000 aae_loss_1:0.020689 aae_loss_2:0.020710\n",
      "epoch:783 loss:7.971197 accu:0.500000 aae_loss_1:0.016689 aae_loss_2:0.016706\n",
      "epoch:784 loss:7.971198 accu:0.500000 aae_loss_1:0.015382 aae_loss_2:0.015398\n",
      "epoch:785 loss:7.971195 accu:0.500000 aae_loss_1:0.014049 aae_loss_2:0.014063\n",
      "epoch:786 loss:7.971197 accu:0.500000 aae_loss_1:0.014026 aae_loss_2:0.014040\n",
      "epoch:787 loss:7.971197 accu:0.500000 aae_loss_1:0.013283 aae_loss_2:0.013296\n",
      "epoch:788 loss:7.971198 accu:0.500000 aae_loss_1:0.014384 aae_loss_2:0.014398\n",
      "epoch:789 loss:7.971199 accu:0.500000 aae_loss_1:0.012576 aae_loss_2:0.012589\n",
      "epoch:790 loss:7.971200 accu:0.500000 aae_loss_1:0.012770 aae_loss_2:0.012783\n",
      "epoch:791 loss:7.971198 accu:0.500000 aae_loss_1:0.013476 aae_loss_2:0.013490\n",
      "epoch:792 loss:7.971197 accu:0.500000 aae_loss_1:0.012964 aae_loss_2:0.012977\n",
      "epoch:793 loss:7.971197 accu:0.500000 aae_loss_1:0.014001 aae_loss_2:0.014015\n",
      "epoch:794 loss:7.971199 accu:0.500000 aae_loss_1:0.013472 aae_loss_2:0.013485\n",
      "epoch:795 loss:7.971195 accu:0.500000 aae_loss_1:0.014290 aae_loss_2:0.014304\n",
      "epoch:796 loss:7.971201 accu:0.500000 aae_loss_1:0.012309 aae_loss_2:0.012321\n",
      "epoch:797 loss:7.971197 accu:0.500000 aae_loss_1:0.013683 aae_loss_2:0.013697\n",
      "epoch:798 loss:7.971199 accu:0.500000 aae_loss_1:0.012636 aae_loss_2:0.012649\n",
      "epoch:799 loss:7.971196 accu:0.500000 aae_loss_1:0.014258 aae_loss_2:0.014272\n",
      "epoch:800 loss:7.971195 accu:0.500000 aae_loss_1:0.013267 aae_loss_2:0.013280\n",
      "epoch:801 loss:7.971197 accu:0.500000 aae_loss_1:0.014103 aae_loss_2:0.014117\n",
      "epoch:802 loss:7.971197 accu:0.500000 aae_loss_1:0.013509 aae_loss_2:0.013522\n",
      "epoch:803 loss:7.971197 accu:0.500000 aae_loss_1:0.013406 aae_loss_2:0.013419\n",
      "epoch:804 loss:7.971195 accu:0.500000 aae_loss_1:0.012901 aae_loss_2:0.012914\n",
      "epoch:805 loss:7.971195 accu:0.500000 aae_loss_1:0.012596 aae_loss_2:0.012609\n",
      "epoch:806 loss:7.971198 accu:0.500000 aae_loss_1:0.012939 aae_loss_2:0.012952\n",
      "epoch:807 loss:7.971197 accu:0.500000 aae_loss_1:0.013655 aae_loss_2:0.013668\n",
      "epoch:808 loss:7.971195 accu:0.500000 aae_loss_1:0.014241 aae_loss_2:0.014255\n",
      "epoch:809 loss:7.971198 accu:0.500000 aae_loss_1:0.015581 aae_loss_2:0.015597\n",
      "epoch:810 loss:7.971199 accu:0.500000 aae_loss_1:0.014879 aae_loss_2:0.014893\n",
      "epoch:811 loss:7.971196 accu:0.500000 aae_loss_1:0.015189 aae_loss_2:0.015205\n",
      "epoch:812 loss:7.971196 accu:0.500000 aae_loss_1:0.013777 aae_loss_2:0.013790"
     ]
    }
   ],
   "source": [
    "real_labels = np.ones(shape=(BATCH_SIZE , 1)) #真实样本label为1\n",
    "fake_labels = np.zeros(shape=(BATCH_SIZE , 1)) #假样本label为0\n",
    "\n",
    "for i in range(10000):\n",
    "    noise_encoder_feature = np.random.normal(size=(BATCH_SIZE , LATENT_DIM))\n",
    "\n",
    "    #real_image = load_image()\n",
    "    real_image = load_mnist()\n",
    "    #训练判别器\n",
    "    real_encoder_feature = encoder_i.predict(real_image)\n",
    "\n",
    "    noise_loss = discriminator_i.train_on_batch(noise_encoder_feature , real_labels)\n",
    "    real_loss = discriminator_i.train_on_batch(real_encoder_feature , fake_labels)\n",
    "\n",
    "    loss = np.add(noise_loss , real_loss)/2\n",
    "\n",
    "    #训练aae\n",
    "    aae_loss = aae.train_on_batch(real_image , [real_image , real_labels])\n",
    "    \n",
    "    print('epoch:%d loss:%f accu:%f aae_loss_1:%f aae_loss_2:%f' % (i , loss[0] , loss[1] , aae_loss[0] , aae_loss[1]))\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        #write_image(i)\n",
    "        write_image_mnist(i)\n",
    "    \n",
    "#write_image(999)\n",
    "write_image_mnist(999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "discriminator (Sequential)   (None, 1)                 533505    \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 0\n",
      "Non-trainable params: 533,505\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator (Sequential)       (None, 28, 28, 1)         1097744   \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z (InputLayer)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "generator_Model (Model)      (None, 96, 96, 3)         29029120  \n",
      "_________________________________________________________________\n",
      "discriminator_Model (Model)  (None, 1)                 14320641  \n",
      "=================================================================\n",
      "Total params: 43,349,761\n",
      "Trainable params: 29,025,536\n",
      "Non-trainable params: 14,324,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "combined_model_i.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeli = Sequential()\n",
    "\n",
    "modeli.add(Dense(128 * 7 * 7, activation=\"relu\", input_shape=(LATENT_DIM,)))\n",
    "modeli.add(Reshape((7, 7, 128)))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(UpSampling2D())\n",
    "modeli.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(BatchNormalization(momentum=0.8))\n",
    "modeli.add(Activation(\"relu\"))\n",
    "modeli.add(Conv2D(CHANNEL, kernel_size=3, padding=\"same\"))\n",
    "modeli.add(Activation(\"tanh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modeli.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_inbound_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-514ad70b5d55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-17d76ed16df1>\u001b[0m in \u001b[0;36mencoder\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mencoder_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0m__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mencoder_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[0;32m     90\u001b[0m             \u001b[1;31m# Graph network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# Subclassed network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[1;32m--> 235\u001b[1;33m             self.inputs, self.outputs)\n\u001b[0m\u001b[0;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1410\u001b[0m                   \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                   \u001b[0mnode_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1412\u001b[1;33m                   tensor_index=tensor_index)\n\u001b[0m\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[1;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[0;32m   1397\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[1;32m-> 1399\u001b[1;33m                       node_index, tensor_index)\n\u001b[0m\u001b[0;32m   1400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[1;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[0;32m   1397\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1398\u001b[0m             build_map(x, finished_nodes, nodes_in_progress, layer,\n\u001b[1;32m-> 1399\u001b[1;33m                       node_index, tensor_index)\n\u001b[0m\u001b[0;32m   1400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m         \u001b[0mfinished_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mbuild_map\u001b[1;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcycle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdetected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \"\"\"\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Prevent cycles.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_inbound_nodes'"
     ]
    }
   ],
   "source": [
    "aa=encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
