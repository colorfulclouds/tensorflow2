{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    data = pd.read_csv(file_name , header=0,\n",
    "                      delimiter = '\\t' , quoting = 3)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = read_data('labeledTrainData.csv')\n",
    "test = read_data('testData.csv')\n",
    "unlabeled_train = read_data('unlabeledTrainData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'].size + test['review'].size + unlabeled_train['review'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#每一个\n",
    "def review_to_wordlist(review , remove_stopwords = False):\n",
    "    #不删除stopwords 因为在强大的word2vec有利用价值\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    review_text = re.sub(\"[^a-zA-Z]\" , ' ' , review_text)\n",
    "    \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review , tokenizer , remove_stopwords = False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #上面的函数 将段落分为句子\n",
    "    #strip默认将字符串最前面和最后面的空格去掉\n",
    "    \n",
    "    #循环处理每一个句子\n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence , remove_stopwords))\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "#一个评论是一段\n",
    "#将一段评论分为一些句子\n",
    "#每个句子还要再分为一些词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file I:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print('training set')\n",
    "for review in train['review']:\n",
    "    sentences += review_to_sentences(review , tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabel set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file I:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "I:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "print('unlabel set')\n",
    "for review in unlabeled_train['review']:\n",
    "    sentences += review_to_sentences(review , tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)\n",
    "#看第一条评论包含什么哪些单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s',\n",
    "                   level = logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 400\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-03-08 19:14:34,555 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-08 19:15:40,603 : INFO : collecting all words and their counts\n",
      "2018-03-08 19:15:40,607 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-08 19:15:40,715 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2018-03-08 19:15:40,814 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2018-03-08 19:15:40,895 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2018-03-08 19:15:40,980 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2018-03-08 19:15:41,058 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2018-03-08 19:15:41,144 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2018-03-08 19:15:41,223 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2018-03-08 19:15:41,305 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2018-03-08 19:15:41,388 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2018-03-08 19:15:41,465 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2018-03-08 19:15:41,546 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2018-03-08 19:15:41,629 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2018-03-08 19:15:41,710 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2018-03-08 19:15:41,789 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2018-03-08 19:15:41,879 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2018-03-08 19:15:41,960 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2018-03-08 19:15:42,042 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2018-03-08 19:15:42,129 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2018-03-08 19:15:42,210 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2018-03-08 19:15:42,294 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2018-03-08 19:15:42,376 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2018-03-08 19:15:42,458 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2018-03-08 19:15:42,547 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2018-03-08 19:15:42,670 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2018-03-08 19:15:42,789 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2018-03-08 19:15:42,876 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2018-03-08 19:15:42,960 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2018-03-08 19:15:43,045 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2018-03-08 19:15:43,127 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2018-03-08 19:15:43,209 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2018-03-08 19:15:43,291 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2018-03-08 19:15:43,378 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2018-03-08 19:15:43,463 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2018-03-08 19:15:43,554 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2018-03-08 19:15:43,637 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2018-03-08 19:15:43,721 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
      "2018-03-08 19:15:43,812 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
      "2018-03-08 19:15:43,907 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
      "2018-03-08 19:15:44,001 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
      "2018-03-08 19:15:44,085 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
      "2018-03-08 19:15:44,165 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
      "2018-03-08 19:15:44,244 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
      "2018-03-08 19:15:44,359 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
      "2018-03-08 19:15:44,458 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
      "2018-03-08 19:15:44,545 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
      "2018-03-08 19:15:44,637 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
      "2018-03-08 19:15:44,718 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
      "2018-03-08 19:15:44,807 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
      "2018-03-08 19:15:44,900 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
      "2018-03-08 19:15:44,986 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
      "2018-03-08 19:15:45,076 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
      "2018-03-08 19:15:45,162 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
      "2018-03-08 19:15:45,241 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
      "2018-03-08 19:15:45,327 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
      "2018-03-08 19:15:45,410 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
      "2018-03-08 19:15:45,490 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
      "2018-03-08 19:15:45,574 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
      "2018-03-08 19:15:45,669 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
      "2018-03-08 19:15:45,761 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
      "2018-03-08 19:15:45,850 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
      "2018-03-08 19:15:45,942 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
      "2018-03-08 19:15:46,035 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
      "2018-03-08 19:15:46,121 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
      "2018-03-08 19:15:46,209 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
      "2018-03-08 19:15:46,305 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
      "2018-03-08 19:15:46,394 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
      "2018-03-08 19:15:46,487 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
      "2018-03-08 19:15:46,586 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
      "2018-03-08 19:15:46,678 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
      "2018-03-08 19:15:46,771 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
      "2018-03-08 19:15:46,865 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-08 19:15:46,957 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n",
      "2018-03-08 19:15:47,043 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n",
      "2018-03-08 19:15:47,123 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
      "2018-03-08 19:15:47,201 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n",
      "2018-03-08 19:15:47,284 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
      "2018-03-08 19:15:47,437 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
      "2018-03-08 19:15:47,614 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
      "2018-03-08 19:15:47,734 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
      "2018-03-08 19:15:47,785 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
      "2018-03-08 19:15:47,786 : INFO : Loading a fresh vocabulary\n",
      "2018-03-08 19:15:47,886 : INFO : min_count=400 retains 3385 unique words (2% of original 123504, drops 120119)\n",
      "2018-03-08 19:15:47,888 : INFO : min_count=400 leaves 15623231 word corpus (87% of original 17798082, drops 2174851)\n",
      "2018-03-08 19:15:47,912 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2018-03-08 19:15:47,922 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2018-03-08 19:15:47,925 : INFO : downsampling leaves estimated 10938094 word corpus (70.0% of prior 15623231)\n",
      "2018-03-08 19:15:47,947 : INFO : estimated required memory for 3385 words and 300 dimensions: 9816500 bytes\n",
      "2018-03-08 19:15:47,952 : INFO : resetting layer weights\n",
      "2018-03-08 19:15:48,040 : INFO : training model with 4 workers on 3385 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-08 19:15:49,073 : INFO : EPOCH 1 - PROGRESS: at 4.21% examples, 452919 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:15:50,073 : INFO : EPOCH 1 - PROGRESS: at 8.06% examples, 434467 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:15:51,102 : INFO : EPOCH 1 - PROGRESS: at 11.67% examples, 416822 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:15:52,113 : INFO : EPOCH 1 - PROGRESS: at 15.40% examples, 412981 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:15:53,119 : INFO : EPOCH 1 - PROGRESS: at 19.38% examples, 415795 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:15:54,135 : INFO : EPOCH 1 - PROGRESS: at 22.65% examples, 405023 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:15:55,157 : INFO : EPOCH 1 - PROGRESS: at 25.91% examples, 396868 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:15:56,182 : INFO : EPOCH 1 - PROGRESS: at 29.36% examples, 393588 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:15:57,182 : INFO : EPOCH 1 - PROGRESS: at 33.38% examples, 397537 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:15:58,183 : INFO : EPOCH 1 - PROGRESS: at 36.73% examples, 394683 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:15:59,191 : INFO : EPOCH 1 - PROGRESS: at 40.92% examples, 400234 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:00,193 : INFO : EPOCH 1 - PROGRESS: at 45.12% examples, 405117 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:01,199 : INFO : EPOCH 1 - PROGRESS: at 49.55% examples, 411420 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:02,213 : INFO : EPOCH 1 - PROGRESS: at 53.83% examples, 414895 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:03,215 : INFO : EPOCH 1 - PROGRESS: at 57.90% examples, 417368 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:04,222 : INFO : EPOCH 1 - PROGRESS: at 62.37% examples, 421707 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:05,252 : INFO : EPOCH 1 - PROGRESS: at 66.90% examples, 425323 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:06,274 : INFO : EPOCH 1 - PROGRESS: at 71.43% examples, 428761 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:07,286 : INFO : EPOCH 1 - PROGRESS: at 74.74% examples, 425005 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:08,306 : INFO : EPOCH 1 - PROGRESS: at 78.05% examples, 421466 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:16:09,311 : INFO : EPOCH 1 - PROGRESS: at 82.33% examples, 423468 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:10,327 : INFO : EPOCH 1 - PROGRESS: at 86.53% examples, 424875 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:11,331 : INFO : EPOCH 1 - PROGRESS: at 90.48% examples, 425230 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:12,343 : INFO : EPOCH 1 - PROGRESS: at 94.65% examples, 426223 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:13,359 : INFO : EPOCH 1 - PROGRESS: at 98.00% examples, 423642 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:14,173 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-08 19:16:14,184 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-08 19:16:14,211 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-08 19:16:14,233 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-08 19:16:14,236 : INFO : EPOCH - 1 : training on 17798082 raw words (10939780 effective words) took 26.2s, 417895 effective words/s\n",
      "2018-03-08 19:16:15,291 : INFO : EPOCH 2 - PROGRESS: at 3.48% examples, 369552 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:16,304 : INFO : EPOCH 2 - PROGRESS: at 5.81% examples, 311938 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:17,306 : INFO : EPOCH 2 - PROGRESS: at 9.12% examples, 326314 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:18,312 : INFO : EPOCH 2 - PROGRESS: at 13.31% examples, 357471 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:19,348 : INFO : EPOCH 2 - PROGRESS: at 16.84% examples, 359482 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:20,354 : INFO : EPOCH 2 - PROGRESS: at 20.51% examples, 365579 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:21,356 : INFO : EPOCH 2 - PROGRESS: at 22.87% examples, 350412 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:22,364 : INFO : EPOCH 2 - PROGRESS: at 26.41% examples, 354464 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:23,365 : INFO : EPOCH 2 - PROGRESS: at 30.32% examples, 362661 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:24,372 : INFO : EPOCH 2 - PROGRESS: at 33.72% examples, 362389 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:25,383 : INFO : EPOCH 2 - PROGRESS: at 36.68% examples, 358708 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:26,394 : INFO : EPOCH 2 - PROGRESS: at 39.59% examples, 355100 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:27,406 : INFO : EPOCH 2 - PROGRESS: at 43.25% examples, 358609 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:28,417 : INFO : EPOCH 2 - PROGRESS: at 47.67% examples, 367196 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:29,427 : INFO : EPOCH 2 - PROGRESS: at 50.68% examples, 364588 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:30,437 : INFO : EPOCH 2 - PROGRESS: at 54.26% examples, 366061 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:31,452 : INFO : EPOCH 2 - PROGRESS: at 58.51% examples, 371931 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:32,463 : INFO : EPOCH 2 - PROGRESS: at 60.90% examples, 365764 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:33,465 : INFO : EPOCH 2 - PROGRESS: at 62.82% examples, 357501 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:34,513 : INFO : EPOCH 2 - PROGRESS: at 66.95% examples, 361425 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:35,530 : INFO : EPOCH 2 - PROGRESS: at 69.59% examples, 357715 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:36,557 : INFO : EPOCH 2 - PROGRESS: at 72.44% examples, 355246 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:37,565 : INFO : EPOCH 2 - PROGRESS: at 75.42% examples, 353846 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:38,581 : INFO : EPOCH 2 - PROGRESS: at 79.40% examples, 356967 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-08 19:16:39,582 : INFO : EPOCH 2 - PROGRESS: at 83.34% examples, 359817 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:40,586 : INFO : EPOCH 2 - PROGRESS: at 86.08% examples, 357533 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:41,599 : INFO : EPOCH 2 - PROGRESS: at 89.58% examples, 358403 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-08 19:16:42,613 : INFO : EPOCH 2 - PROGRESS: at 93.40% examples, 360298 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:43,631 : INFO : EPOCH 2 - PROGRESS: at 95.86% examples, 356787 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:44,643 : INFO : EPOCH 2 - PROGRESS: at 99.27% examples, 357439 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:44,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-08 19:16:44,818 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-08 19:16:44,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-08 19:16:44,841 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-08 19:16:44,849 : INFO : EPOCH - 2 : training on 17798082 raw words (10938093 effective words) took 30.6s, 357618 effective words/s\n",
      "2018-03-08 19:16:45,930 : INFO : EPOCH 3 - PROGRESS: at 3.25% examples, 344480 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:46,931 : INFO : EPOCH 3 - PROGRESS: at 6.96% examples, 373220 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:47,980 : INFO : EPOCH 3 - PROGRESS: at 10.21% examples, 359945 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-08 19:16:48,990 : INFO : EPOCH 3 - PROGRESS: at 12.92% examples, 343187 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:49,990 : INFO : EPOCH 3 - PROGRESS: at 15.73% examples, 335960 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:50,993 : INFO : EPOCH 3 - PROGRESS: at 19.26% examples, 343117 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:52,003 : INFO : EPOCH 3 - PROGRESS: at 23.48% examples, 359227 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:53,009 : INFO : EPOCH 3 - PROGRESS: at 25.69% examples, 344221 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:54,016 : INFO : EPOCH 3 - PROGRESS: at 28.43% examples, 339155 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:55,023 : INFO : EPOCH 3 - PROGRESS: at 32.40% examples, 347885 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:16:56,034 : INFO : EPOCH 3 - PROGRESS: at 35.10% examples, 342803 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:57,061 : INFO : EPOCH 3 - PROGRESS: at 38.18% examples, 341590 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:16:58,083 : INFO : EPOCH 3 - PROGRESS: at 42.30% examples, 349505 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-08 19:16:59,103 : INFO : EPOCH 3 - PROGRESS: at 45.55% examples, 349448 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:00,106 : INFO : EPOCH 3 - PROGRESS: at 48.27% examples, 346159 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:01,113 : INFO : EPOCH 3 - PROGRESS: at 50.40% examples, 339039 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:02,127 : INFO : EPOCH 3 - PROGRESS: at 53.99% examples, 341846 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-08 19:17:03,141 : INFO : EPOCH 3 - PROGRESS: at 58.01% examples, 347374 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-08 19:17:04,157 : INFO : EPOCH 3 - PROGRESS: at 60.90% examples, 345630 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:05,182 : INFO : EPOCH 3 - PROGRESS: at 64.33% examples, 346578 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:06,196 : INFO : EPOCH 3 - PROGRESS: at 68.43% examples, 351128 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:07,202 : INFO : EPOCH 3 - PROGRESS: at 71.22% examples, 349045 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:08,219 : INFO : EPOCH 3 - PROGRESS: at 74.23% examples, 347991 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-08 19:17:09,223 : INFO : EPOCH 3 - PROGRESS: at 78.31% examples, 352020 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:10,238 : INFO : EPOCH 3 - PROGRESS: at 80.76% examples, 348334 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:11,240 : INFO : EPOCH 3 - PROGRESS: at 84.01% examples, 348638 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:12,241 : INFO : EPOCH 3 - PROGRESS: at 87.96% examples, 351761 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:13,255 : INFO : EPOCH 3 - PROGRESS: at 90.81% examples, 350215 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:14,264 : INFO : EPOCH 3 - PROGRESS: at 94.15% examples, 350514 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:15,273 : INFO : EPOCH 3 - PROGRESS: at 98.51% examples, 354604 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:15,602 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-08 19:17:15,628 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-08 19:17:15,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-08 19:17:15,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-08 19:17:15,670 : INFO : EPOCH - 3 : training on 17798082 raw words (10936116 effective words) took 30.8s, 355402 effective words/s\n",
      "2018-03-08 19:17:16,710 : INFO : EPOCH 4 - PROGRESS: at 2.39% examples, 262011 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:17,715 : INFO : EPOCH 4 - PROGRESS: at 6.18% examples, 335259 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:18,719 : INFO : EPOCH 4 - PROGRESS: at 10.53% examples, 380420 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:19,724 : INFO : EPOCH 4 - PROGRESS: at 13.03% examples, 352676 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:20,736 : INFO : EPOCH 4 - PROGRESS: at 15.84% examples, 342700 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:21,747 : INFO : EPOCH 4 - PROGRESS: at 20.00% examples, 359367 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:22,766 : INFO : EPOCH 4 - PROGRESS: at 22.59% examples, 347630 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:17:23,816 : INFO : EPOCH 4 - PROGRESS: at 25.45% examples, 341240 words/s, in_qsize 8, out_qsize 2\n",
      "2018-03-08 19:17:24,823 : INFO : EPOCH 4 - PROGRESS: at 29.64% examples, 353999 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:25,848 : INFO : EPOCH 4 - PROGRESS: at 33.44% examples, 358216 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:17:26,869 : INFO : EPOCH 4 - PROGRESS: at 37.01% examples, 360670 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:27,914 : INFO : EPOCH 4 - PROGRESS: at 39.42% examples, 351376 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:28,951 : INFO : EPOCH 4 - PROGRESS: at 42.25% examples, 347479 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:29,953 : INFO : EPOCH 4 - PROGRESS: at 45.55% examples, 348409 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:30,964 : INFO : EPOCH 4 - PROGRESS: at 49.49% examples, 353841 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:31,968 : INFO : EPOCH 4 - PROGRESS: at 52.78% examples, 353868 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:32,994 : INFO : EPOCH 4 - PROGRESS: at 55.55% examples, 350580 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:34,007 : INFO : EPOCH 4 - PROGRESS: at 58.29% examples, 347936 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:35,009 : INFO : EPOCH 4 - PROGRESS: at 61.81% examples, 349899 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:36,017 : INFO : EPOCH 4 - PROGRESS: at 66.17% examples, 356108 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:37,021 : INFO : EPOCH 4 - PROGRESS: at 69.14% examples, 354613 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:38,033 : INFO : EPOCH 4 - PROGRESS: at 72.09% examples, 353069 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:39,033 : INFO : EPOCH 4 - PROGRESS: at 74.96% examples, 351352 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:40,060 : INFO : EPOCH 4 - PROGRESS: at 78.00% examples, 350137 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:41,067 : INFO : EPOCH 4 - PROGRESS: at 80.99% examples, 349064 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:42,073 : INFO : EPOCH 4 - PROGRESS: at 83.73% examples, 347155 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:43,093 : INFO : EPOCH 4 - PROGRESS: at 87.47% examples, 349215 words/s, in_qsize 5, out_qsize 2\n",
      "2018-03-08 19:17:44,098 : INFO : EPOCH 4 - PROGRESS: at 91.37% examples, 351971 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:45,110 : INFO : EPOCH 4 - PROGRESS: at 95.65% examples, 355523 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:46,113 : INFO : EPOCH 4 - PROGRESS: at 98.82% examples, 355489 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-08 19:17:46,513 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-08 19:17:46,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-08 19:17:46,569 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-08 19:17:46,595 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-08 19:17:46,604 : INFO : EPOCH - 4 : training on 17798082 raw words (10937294 effective words) took 30.9s, 354001 effective words/s\n",
      "2018-03-08 19:17:47,687 : INFO : EPOCH 5 - PROGRESS: at 3.59% examples, 385218 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:48,695 : INFO : EPOCH 5 - PROGRESS: at 7.60% examples, 407800 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:49,712 : INFO : EPOCH 5 - PROGRESS: at 10.59% examples, 378583 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:50,724 : INFO : EPOCH 5 - PROGRESS: at 14.62% examples, 391732 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-08 19:17:51,745 : INFO : EPOCH 5 - PROGRESS: at 18.98% examples, 405908 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:52,757 : INFO : EPOCH 5 - PROGRESS: at 21.92% examples, 391051 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:53,763 : INFO : EPOCH 5 - PROGRESS: at 25.69% examples, 393630 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:54,763 : INFO : EPOCH 5 - PROGRESS: at 29.98% examples, 403367 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:55,771 : INFO : EPOCH 5 - PROGRESS: at 32.97% examples, 393785 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:56,782 : INFO : EPOCH 5 - PROGRESS: at 35.21% examples, 378678 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:17:57,789 : INFO : EPOCH 5 - PROGRESS: at 39.31% examples, 384753 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:58,811 : INFO : EPOCH 5 - PROGRESS: at 41.87% examples, 375601 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:17:59,812 : INFO : EPOCH 5 - PROGRESS: at 44.79% examples, 371321 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:00,827 : INFO : EPOCH 5 - PROGRESS: at 48.27% examples, 371899 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-08 19:18:01,829 : INFO : EPOCH 5 - PROGRESS: at 50.85% examples, 365989 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-08 19:18:02,837 : INFO : EPOCH 5 - PROGRESS: at 53.67% examples, 362097 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:18:03,839 : INFO : EPOCH 5 - PROGRESS: at 56.60% examples, 359914 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:18:04,851 : INFO : EPOCH 5 - PROGRESS: at 59.84% examples, 359742 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:18:05,856 : INFO : EPOCH 5 - PROGRESS: at 63.78% examples, 363229 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:06,877 : INFO : EPOCH 5 - PROGRESS: at 65.96% examples, 356734 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:07,882 : INFO : EPOCH 5 - PROGRESS: at 69.54% examples, 358351 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:08,885 : INFO : EPOCH 5 - PROGRESS: at 73.56% examples, 362074 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:18:09,894 : INFO : EPOCH 5 - PROGRESS: at 76.16% examples, 358473 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:18:10,919 : INFO : EPOCH 5 - PROGRESS: at 78.67% examples, 354710 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:18:11,925 : INFO : EPOCH 5 - PROGRESS: at 81.32% examples, 352004 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:12,933 : INFO : EPOCH 5 - PROGRESS: at 85.40% examples, 355574 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-08 19:18:13,935 : INFO : EPOCH 5 - PROGRESS: at 88.98% examples, 356899 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-08 19:18:14,941 : INFO : EPOCH 5 - PROGRESS: at 91.32% examples, 353303 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:15,946 : INFO : EPOCH 5 - PROGRESS: at 95.00% examples, 354821 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:16,980 : INFO : EPOCH 5 - PROGRESS: at 98.89% examples, 356875 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-08 19:18:17,316 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-08 19:18:17,332 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-08 19:18:17,359 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-08 19:18:17,377 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-08 19:18:17,381 : INFO : EPOCH - 5 : training on 17798082 raw words (10939374 effective words) took 30.7s, 356196 effective words/s\n",
      "2018-03-08 19:18:17,384 : INFO : training on a 88990410 raw words (54690657 effective words) took 149.3s, 366210 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences , workers=num_workers,\n",
    "                         size = num_features , min_count = min_word_count,\n",
    "                         window = context , sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-08 19:19:23,196 : INFO : saving Word2Vec object under 300f_40minword_10window, separately None\n",
      "2018-03-08 19:19:23,200 : INFO : not storing attribute vectors_norm\n",
      "2018-03-08 19:19:23,204 : INFO : not storing attribute cum_table\n",
      "2018-03-08 19:19:23,347 : INFO : saved 300f_40minword_10window\n"
     ]
    }
   ],
   "source": [
    "model_name = '300f_40minword_10window'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'boy'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('smile cry eat boy'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('blue', 0.6991257667541504),\n",
       " ('green', 0.6184930801391602),\n",
       " ('snow', 0.5934262275695801),\n",
       " ('heat', 0.5785943269729614),\n",
       " ('sky', 0.5500898957252502),\n",
       " ('wearing', 0.5422793626785278),\n",
       " ('pink', 0.5409970283508301),\n",
       " ('water', 0.5348068475723267),\n",
       " ('tiger', 0.5320825576782227),\n",
       " ('eyed', 0.5171416997909546)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('willing', 0.6274535059928894),\n",
       " ('sure', 0.6025159358978271),\n",
       " ('convinced', 0.5942209959030151),\n",
       " ('aware', 0.5764584541320801),\n",
       " ('proud', 0.5758904218673706),\n",
       " ('gonna', 0.5232254266738892),\n",
       " ('glad', 0.5095299482345581),\n",
       " ('ashamed', 0.5074025392532349),\n",
       " ('guessing', 0.5048553347587585),\n",
       " ('prepared', 0.49442702531814575)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('afraid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
